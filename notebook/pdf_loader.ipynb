{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d497ec74",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b736b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashis\\OneDrive\\Desktop\\RAG_sample\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447c3ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 PDF files to process\n",
      "\n",
      "Processing: 1409.7495v2.pdf\n",
      " ✅ Loaded 11 pages\n",
      "\n",
      "Processing: 1510.02192v1.pdf\n",
      " ✅ Loaded 9 pages\n",
      "\n",
      "Processing: 1712.03935v1.pdf\n",
      " ✅ Loaded 13 pages\n",
      "\n",
      "Processing: 1905.10437v4.pdf\n",
      " ✅ Loaded 31 pages\n",
      "\n",
      "Processing: 2205.07649v2.pdf\n",
      " ✅ Loaded 21 pages\n",
      "\n",
      "Total documents loaded: 85\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdfs inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\" ✅ Loaded {len(documents)} pages\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all pdfs\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91499f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nYaroslav Ganin GANIN @SKOLTECH .RU\\nVictor Lempitsky LEMPITSKY @SKOLTECH .RU\\nSkolkovo Institute of Science and Technology (Skoltech)\\nAbstract\\nTop-performing deep architectures are trained on\\nmassive amounts of labeled data. In the absence\\nof labeled data for a certain task, domain adap-\\ntation often provides an attractive option given\\nthat labeled data of similar nature but from a dif-\\nferent domain (e.g. synthetic images) are avail-\\nable. Here, we propose a new approach to do-\\nmain adaptation in deep architectures that can\\nbe trained on large amount of labeled data from\\nthe source domain and large amount of unlabeled\\ndata from the target domain (no labeled target-\\ndomain data is necessary).\\nAs the training progresses, the approach pro-\\nmotes the emergence of “deep” features that are\\n(i) discriminative for the main learning task on\\nthe source domain and (ii) invariant with respect\\nto the shift between the domains. We show that\\nthis adaptation behaviour can be achieved in al-\\nmost any feed-forward model by augmenting it\\nwith few standard layers and a simple new gra-\\ndient reversal layer. The resulting augmented\\narchitecture can be trained using standard back-\\npropagation.\\nOverall, the approach can be implemented with\\nlittle effort using any of the deep-learning pack-\\nages. The method performs very well in a se-\\nries of image classiﬁcation experiments, achiev-\\ning adaptation effect in the presence of big do-\\nmain shifts and outperforming previous state-of-\\nthe-art on Ofﬁce datasets.\\n1. Introduction\\nDeep feed-forward architectures have brought impressive\\nadvances to the state-of-the-art across a wide variety of\\nmachine-learning tasks and applications. At the moment,\\nhowever, these leaps in performance come only when a\\nlarge amount of labeled training data is available. At the\\nsame time, for problems lacking labeled data, it may be\\nstill possible to obtain training sets that are big enough for\\ntraining large-scale deep models, but that suffer from the\\nshift in data distribution from the actual data encountered\\nat “test time”. One particularly important example is syn-\\nthetic or semi-synthetic training data, which may come in\\nabundance and be fully labeled, but which inevitably have\\na distribution that is different from real data (Liebelt &\\nSchmid, 2010; Stark et al., 2010; V´azquez et al., 2014; Sun\\n& Saenko, 2014).\\nLearning a discriminative classiﬁer or other predictor in\\nthe presence of a shift between training and test distribu-\\ntions is known as domain adaptation (DA). A number of\\napproaches to domain adaptation has been suggested in the\\ncontext of shallow learning, e.g. in the situation when data\\nrepresentation/features are given and ﬁxed. The proposed\\napproaches then build the mappings between the source\\n(training-time) and the target (test-time) domains, so that\\nthe classiﬁer learned for the source domain can also be ap-\\nplied to the target domain, when composed with the learned\\nmapping between domains. The appeal of the domain\\nadaptation approaches is the ability to learn a mapping be-\\ntween domains in the situation when the target domain data\\nare either fully unlabeled ( unsupervised domain annota-\\ntion) or have few labeled samples (semi-supervised domain\\nadaptation). Below, we focus on the harder unsupervised\\ncase, although the proposed approach can be generalized to\\nthe semi-supervised case rather straightforwardly.\\nUnlike most previous papers on domain adaptation that\\nworked with ﬁxed feature representations, we focus on\\ncombining domain adaptation and deep feature learning\\nwithin one training process (deep domain adaptation). Our\\ngoal is to embed domain adaptation into the process of\\nlearning representation, so that the ﬁnal classiﬁcation de-\\ncisions are made based on features that are both discrim-\\ninative and invariant to the change of domains, i.e. have\\nthe same or very similar distributions in the source and the\\ntarget domains. In this way, the obtained feed-forward net-\\nwork can be applicable to the target domain without being\\nhindered by the shift between the two domains.\\nWe thus focus on learning features that combine (i)\\ndiscriminativeness and (ii) domain-invariance. This is\\nachieved by jointly optimizing the underlying features as\\nwell as two discriminative classiﬁers operating on these\\nfeatures: (i) the label predictor that predicts class labels\\nand is used both during training and at test time and (ii) the\\narXiv:1409.7495v2  [stat.ML]  27 Feb 2015'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\ndomain classiﬁer that discriminates between the source and\\nthe target domains during training. While the parameters of\\nthe classiﬁers are optimized in order to minimize their error\\non the training set, the parameters of the underlying deep\\nfeature mapping are optimized in order tominimize the loss\\nof the label classiﬁer and tomaximize the loss of the domain\\nclassiﬁer. The latter encourages domain-invariant features\\nto emerge in the course of the optimization.\\nCrucially, we show that all three training processes can\\nbe embedded into an appropriately composed deep feed-\\nforward network (Figure 1) that uses standard layers and\\nloss functions, and can be trained using standard backprop-\\nagation algorithms based on stochastic gradient descent or\\nits modiﬁcations (e.g. SGD with momentum). Our ap-\\nproach is generic as it can be used to add domain adaptation\\nto any existing feed-forward architecture that is trainable by\\nbackpropagation. In practice, the only non-standard com-\\nponent of the proposed architecture is a rather trivial gra-\\ndient reversal layer that leaves the input unchanged during\\nforward propagation and reverses the gradient by multiply-\\ning it by a negative scalar during the backpropagation.\\nBelow, we detail the proposed approach to domain adap-\\ntation in deep architectures, and present results on tradi-\\ntional deep learning image datasets (such as MNIST (Le-\\nCun et al., 1998) and SVHN (Netzer et al., 2011)) as well\\nas on O FFICE benchmarks (Saenko et al., 2010), where\\nthe proposed method considerably improves over previous\\nstate-of-the-art accuracy.\\n2. Related work\\nA large number of domain adaptation methods have been\\nproposed over the recent years, and here we focus on the\\nmost related ones. Multiple methods perform unsuper-\\nvised domain adaptation by matching the feature distri-\\nbutions in the source and the target domains. Some ap-\\nproaches perform this by reweighing or selecting samples\\nfrom the source domain (Borgwardt et al., 2006; Huang\\net al., 2006; Gong et al., 2013), while others seek an ex-\\nplicit feature space transformation that would map source\\ndistribution into the target ones (Pan et al., 2011; Gopalan\\net al., 2011; Baktashmotlagh et al., 2013). An important\\naspect of the distribution matching approach is the way the\\n(dis)similarity between distributions is measured. Here,\\none popular choice is matching the distribution means in\\nthe kernel-reproducing Hilbert space (Borgwardt et al.,\\n2006; Huang et al., 2006), whereas (Gong et al., 2012; Fer-\\nnando et al., 2013) map the principal axes associated with\\neach of the distributions. Our approach also attempts to\\nmatch feature space distributions, however this is accom-\\nplished by modifying the feature representation itself rather\\nthan by reweighing or geometric transformation. Also, our\\nmethod uses (implicitly) a rather different way to measure\\nthe disparity between distributions based on their separa-\\nbility by a deep discriminatively-trained classiﬁer.\\nSeveral approaches perform gradual transition from the\\nsource to the target domain (Gopalan et al., 2011; Gong\\net al., 2012) by a gradual change of the training distribu-\\ntion. Among these methods, (S. Chopra & Gopalan, 2013)\\ndoes this in a “deep” way by the layerwise training of a\\nsequence of deep autoencoders, while gradually replacing\\nsource-domain samples with target-domain samples. This\\nimproves over a similar approach of (Glorot et al., 2011)\\nthat simply trains a single deep autoencoder for both do-\\nmains. In both approaches, the actual classiﬁer/predictor\\nis learned in a separate step using the feature representa-\\ntion learned by autoencoder(s). In contrast to (Glorot et al.,\\n2011; S. Chopra & Gopalan, 2013), our approach performs\\nfeature learning, domain adaptation and classiﬁer learning\\njointly, in a uniﬁed architecture, and using a single learning\\nalgorithm (backpropagation). We therefore argue that our\\napproach is simpler (both conceptually and in terms of its\\nimplementation). Our method also achieves considerably\\nbetter results on the popular OFFICE benchmark.\\nWhile the above approaches perform unsupervised domain\\nadaptation, there are approaches that perform supervised\\ndomain adaptation by exploiting labeled data from the tar-\\nget domain. In the context of deep feed-forward archi-\\ntectures, such data can be used to “ﬁne-tune” the net-\\nwork trained on the source domain (Zeiler & Fergus, 2013;\\nOquab et al., 2014; Babenko et al., 2014). Our approach\\ndoes not require labeled target-domain data. At the same\\ntime, it can easily incorporate such data when it is avail-\\nable.\\nAn idea related to ours is described in (Goodfellow et al.,\\n2014). While their goal is quite different (building gener-\\native deep networks that can synthesize samples), the way\\nthey measure and minimize the discrepancy between the\\ndistribution of the training data and the distribution of the\\nsynthesized data is very similar to the way our architecture\\nmeasures and minimizes the discrepancy between feature\\ndistributions for the two domains.\\nFinally, a recent and concurrent report by (Tzeng et al.,\\n2014) also focuses on domain adaptation in feed-forward\\nnetworks. Their set of techniques measures and minimizes\\nthe distance of the data means across domains. This ap-\\nproach may be regarded as a “ﬁrst-order” approximation\\nto our approach, which seeks a tighter alignment between\\ndistributions.\\n3. Deep Domain Adaptation\\n3.1. The model\\nWe now detail the proposed model for the domain adap-\\ntation. We assume that the model works with input sam-\\nples x ∈ X, where X is some input space and cer-\\ntain labels (output) y from the label space Y. Below,\\nwe assume classiﬁcation problems where Y is a ﬁnite set\\n(Y = {1,2,...L }), however our approach is generic and\\ncan handle any output label space that other deep feed-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nFigure 1.The proposed architecture includes a deep feature extractor (green) and a deep label predictor (blue), which together form\\na standard feed-forward architecture. Unsupervised domain adaptation is achieved by adding a domain classiﬁer (red) connected to the\\nfeature extractor via a gradient reversal layer that multiplies the gradient by a certain negative constant during the backpropagation-\\nbased training. Otherwise, the training proceeds in a standard way and minimizes the label prediction loss (for source examples) and\\nthe domain classiﬁcation loss (for all samples). Gradient reversal ensures that the feature distributions over the two domains are made\\nsimilar (as indistinguishable as possible for the domain classiﬁer), thus resulting in the domain-invariant features.\\nforward models can handle. We further assume that there\\nexist two distributions S(x,y) and T(x,y) on X ⊗Y,\\nwhich will be referred to as the source distribution and\\nthe target distribution (or the source domain and the tar-\\nget domain). Both distributions are assumed complex and\\nunknown, and furthermore similar but different (in other\\nwords, Sis “shifted” from T by some domain shift).\\nOur ultimate goal is to be able to predict labels y given\\nthe input x for the target distribution. At training time,\\nwe have an access to a large set of training samples\\n{x1,x2,..., xN}from both the source and the target do-\\nmains distributed according to the marginal distributions\\nS(x) and T(x). We denote with di the binary variable (do-\\nmain label) for the i-th example, which indicates whether\\nxi come from the source distribution (xi∼S(x) if di=0) or\\nfrom the target distribution (xi∼T(x) if di=1). For the ex-\\namples from the source distribution (di=0) the correspond-\\ning labels yi ∈Y are known at training time. For the ex-\\namples from the target domains, we do not know the labels\\nat training time, and we want to predict such labels at test\\ntime.\\nWe now deﬁne a deep feed-forward architecture that for\\neach input x predicts its label y ∈Y and its domain label\\nd ∈{0,1}. We decompose such mapping into three parts.\\nWe assume that the input x is ﬁrst mapped by a mapping\\nGf (a feature extractor) to a D-dimensional feature vector\\nf ∈RD. The feature mapping may also include several\\nfeed-forward layers and we denote the vector of parame-\\nters of all layers in this mapping as θf, i.e. f = Gf(x; θf).\\nThen, the feature vector f is mapped by a mapping Gy (la-\\nbel predictor) to the label y, and we denote the parameters\\nof this mapping with θy. Finally, the same feature vector f\\nis mapped to the domain label dby a mapping Gd (domain\\nclassiﬁer) with the parameters θd (Figure 1).\\nDuring the learning stage, we aim to minimize the label\\nprediction loss on the annotated part (i.e. the source part)\\nof the training set, and the parameters of both the feature\\nextractor and the label predictor are thus optimized in or-\\nder to minimize the empirical loss for the source domain\\nsamples. This ensures the discriminativeness of the fea-\\ntures f and the overall good prediction performance of the\\ncombination of the feature extractor and the label predictor\\non the source domain.\\nAt the same time, we want to make the features f\\ndomain-invariant. That is, we want to make the dis-\\ntributions S(f) = {Gf(x; θf) |x∼S(x)}and T(f) =\\n{Gf(x; θf) |x∼T(x)}to be similar. Under the covariate\\nshift assumption, this would make the label prediction ac-\\ncuracy on the target domain to be the same as on the source\\ndomain (Shimodaira, 2000). Measuring the dissimilarity\\nof the distributions S(f) and T(f) is however non-trivial,\\ngiven that f is high-dimensional, and that the distributions\\nthemselves are constantly changing as learning progresses.\\nOne way to estimate the dissimilarity is to look at the loss\\nof the domain classiﬁer Gd, provided that the parameters\\nθd of the domain classiﬁer have been trained to discrim-\\ninate between the two feature distributions in an optimal\\nway.\\nThis observation leads to our idea. At training time, in or-\\nder to obtain domain-invariant features, we seek the param-\\neters θf of the feature mapping that maximize the loss of\\nthe domain classiﬁer (by making the two feature distribu-\\ntions as similar as possible), while simultaneously seeking\\nthe parameters θd of the domain classiﬁer that minimize the\\nloss of the domain classiﬁer. In addition, we seek to mini-\\nmize the loss of the label predictor.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nMore formally, we consider the functional:\\nE(θf,θy,θd) =\\n∑\\ni=1..N\\ndi=0\\nLy\\n(\\nGy(Gf(xi; θf); θy),yi\\n)\\n−\\nλ\\n∑\\ni=1..N\\nLd\\n(\\nGd(Gf(xi; θf); θd),yi\\n)\\n=\\n=\\n∑\\ni=1..N\\ndi=0\\nLi\\ny(θf,θy) −λ\\n∑\\ni=1..N\\nLi\\nd(θf,θd) (1)\\nHere, Ly(·,·) is the loss for label prediction (e.g. multino-\\nmial), Ld(·,·) is the loss for the domain classiﬁcation (e.g.\\nlogistic), while Li\\ny and Li\\nd denote the corresponding loss\\nfunctions evaluated at the i-th training example.\\nBased on our idea, we are seeking the parametersˆθf,ˆθy,ˆθd\\nthat deliver a saddle point of the functional (1):\\n(ˆθf,ˆθy) = arg min\\nθf ,θy\\nE(θf,θy,ˆθd) (2)\\nˆθd = arg max\\nθd\\nE(ˆθf,ˆθy,θd) . (3)\\nAt the saddle point, the parametersθd of the domain classi-\\nﬁer θd minimize the domain classiﬁcation loss (since it en-\\nters into (1) with the minus sign) while the parametersθy of\\nthe label predictor minimize the label prediction loss. The\\nfeature mapping parameters θf minimize the label predic-\\ntion loss (i.e. the features are discriminative), while maxi-\\nmizing the domain classiﬁcation loss (i.e. the features are\\ndomain-invariant). The parameter λcontrols the trade-off\\nbetween the two objectives that shape the features during\\nlearning.\\nBelow, we demonstrate that standard stochastic gradient\\nsolvers (SGD) can be adapted for the search of the saddle\\npoint (2)-(3).\\n3.2. Optimization with backpropagation\\nA saddle point (2)-(3) can be found as a stationary point of\\nthe following stochastic updates:\\nθf ←− θf −µ\\n(\\n∂Li\\ny\\n∂θf\\n−λ∂Li\\nd\\n∂θf\\n)\\n(4)\\nθy ←− θy −µ∂Li\\ny\\n∂θy\\n(5)\\nθd ←− θd −µ∂Li\\nd\\n∂θd\\n(6)\\nwhere µis the learning rate (which can vary over time).\\nThe updates (4)-(6) are very similar to stochastic gradient\\ndescent (SGD) updates for a feed-forward deep model that\\ncomprises feature extractor fed into the label predictor and\\ninto the domain classiﬁer. The difference is the −λfactor\\nin (4) (the difference is important, as without such factor,\\nstochastic gradient descent would try to make features dis-\\nsimilar across domains in order to minimize the domain\\nclassiﬁcation loss). Although direct implementation of (4)-\\n(6) as SGD is not possible, it is highly desirable to reduce\\nthe updates (4)-(6) to some form of SGD, since SGD (and\\nits variants) is the main learning algorithm implemented in\\nmost packages for deep learning.\\nFortunately, such reduction can be accomplished by intro-\\nducing a special gradient reversal layer (GRL) deﬁned as\\nfollows. The gradient reversal layer has no parameters as-\\nsociated with it (apart from the meta-parameter λ, which\\nis not updated by backpropagation). During the forward\\npropagation, GRL acts as an identity transform. During\\nthe backpropagation though, GRL takes the gradient from\\nthe subsequent level, multiplies it by −λ and passes it to\\nthe preceding layer. Implementing such layer using exist-\\ning object-oriented packages for deep learning is simple, as\\ndeﬁning procedures for forwardprop (identity transform),\\nbackprop (multiplying by a constant), and parameter up-\\ndate (nothing) is trivial.\\nThe GRL as deﬁned above is inserted between the feature\\nextractor and the domain classiﬁer, resulting in the archi-\\ntecture depicted in Figure 1. As the backpropagation pro-\\ncess passes through the GRL, the partial derivatives of the\\nloss that is downstream the GRL (i.e. Ld) w.r.t. the layer\\nparameters that are upstream the GRL (i.e. θf) get multi-\\nplied by −λ, i.e. ∂Ld\\n∂θf\\nis effectively replaced with −λ∂Ld\\n∂θf\\n.\\nTherefore, running SGD in the resulting model implements\\nthe updates (4)-(6) and converges to a saddle point of (1).\\nMathematically, we can formally treat the gradient reversal\\nlayer as a “pseudo-function”Rλ(x) deﬁned by two (incom-\\npatible) equations describing its forward- and backpropa-\\ngation behaviour:\\nRλ(x) = x (7)\\ndRλ\\ndx = −λI (8)\\nwhere I is an identity matrix. We can then deﬁne the\\nobjective “pseudo-function” of (θf,θy,θd) that is being\\noptimized by the stochastic gradient descent within our\\nmethod:\\n˜E(θf,θy,θd) =\\n∑\\ni=1..N\\ndi=0\\nLy\\n(\\nGy(Gf(xi; θf); θy),yi\\n)\\n+\\n∑\\ni=1..N\\nLd\\n(\\nGd(Rλ(Gf(xi; θf)); θd),yi\\n)\\n(9)\\nRunning updates (4)-(6) can then be implemented as do-\\ning SGD for (9) and leads to the emergence of features\\nthat are domain-invariant and discriminative at the same\\ntime. After the learning, the label predictor y(x) =\\nGy(Gf(x; θf); θy) can be used to predict labels for sam-\\nples from the target domain (as well as from the source\\ndomain).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nThe simple learning procedure outlined above can be re-\\nderived/generalized along the lines suggested in (Goodfel-\\nlow et al., 2014) (see Appendix A).\\n3.3. Relation to H∆H-distance\\nIn this section we give a brief analysis of our method in\\nterms of H∆H-distance (Ben-David et al., 2010; Cortes &\\nMohri, 2011) which is widely used in the theory of non-\\nconservative domain adaptation. Formally,\\ndH∆H(S,T) = 2 sup\\nh1,h2∈H\\n|Pf ∼S[h1(f) ̸= h2(f)]−\\n−Pf ∼T[h1(f) ̸= h2(f)]| (10)\\ndeﬁnes a discrepancy distance between two distributions S\\nand T w.r.t. a hypothesis set H. Using this notion one can\\nobtain a probabilistic bound (Ben-David et al., 2010) on the\\nperformance εT(h) of some classiﬁer hfrom T evaluated\\non the target domain given its performance εS(h) on the\\nsource domain:\\nεT(h) ≤εS(h) + 1\\n2dH∆H(S,T) + C, (11)\\nwhere Sand T are source and target distributions respec-\\ntively, and Cdoes not depend on particular h.\\nConsider ﬁxed Sand T over the representation space pro-\\nduced by the feature extractor Gf and a family of label\\npredictors Hp. We assume that the family of domain classi-\\nﬁers Hd is rich enough to contain the symmetric difference\\nhypothesis set of Hp:\\nHp∆Hp = {h|h= h1 ⊕h2 , h1,h2 ∈Hp}. (12)\\nIt is not an unrealistic assumption as we have a freedom to\\npick Hd whichever we want. For example, we can set the\\narchitecture of the domain discriminator to be the layer-\\nby-layer concatenation of two replicas of the label predic-\\ntor followed by a two layer non-linear perceptron aimed to\\nlearn the XOR-function. Given the assumption holds, one\\ncan easily show that training the Gd is closely related to\\nthe estimation of dHp∆Hp (S,T). Indeed,\\ndHp∆Hp (S,T) =\\n= 2 sup\\nh∈Hp∆Hp\\n|Pf ∼S[h(f) = 1] −Pf ∼T[h(f) = 1]|≤\\n≤2 sup\\nh∈Hd\\n|Pf ∼S[h(f) = 1] −Pf ∼T[h(f) = 1]|=\\n= 2 sup\\nh∈Hd\\n|1 −α(h)|= 2 sup\\nh∈Hd\\n[α(h) −1]\\n(13)\\nwhere α(h) = Pf ∼S[h(f) = 0] + Pf ∼T[h(f) = 1] is max-\\nimized by the optimal Gd.\\nThus, optimal discriminator gives the upper bound for\\ndHp∆Hp (S,T). At the same time, backpropagation of\\nthe reversed gradient changes the representation space\\nso that α(Gd) becomes smaller effectively reducing\\ndHp∆Hp (S,T) and leading to the better approximation of\\nεT(Gy) by εS(Gy).\\n4. Experiments\\nWe perform extensive evaluation of the proposed approach\\non a number of popular image datasets and their modiﬁ-\\ncations. These include large-scale datasets of small im-\\nages popular with deep learning methods, and the O FFICE\\ndatasets (Saenko et al., 2010), which are ade facto standard\\nfor domain adaptation in computer vision, but have much\\nfewer images.\\nBaselines. For the bulk of experiments the following base-\\nlines are evaluated. The source-only model is trained with-\\nout consideration for target-domain data (no domain clas-\\nsiﬁer branch included into the network). The train-on-\\ntarget model is trained on the target domain with class\\nlabels revealed. This model serves as an upper bound on\\nDA methods, assuming that target data are abundant and\\nthe shift between the domains is considerable.\\nIn addition, we compare our approach against the recently\\nproposed unsupervised DA method based on subspace\\nalignment (SA) (Fernando et al., 2013), which is simple\\nto setup and test on new datasets, but has also been shown\\nto perform very well in experimental comparisons with\\nother “shallow” DA methods. To boost the performance\\nof this baseline, we pick its most important free parame-\\nter (the number of principal components) from the range\\n{2,..., 60}, so that the test performance on the target do-\\nmain is maximized. To apply SA in our setting, we train\\na source-only model and then consider the activations of\\nthe last hidden layer in the label predictor (before the ﬁnal\\nlinear classiﬁer) as descriptors/features, and learn the map-\\nping between the source and the target domains (Fernando\\net al., 2013).\\nSince the SA baseline requires to train a new classiﬁer after\\nadapting the features, and in order to put all the compared\\nsettings on an equal footing, we retrain the last layer of\\nthe label predictor using a standard linear SVM (Fan et al.,\\n2008) for all four considered methods (including ours; the\\nperformance on the target domain remains approximately\\nthe same after the retraining).\\nFor the O FFICE dataset (Saenko et al., 2010), we directly\\ncompare the performance of our full network (feature ex-\\ntractor and label predictor) against recent DA approaches\\nusing previously published results.\\nCNN architectures. In general, we compose feature ex-\\ntractor from two or three convolutional layers, picking their\\nexact conﬁgurations from previous works. We give the ex-\\nact architectures in Appendix B.\\nFor the domain adaptator we stick to the three fully con-\\nnected layers ( x → 1024 → 1024 → 2), except for\\nMNIST where we used a simpler ( x → 100 → 2) ar-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nMNIST S YN NUMBERS SVHN S YN SIGNS\\nSOURCE\\nTARGET\\nMNIST-M SVHN MNIST GTSRB\\nFigure 2.Examples of domain pairs used in the experiments. See Section 4.1 for details.\\nMETHOD\\nSOURCE MNIST S YN NUMBERS SVHN S YN SIGNS\\nTARGET MNIST-M SVHN MNIST GTSRB\\nSOURCE ONLY .5749 .8665 .5919 .7400\\nSA (F ERNANDO ET AL ., 2013) .6078 (7.9%) .8672 (1.3%) .6157 (5.9%) .7635 (9.1%)\\nPROPOSED APPROACH .8149 (57.9%) .9048 (66.1%) .7107 (29.3%) .8866 (56.7%)\\nTRAIN ON TARGET .9891 .9244 .9951 .9987\\nTable 1.Classiﬁcation accuracies for digit image classiﬁcations for different source and target domains. MNIST-M corresponds to\\ndifference-blended digits over non-uniform background. The ﬁrst row corresponds to the lower performance bound (i.e. if no adaptation\\nis performed). The last row corresponds to training on the target domain data with known class labels (upper bound on the DA perfor-\\nmance). For each of the two DA methods (ours and (Fernando et al., 2013)) we show how much of the gap between the lower and the\\nupper bounds was covered (in brackets). For all ﬁve cases, our approach outperforms (Fernando et al., 2013) considerably, and covers a\\nbig portion of the gap.\\nchitecture to speed up the experiments.\\nFor loss functions, we set Ly and Ld to be the logistic re-\\ngression loss and the binomial cross-entropy respectively.\\nCNN training procedure. The model is trained on 128-\\nsized batches. Images are preprocessed by the mean sub-\\ntraction. A half of each batch is populated by the sam-\\nples from the source domain (with known labels), the rest\\nis comprised of the target domain (with unknown labels).\\nIn order to suppress noisy signal from the domain classiﬁer\\nat the early stages of the training procedure instead of ﬁxing\\nthe adaptation factor λ, we gradually change it from 0 to 1\\nusing the following schedule:\\nλp = 2\\n1 + exp(−γ·p) −1, (14)\\nwhere γwas set to 10 in all experiments (the schedule was\\nnot optimized/tweaked). Further details on the CNN train-\\ning can be found in Appendix C.\\nVisualizations. We use t-SNE (van der Maaten, 2013) pro-\\njection to visualize feature distributions at different points\\nof the network, while color-coding the domains (Figure 3).\\nWe observe strong correspondence between the success of\\nthe adaptation in terms of the classiﬁcation accuracy for the\\ntarget domain, and the overlap between the domain distri-\\nbutions in such visualizations.\\nChoosing meta-parameters. In general, good unsu-\\npervised DA methods should provide ways to set meta-\\nparameters (such as λ, the learning rate, the momentum\\nrate, the network architecture for our method) in an unsu-\\npervised way, i.e. without referring to labeled data in the\\ntarget domain. In our method, one can assess the per-\\nformance of the whole system (and the effect of chang-\\ning hyper-parameters) by observing the test error on the\\nsource domain and the domain classiﬁer error. In general,\\nwe observed a good correspondence between the success of\\nadaptation and these errors (adaptation is more successful\\nwhen the source domain test error is low, while the domain\\nclassiﬁer error is high). In addition, the layer, where the\\nthe domain adaptator is attached can be picked by comput-\\ning difference between means as suggested in (Tzeng et al.,\\n2014).\\n4.1. Results\\nWe now discuss the experimental settings and the results.\\nIn each case, we train on the source dataset and test on a\\ndifferent target domain dataset, with considerable shifts be-\\ntween domains (see Figure 2). The results are summarized\\nin Table 1 and Table 2.\\nMNIST →MNIST-M. Our ﬁrst experiment deals with\\nthe MNIST dataset (LeCun et al., 1998) (source). In or-\\nder to obtain the target domain (MNIST-M) we blend dig-\\nits from the original set over patches randomly extracted\\nfrom color photos from BSDS500 (Arbelaez et al., 2011).\\nThis operation is formally deﬁned for two images I1,I2 as\\nIout\\nijk = |I1\\nijk −I2\\nijk|, where i,j are the coordinates of a\\npixel and k is a channel index. In other words, an output\\nsample is produced by taking a patch from a photo and in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nMETHOD\\nSOURCE AMAZON DSLR W EBCAM\\nTARGET WEBCAM WEBCAM DSLR\\nGFK(PLS, PCA) (G ONG ET AL ., 2012) .464 ±.005 .613 ±.004 .663 ±.004\\nSA (F ERNANDO ET AL ., 2013) .450 .648 .699\\nDA-NBNN (T OMMASI & CAPUTO , 2013) .528 ±.037 .766 ±.017 .762 ±.025\\nDLID (S. C HOPRA & GOPALAN , 2013) .519 .782 .899\\nDECAF6 SOURCE ONLY (DONAHUE ET AL ., 2014) .522 ±.017 .915 ±.015 –\\nDANN (G HIFARY ET AL ., 2014) .536 ±.002 .712 ±.000 .835 ±.000\\nDDC (T ZENG ET AL ., 2014) .594 ±.008 .925 ±.003 .917 ±.008\\nPROPOSED APPROACH .673 ±.017 .940 ±.008 .937 ±.010\\nTable 2.Accuracy evaluation of different DA approaches on the standard O FFICE (Saenko et al., 2010) dataset. Our method (last row)\\noutperforms competitors setting the new state-of-the-art.\\nMNIST →MNIST-M: top feature extractor layer\\n(a) Non-adapted\\n (b) Adapted\\nSYN NUMBERS →SVHN: last hidden layer of the label predictor\\n(a) Non-adapted\\n (b) Adapted\\nFigure 3.The effect of adaptation on the distribution of the extracted features (best viewed in color). The ﬁgure shows t-SNE (van der\\nMaaten, 2013) visualizations of the CNN’s activations(a) in case when no adaptation was performed and(b) in case when our adaptation\\nprocedure was incorporated into training. Blue points correspond to the source domain examples, whilered ones correspond to the target\\ndomain. In all cases, the adaptation in our method makes the two distributions of features much closer.\\nverting its pixels at positions corresponding to the pixels of\\na digit. For a human the classiﬁcation task becomes only\\nslightly harder compared to the original dataset (the digits\\nare still clearly distinguishable) whereas for a CNN trained\\non MNIST this domain is quite distinct, as the background\\nand the strokes are no longer constant. Consequently, the\\nsource-only model performs poorly. Our approach suc-\\nceeded at aligning feature distributions (Figure 3), which\\nled to successful adaptation results (considering that the\\nadaptation is unsupervised). At the same time, the im-\\nprovement over source-only model achieved by subspace\\nalignment (SA) (Fernando et al., 2013) is quite modest,\\nthus highlighting the difﬁculty of the adaptation task.\\nSynthetic numbers →SVHN. To address a common sce-\\nnario of training on synthetic data and testing on real data,\\nwe use Street-View House Number dataset SVHN (Netzer\\net al., 2011) as the target domain and synthetic digits as the\\nsource. The latter (S YN NUMBERS ) consists of 500,000\\nimages generated by ourselves from Windows fonts by\\nvarying the text (that includes different one-, two-, and\\nthree-digit numbers), positioning, orientation, background\\nand stroke colors, and the amount of blur. The degrees of\\nvariation were chosen manually to simulate SVHN, how-\\never the two datasets are still rather distinct, the biggest\\ndifference being the structured clutter in the background of\\nSVHN images.\\nThe proposed backpropagation-based technique works well\\ncovering two thirds of the gap between training with source\\ndata only and training on target domain data with known\\ntarget labels. In contrast, SA (Fernando et al., 2013) does\\nnot result in any signiﬁcant improvement in the classiﬁca-\\ntion accuracy, thus highlighting that the adaptation task is\\neven more challenging than in the case of the MNIST ex-\\nperiment.\\nMNIST ↔SVHN. In this experiment, we further increase\\nthe gap between distributions, and test on MNIST and\\nSVHN, which are signiﬁcantly different in appearance.\\nTraining on SVHN even without adaptation is challeng-\\ning — classiﬁcation error stays high during the ﬁrst 150\\nepochs. In order to avoid ending up in a poor local min-\\nimum we, therefore, do not use learning rate annealing\\nhere. Obviously, the two directions (MNIST →SVHN\\nand SVHN → MNIST) are not equally difﬁcult. As\\nSVHN is more diverse, a model trained on SVHN is ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\n1 2 3 4 5\\n·104\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nBatches seen\\nValidation error\\nReal data only\\nSynthetic data only\\nBoth\\nFigure 4.Semi-supervised domain adaptation for the trafﬁc signs.\\nAs labeled target domain data are shown to the method, it achieves\\nsigniﬁcantly lower error than the model trained on target domain\\ndata only or on source domain data only.\\npected to be more generic and to perform reasonably on\\nthe MNIST dataset. This, indeed, turns out to be the case\\nand is supported by the appearance of the feature distribu-\\ntions. We observe a quite strong separation between the\\ndomains when we feed them into the CNN trained solely\\non MNIST, whereas for the SVHN-trained network the\\nfeatures are much more intermixed. This difference prob-\\nably explains why our method succeeded in improving the\\nperformance by adaptation in the SVHN →MNIST sce-\\nnario (see Table 1) but not in the opposite direction (SA is\\nnot able to perform adaptation in this case either). Unsu-\\npervised adaptation from MNIST to SVHN gives a failure\\nexample for our approach (we are unaware of any unsuper-\\nvised DA methods capable of performing such adaptation).\\nSynthetic Signs →GTSRB. Overall, this setting is sim-\\nilar to the S YN NUMBERS →SVHN experiment, except\\nthe distribution of the features is more complex due to the\\nsigniﬁcantly larger number of classes (43 instead of 10).\\nFor the source domain we obtained 100,000 synthetic im-\\nages (which we call S YN SIGNS ) simulating various pho-\\ntoshooting conditions. Once again, our method achieves\\na sensible increase in performance once again proving its\\nsuitability for the synthetic-to-real data adaptation.\\nAs an additional experiment, we also evaluate the pro-\\nposed algorithm for semi-supervised domain adaptation,\\ni.e. when one is additionally provided with a small amount\\nof labeled target data. For that purpose we split GTSRB\\ninto the train set (1280 random samples with labels) and\\nthe validation set (the rest of the dataset). The validation\\npart is used solely for the evaluation and does not partic-\\nipate in the adaptation. The training procedure changes\\nslightly as the label predictor is now exposed to the tar-\\nget data. Figure 4 shows the change of the validation error\\nthroughout the training. While the graph clearly suggests\\nthat our method can be used in the semi-supervised setting,\\nthorough veriﬁcation of semi-supervised setting is left for\\nfuture work.\\nOfﬁce dataset. We ﬁnally evaluate our method on O F-\\nFICE dataset, which is a collection of three distinct do-\\nmains: A MAZON , DSLR, and W EBCAM . Unlike previ-\\nously discussed datasets, OFFICE is rather small-scale with\\nonly 2817 labeled images spread across 31 different cat-\\negories in the largest domain. The amount of available\\ndata is crucial for a successful training of a deep model,\\nhence we opted for the ﬁne-tuning of the CNN pre-trained\\non the ImageNet (Jia et al., 2014) as it is done in some re-\\ncent DA works (Donahue et al., 2014; Tzeng et al., 2014;\\nHoffman et al., 2013). We make our approach more com-\\nparable with (Tzeng et al., 2014) by using exactly the same\\nnetwork architecture replacing domain mean-based regu-\\nlarization with the domain classiﬁer.\\nFollowing most previous works, we evaluate our method\\nusing 5 random splits for each of the 3 transfer tasks com-\\nmonly used for evaluation. Our training protocol is close to\\n(Tzeng et al., 2014; Saenko et al., 2010; Gong et al., 2012)\\nas we use the same number of labeled source-domain im-\\nages per category. Unlike those works and similarly to e.g.\\nDLID (S. Chopra & Gopalan, 2013) we use the whole un-\\nlabeled target domain (as the premise of our method is the\\nabundance of unlabeled data in the target domain). Un-\\nder this transductive setting, our method is able to improve\\npreviously-reported state-of-the-art accuracy for unsuper-\\nvised adaptation very considerably (Table 2), especially in\\nthe most challenging AMAZON →WEBCAM scenario (the\\ntwo domains with the largest domain shift).\\n5. Discussion\\nWe have proposed a new approach to unsupervised do-\\nmain adaptation of deep feed-forward architectures, which\\nallows large-scale training based on large amount of an-\\nnotated data in the source domain and large amount of\\nunannotated data in the target domain. Similarly to many\\nprevious shallow and deep DA techniques, the adaptation\\nis achieved through aligning the distributions of features\\nacross the two domains. However, unlike previous ap-\\nproaches, the alignment is accomplished through standard\\nbackpropagation training. The approach is therefore rather\\nscalable, and can be implemented using any deep learning\\npackage. To this end we plan to release the source code for\\nthe Gradient Reversal layer along with the usage examples\\nas an extension to Caffe (Jia et al., 2014).\\nFurther evaluation on larger-scale tasks and in semi-\\nsupervised settings constitutes future work. It is also in-\\nteresting whether the approach can beneﬁt from a good ini-\\ntialization of the feature extractor. For this, a natural choice\\nwould be to use deep autoencoder/deconvolution network\\ntrained on both domains (or on the target domain) in the\\nsame vein as (Glorot et al., 2011; S. Chopra & Gopalan,\\n2013), effectively using (Glorot et al., 2011; S. Chopra &\\nGopalan, 2013) as an initialization to our method.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nAppendix A. An alternative optimization\\napproach\\nThere exists an alternative construction (inspired by (Good-\\nfellow et al., 2014)) that leads to the same updates (4)-(6).\\nRather than using the gradient reversal layer, the construc-\\ntion introduces two different loss functions for the domain\\nclassiﬁer. Minimization of the ﬁrst domain loss ( Ld+)\\nshould lead to a better domain discrimination, while the\\nsecond domain loss (Ld−) is minimized when the domains\\nare distinct. Stochastic updates for θf and θd are then de-\\nﬁned as:\\nθf ←− θf −µ\\n(\\n∂Li\\ny\\n∂θf\\n+ ∂Li\\nd−\\n∂θf\\n)\\nθd ←− θd −µ∂Li\\nd+\\n∂θd\\n,\\nThus, different parameters participate in the optimization\\nof different losses\\nIn this framework, the gradient reversal layer constitutes\\na special case, corresponding to the pair of domain losses\\n(Ld,−λLd). However, other pairs of loss functions can be\\nused. One example would be the binomial cross-entropy\\n(Goodfellow et al., 2014):\\nLd+(q,d) =\\n∑\\ni=1..N\\ndilog(qi) + (1−di) log(1−qi) ,\\nwhere dindicates domain indices and qis an output of the\\npredictor. In that case “adversarial” loss is easily obtained\\nby swapping domain labels, i.e.Ld−(q,d) = Ld+(q,1−d).\\nThis particular pair has a potential advantage of produc-\\ning stronger gradients at early learning stages if the do-\\nmains are quite dissimilar. In our experiments, however,\\nwe did not observe any signiﬁcant improvement resulting\\nfrom this choice of losses.\\nAppendix B. CNN architectures\\nFour different architectures were used in our experiments\\n(ﬁrst three are shown in Figure 5):\\n• A smaller one (a) if the source domain is MNIST. This\\narchitecture was inspired by the classical LeNet-5 (Le-\\nCun et al., 1998).\\n• (b) for the experiments involving SVHN dataset. This\\none is adopted from (Srivastava et al., 2014).\\n• (c) in the S YN SINGS →GTSRB setting. We used\\nthe single-CNN baseline from (Cires ¸an et al., 2012)\\nas our starting point.\\n• Finally, we use pre-trained AlexNet from the\\nCaffe-package (Jia et al., 2014) for the O FFICE do-\\nmains. Adaptation architecture is identical to (Tzeng\\net al., 2014): 2-layer domain classiﬁer ( x→1024 →\\n1024 →2) is attached to the 256-dimensional bottle-\\nneck of fc7.\\nThe domain classiﬁer branch in all cases is somewhat ar-\\nbitrary (better adaptation performance might be attained if\\nthis part of the architecture is tuned).\\nAppendix C. Training procedure\\nWe use stochastic gradient descent with 0.9 momentum and\\nthe learning rate annealing described by the following for-\\nmula:\\nµp = µ0\\n(1 + α·p)β ,\\nwhere p is the training progress linearly changing from 0\\nto 1, µ0 = 0 .01, α = 10 and β = 0 .75 (the schedule\\nwas optimized to promote convergence and low error on\\nthe source domain).\\nFollowing (Srivastava et al., 2014) we also use dropout and\\nℓ2-norm restriction when we train the SVHN architecture.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nconv 5x532 mapsReLU\\nmax-pool 2x22x2 stride\\nconv 5x548 mapsReLU\\nmax-pool 2x22x2 stride\\nfully-conn100 unitsReLU\\nfully-conn100 unitsReLU\\nfully-conn10 unitsSoft-max\\nGRL\\n fully-conn100 unitsReLU\\nfully-conn1 unitLogistic\\n(a) MNIST architecture\\nconv 5x564 mapsReLU\\nmax-pool 3x32x2 stride\\nconv 5x564 mapsReLU\\nmax-pool 3x32x2 stride\\nconv 5x5128 mapsReLU\\nfully-conn3072 unitsReLU\\nfully-conn2048 unitsReLU\\nfully-conn10 unitsSoft-max\\nGRL\\n fully-conn1024 unitsReLU\\nfully-conn1024 unitsReLU\\nfully-conn1 unitLogistic\\n(b) SVHN architecture\\nconv 5x596 mapsReLU\\nmax-pool 2x22x2 stride\\nconv 3x3144 mapsReLU\\nmax-pool 2x22x2 stride\\nconv 5x5256 mapsReLU\\nmax-pool 2x22x2 stride\\nfully-conn512 unitsReLU\\nfully-conn10 unitsSoft-max\\nGRL\\n fully-conn1024 unitsReLU\\nfully-conn1024 unitsReLU\\nfully-conn1 unitLogistic\\n(c) GTSRB architecture\\nFigure 5.CNN architectures used in the experiments. Boxes correspond to transformations applied to the data. Color-coding is the same\\nas in Figure 1.\\nReferences\\nArbelaez, Pablo, Maire, Michael, Fowlkes, Charless, and\\nMalik, Jitendra. Contour detection and hierarchical im-\\nage segmentation. PAMI, 33, 2011.\\nBabenko, Artem, Slesarev, Anton, Chigorin, Alexander,\\nand Lempitsky, Victor S. Neural codes for image re-\\ntrieval. In ECCV, pp. 584–599, 2014.\\nBaktashmotlagh, Mahsa, Harandi, Mehrtash Tafazzoli,\\nLovell, Brian C., and Salzmann, Mathieu. Unsupervised\\ndomain adaptation by domain invariant projection. In\\nICCV, pp. 769–776, 2013.\\nBen-David, Shai, Blitzer, John, Crammer, Koby, Kulesza,\\nAlex, Pereira, Fernando, and Vaughan, Jennifer Wort-\\nman. A theory of learning from different domains.\\nJMLR, 79, 2010.\\nBorgwardt, Karsten M., Gretton, Arthur, Rasch, Malte J.,\\nKriegel, Hans-Peter, Sch ¨olkopf, Bernhard, and Smola,\\nAlexander J. Integrating structured biological data by\\nkernel maximum mean discrepancy. In ISMB, pp. 49–\\n57, 2006.\\nCires ¸an, Dan, Meier, Ueli, Masci, Jonathan, and Schmid-\\nhuber, J ¨urgen. Multi-column deep neural network for\\ntrafﬁc sign classiﬁcation. Neural Networks, (32):333–\\n338, 2012.\\nCortes, Corinna and Mohri, Mehryar. Domain adaptation\\nin regression. In Algorithmic Learning Theory, 2011.\\nDonahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman,\\nJudy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. De-\\ncaf: A deep convolutional activation feature for generic\\nvisual recognition, 2014.\\nFan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,\\nXiang-Rui, and Lin, Chih-Jen. LIBLINEAR: A library\\nfor large linear classiﬁcation. Journal of Machine Learn-\\ning Research, 9:1871–1874, 2008.\\nFernando, Basura, Habrard, Amaury, Sebban, Marc, and\\nTuytelaars, Tinne. Unsupervised visual domain adapta-\\ntion using subspace alignment. In ICCV, 2013.\\nGhifary, Muhammad, Kleijn, W Bastiaan, and Zhang,\\nMengjie. Domain adaptive neural networks for object\\nrecognition. In PRICAI 2014: Trends in Artiﬁcial Intel-\\nligence. 2014.\\nGlorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Do-\\nmain adaptation for large-scale sentiment classiﬁcation:\\nA deep learning approach. In ICML, pp. 513–520, 2011.\\nGong, Boqing, Shi, Yuan, Sha, Fei, and Grauman, Kristen.\\nGeodesic ﬂow kernel for unsupervised domain adapta-\\ntion. In CVPR, pp. 2066–2073, 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nGong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\\nnecting the dots with landmarks: Discriminatively learn-\\ning domain-invariant features for unsupervised domain\\nadaptation. In ICML, pp. 222–230, 2013.\\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\\nBing, Warde-Farley, David, Ozair, Sherjil, Courville,\\nAaron, and Bengio, Yoshua. Generative adversarial nets.\\nIn NIPS, 2014.\\nGopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\\nDomain adaptation for object recognition: An unsuper-\\nvised approach. In ICCV, pp. 999–1006, 2011.\\nHoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\\nSaenko, Kate, and Darrell, Trevor. One-shot adapta-\\ntion of supervised deep convolutional models. CoRR,\\nabs/1312.6204, 2013.\\nHuang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\\nBorgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\\nrecting sample selection bias by unlabeled data. InNIPS,\\npp. 601–608, 2006.\\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev,\\nSergey, Long, Jonathan, Girshick, Ross, Guadar-\\nrama, Sergio, and Darrell, Trevor. Caffe: Convolu-\\ntional architecture for fast feature embedding. CoRR,\\nabs/1408.5093, 2014.\\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-\\nbased learning applied to document recognition. Pro-\\nceedings of the IEEE , 86(11):2278–2324, November\\n1998.\\nLiebelt, Joerg and Schmid, Cordelia. Multi-view object\\nclass detection with a 3d geometric model. In CVPR,\\n2010.\\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,\\nAlessandro, Wu, Bo, and Ng, Andrew Y . Reading dig-\\nits in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning 2011, 2011.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\\nand transferring mid-level image representations using\\nconvolutional neural networks. In CVPR, 2014.\\nPan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\\nYang, Qiang. Domain adaptation via transfer component\\nanalysis. IEEE Transactions on Neural Networks, 22(2):\\n199–210, 2011.\\nS. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\\nlearning for domain adaptation by interpolating between\\ndomains. In ICML Workshop on Challenges in Repre-\\nsentation Learning, 2013.\\nSaenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\\nTrevor. Adapting visual category models to new do-\\nmains. In ECCV, pp. 213–226. 2010.\\nShimodaira, Hidetoshi. Improving predictive inference un-\\nder covariate shift by weighting the log-likelihood func-\\ntion. Journal of Statistical Planning and Inference , 90\\n(2):227–244, October 2000.\\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\\nA simple way to prevent neural networks from overﬁt-\\nting. The Journal of Machine Learning Research, 15(1):\\n1929–1958, 2014.\\nStark, Michael, Goesele, Michael, and Schiele, Bernt. Back\\nto the future: Learning shape models from 3d CAD data.\\nIn BMVC, pp. 1–11, 2010.\\nSun, Baochen and Saenko, Kate. From virtual to reality:\\nFast adaptation of virtual object detectors to real do-\\nmains. In BMVC, 2014.\\nTommasi, Tatiana and Caputo, Barbara. Frustratingly easy\\nnbnn domain adaptation. In ICCV, 2013.\\nTzeng, Eric, Hoffman, Judy, Zhang, Ning, Saenko, Kate,\\nand Darrell, Trevor. Deep domain confusion: Maximiz-\\ning for domain invariance. CoRR, abs/1412.3474, 2014.\\nvan der Maaten, Laurens. Barnes-hut-sne. CoRR,\\nabs/1301.3342, 2013.\\nV´azquez, David, L ´opez, Antonio Manuel, Mar ´ın, Javier,\\nPonsa, Daniel, and Gomez, David Ger ´onimo. Virtual\\nand real world adaptationfor pedestrian detection. IEEE\\nTrans. Pattern Anal. Mach. Intell., 36(4):797–809, 2014.\\nZeiler, Matthew D. and Fergus, Rob. Visualizing\\nand understanding convolutional networks. CoRR,\\nabs/1311.2901, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Simultaneous Deep Transfer Across Domains and Tasks\\nEric Tzeng∗, Judy Hoffman∗, Trevor Darrell\\nUC Berkeley, EECS & ICSI\\n{etzeng,jhoffman,trevor}@eecs.berkeley.edu\\nKate Saenko\\nUMass Lowell, CS\\nsaenko@cs.uml.edu\\nAbstract\\nRecent reports suggest that a generic supervised deep\\nCNN model trained on a large-scale dataset reduces, but\\ndoes not remove, dataset bias. Fine-tuning deep models in\\na new domain can require a signiﬁcant amount of labeled\\ndata, which for many applications is simply not available.\\nWe propose a new CNN architecture to exploit unlabeled and\\nsparsely labeled target domain data. Our approach simulta-\\nneously optimizes for domain invariance to facilitate domain\\ntransfer and uses a soft label distribution matching loss to\\ntransfer information between tasks. Our proposed adapta-\\ntion method offers empirical performance which exceeds\\npreviously published results on two standard benchmark vi-\\nsual domain adaptation tasks, evaluated across supervised\\nand semi-supervised adaptation settings.\\n1. Introduction\\nConsider a group of robots trained by the manufacturer\\nto recognize thousands of common objects using standard\\nimage databases, then shipped to households around the\\ncountry. As each robot starts to operate in its own unique\\nenvironment, it is likely to have degraded performance due\\nto the shift in domain. It is clear that, given enough ex-\\ntra supervised data from the new environment, the original\\nperformance could be recovered. However, state-of-the-art\\nrecognition algorithms rely on high capacity convolutional\\nneural network (CNN) models that require millions of su-\\npervised images for initial training. Even the traditional\\napproach for adapting deep models, ﬁne-tuning [ 14, 29],\\nmay require hundreds or thousands of labeled examples for\\neach object category that needs to be adapted.\\nIt is reasonable to assume that the robot’s new owner\\nwill label a handful of examples for a few types of objects,\\nbut completely unrealistic to presume full supervision in\\nthe new environment. Therefore, we propose an algorithm\\nthat effectively adapts between the training (source) and test\\n(target) environments by utilizing both generic statistics from\\n∗ Authors contributed equally.\\n!\\n!\\nBottleMugChairLaptopKeyboard\\nBottleMugChairLaptopKeyboardBottleMugChairLaptopKeyboard\\nBottleMugChairLaptopKeyboard\\n!\\n!\\nSource domain! Target domain! \\n!\\n!\\n !\\n!\\n!\\n\\x01!\\n!\\n\\x01!\\n!\\n\\x01!\\n!\\n\\x01!\\n1. Maximize domain confusion!\\n2. Transfer task correlation!\\n!\\n!\\nFigure 1. We transfer discriminative category information from\\na source domain to a target domain via two methods. First, we\\nmaximize domain confusion by making the marginal distributions\\nof the two domains as similar as possible. Second, we transfer cor-\\nrelations between classes learned on the source examples directly to\\nthe target examples, thereby preserving the relationships between\\nclasses.\\nunlabeled data collected in the new environment as well as a\\nfew human labeled examples from a subset of the categories\\nof interest. Our approach performs transfer learning both\\nacross domains and across tasks (see Figure 1). Intuitively,\\ndomain transfer is accomplished by making the marginal\\nfeature distributions of source and target as similar to each\\nother as possible. Task transfer is enabled by transferring\\nempirical category correlations learned on the source to the\\ntarget domain. This helps to preserve relationships between\\ncategories, e.g., bottle is similar to mug but different from\\nkeyboard. Previous work proposed techniques for domain\\ntransfer with CNN models [ 12, 24] but did not utilize the\\nlearned source semantic structure for task transfer.\\nTo enable domain transfer, we use the unlabeled target\\ndata to compute an estimated marginal distribution over the\\nnew environment and explicitly optimize a feature repre-\\n1\\narXiv:1510.02192v1  [cs.CV]  8 Oct 2015'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='sentation that minimizes the distance between the source\\nand target domain distributions. Dataset bias was classi-\\ncally illustrated in computer vision by the “name the dataset”\\ngame of Torralba and Efros [31], which trained a classiﬁer\\nto predict which dataset an image originates from, thereby\\nshowing that visual datasets are biased samples of the visual\\nworld. Indeed, this turns out to be formally connected to\\nmeasures of domain discrepancy [ 21, 5]. Optimizing for\\ndomain invariance, therefore, can be considered equivalent\\nto the task of learning to predict the class labels while simul-\\ntaneously ﬁnding a representation that makes the domains\\nappear as similar as possible. This principle forms the do-\\nmain transfer component of our proposed approach. We\\nlearn deep representations by optimizing over a loss which\\nincludes both classiﬁcation error on the labeled data as well\\nas a domain confusion loss which seeks to make the domains\\nindistinguishable.\\nHowever, while maximizing domain confusion pulls the\\nmarginal distributions of the domains together, it does not\\nnecessarily align the classes in the target with those in the\\nsource. Thus, we also explicitly transfer the similarity struc-\\nture amongst categories from the source to the target and\\nfurther optimize our representation to produce the same struc-\\nture in the target domain using the few target labeled exam-\\nples as reference points. We are inspired by prior work on\\ndistilling deep models [3, 16] and extend the ideas presented\\nin these works to a domain adaptation setting. We ﬁrst com-\\npute the average output probability distribution, or “soft\\nlabel,” over the source training examples in each category.\\nThen, for each target labeled example, we directly optimize\\nour model to match the distribution over classes to the soft\\nlabel. In this way we are able to perform task adaptation by\\ntransferring information to categories with no explicit labels\\nin the target domain.\\nWe solve the two problems jointly using a new CNN ar-\\nchitecture, outlined in Figure 2. We combine a domain con-\\nfusion and softmax cross-entropy losses to train the network\\nwith the target data. Our architecture can be used to solve su-\\npervised adaptation, when a small amount of target labeled\\ndata is available from each category, and semi-supervised\\nadaptation, when a small amount of target labeled data is\\navailable from a subset of the categories. We provide a com-\\nprehensive evaluation on the popular Ofﬁce benchmark [28]\\nand the recently introduced cross-dataset collection [30] for\\nclassiﬁcation across visually distinct domains. We demon-\\nstrate that by jointly optimizing for domain confusion and\\nmatching soft labels, we are able to outperform the current\\nstate-of-the-art visual domain adaptation results.\\n2. Related work\\nThere have been many approaches proposed in recent\\nyears to solve the visual domain adaptation problem, which\\nis also commonly framed as the visual dataset bias prob-\\nlem [31]. All recognize that there is a shift in the distri-\\nbution of the source and target data representations. In\\nfact, the size of a domain shift is often measured by the\\ndistance between the source and target subspace representa-\\ntions [5, 11, 21, 25, 27]. A large number of methods have\\nsought to overcome this difference by learning a feature\\nspace transformation to align the source and target repre-\\nsentations [28, 23, 11, 15]. For the supervised adaptation\\nscenario, when a limited amount of labeled data is available\\nin the target domain, some approaches have been proposed\\nto learn a target classiﬁer regularized against the source clas-\\nsiﬁer [32, 2, 1]. Others have sought to both learn a feature\\ntransformation and regularize a target classiﬁer simultane-\\nously [18, 10].\\nRecently, supervised CNN based feature representations\\nhave been shown to be extremely effective for a variety of\\nvisual recognition tasks [22, 9, 14, 29]. In particular, using\\ndeep representations dramatically reduces the effect of reso-\\nlution and lighting on domain shifts [9, 19]. Parallel CNN\\narchitectures such as Siamese networks have been shown\\nto be effective for learning invariant representations [6, 8].\\nHowever, training these networks requires labels for each\\ntraining instance, so it is unclear how to extend these meth-\\nods to unsupervised or semi-supervised settings. Multimodal\\ndeep learning architectures have also been explored to learn\\nrepresentations that are invariant to different input modal-\\nities [26]. However, this method operated primarily in a\\ngenerative context and therefore did not leverage the full\\nrepresentational power of supervised CNN representations.\\nTraining a joint source and target CNN architecture was\\nproposed by [7], but was limited to two layers and so was\\nsigniﬁcantly outperformed by the methods which used a\\ndeeper architecture [ 22], pre-trained on a large auxiliary\\ndata source (ex: ImageNet [4]). [13] proposed pre-training\\nwith a denoising auto-encoder, then training a two-layer net-\\nwork simultaneously with the MMD domain confusion loss.\\nThis effectively learns a domain invariant representation, but\\nagain, because the learned network is relatively shallow, it\\nlacks the strong semantic representation that is learned by di-\\nrectly optimizing a classiﬁcation objective with a supervised\\ndeep CNN.\\nUsing classiﬁer output distributions instead of category\\nlabels during training has been explored in the context of\\nmodel compression or distillation [3, 16]. However, we are\\nthe ﬁrst to apply this technique in a domain adaptation setting\\nin order to transfer class correlations between domains.\\nOther works have cotemporaneously explored the idea\\nof directly optimizing a representation for domain invari-\\nance [12, 24]. However, they either use weaker measures\\nof domain invariance or make use of optimization methods\\nthat are less robust than our proposed method, and they do\\nnot attempt to solve the task transfer problem in the semi-\\nsupervised setting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Source Data\\nbackpack chair\\n bike\\nTarget Databackpack\\n?\\nfc8conv1 conv5 fc6 fc7\\nSource softlabels\\nall target data\\nsource data\\nlabeled target data\\nfc8conv1 conv5\\nsource data\\nsoftmax \\nhigh temp\\nsoftlabel \\nloss\\nfcD\\nfc6 fc7\\nclassiﬁcation \\nloss\\ndomain \\nconfusion \\nloss\\ndomain \\nclassiﬁer \\nloss\\nshared\\nshared\\nshared\\nshared\\nshared\\nFigure 2. Our overall CNN architecture for domain and task transfer. We use a domain confusion loss over all source and target (both labeled\\nand unlabeled) data to learn a domain invariant representation. We simultaneously transfer the learned source semantic structure to the target\\ndomain by optimizing the network to produce activation distributions that match those learned for source data in the source only CNN. Best\\nviewed in color.\\n3. Joint CNN architecture for domain and task\\ntransfer\\nWe ﬁrst give an overview of our convolutional network\\n(CNN) architecture, depicted in Figure 2, that learns a rep-\\nresentation which both aligns visual domains and transfers\\nthe semantic structure from a well labeled source domain to\\nthe sparsely labeled target domain. We assume access to a\\nlimited amount of labeled target data, potentially from only\\na subset of the categories of interest. With limited labels on\\na subset of the categories, the traditional domain transfer ap-\\nproach of ﬁne-tuning on the available target data [14, 29, 17]\\nis not effective. Instead, since the source labeled data shares\\nthe label space of our target domain, we use the source data\\nto guide training of the corresponding classiﬁers.\\nOur method takes as input the labeled source data\\n{xS,yS}(blue box Figure 2) and the target data {xT,yT}\\n(green box Figure 2), where the labels yT are only provided\\nfor a subset of the target examples. Our goal is to produce\\na category classiﬁer θC that operates on an image feature\\nrepresentation f(x; θrepr) parameterized by representation\\nparameters θrepr and can correctly classify target examples\\nat test time.\\nFor a setting with Kcategories, let our desired classiﬁca-\\ntion objective be deﬁned as the standard softmax loss\\nLC(x,y; θrepr,θC) =−\\n∑\\nk\\n1 [y= k] logpk (1)\\nwhere p is the softmax of the classiﬁer activations,\\np= softmax(θT\\nCf(x; θrepr)).\\nWe could use the available source labeled data to train\\nour representation and classiﬁer parameters according to\\nEquation (1), but this often leads to overﬁtting to the source\\ndistribution, causing reduced performance at test time when\\nrecognizing in the target domain. However, we note that\\nif the source and target domains are very similar then the\\nclassiﬁer trained on the source will perform well on the\\ntarget. In fact, it is sufﬁcient for the source and target data to\\nbe similar under the learned representation, θrepr.\\nInspired by the “name the dataset” game of Torralba\\nand Efros [ 31], we can directly train a domain classiﬁer\\nθD to identify whether a training example originates from\\nthe source or target domain given its feature representation.\\nIntuitively, if our choice of representation suffers from do-\\nmain shift, then they will lie in distinct parts of the feature\\nspace, and a classiﬁer will be able to easily separate the\\ndomains. We use this notion to add a new domain confusion\\nloss Lconf(xS,xT,θD; θrepr) to our objective and directly op-\\ntimize our representation so as to minimize the discrepancy\\nbetween the source and target distributions. This loss is\\ndescribed in more detail in Section 3.1.\\nDomain confusion can be applied to learn a representation\\nthat aligns source and target data without any target labeled\\ndata. However, we also presume a handful of sparse labels\\nin the target domain, yT. In this setting, a simple approach is\\nto incorporate the target labeled data along with the source\\nlabeled data into the classiﬁcation objective of Equation (1)1.\\nHowever, ﬁne-tuning with hard category labels limits the\\nimpact of a single training example, making it hard for the\\nnetwork to learn to generalize from the limited labeled data.\\nAdditionally, ﬁne-tuning with hard labels is ineffective when\\nlabeled data is available for only a subset of the categories.\\nFor our approach, we draw inspiration from recent net-\\nwork distillation works [ 3, 16], which demonstrate that a\\nlarge network can be “distilled” into a simpler model by re-\\nplacing the hard labels with the softmax activations from the\\noriginal large model. This modiﬁcation proves to be critical,\\nas the distribution holds key information about the relation-\\n1We present this approach as one of our baselines.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ships between categories and imposes additional structure\\nduring the training process. In essence, because each train-\\ning example is paired with an output distribution, it provides\\nvaluable information about not only the category it belongs\\nto, but also each other category the classiﬁer is trained to\\nrecognize.\\nThus, we propose using the labeled target data to op-\\ntimize the network parameters through a soft label loss,\\nLsoft(xT,yT; θrepr,θC). This loss will train the network pa-\\nrameters to produce a “soft label” activation that matches\\nthe average output distribution of source examples on a net-\\nwork trained to classify source data. This loss is described in\\nmore detail in Section 3.2. By training the network to match\\nthe expected source output distributions on target data, we\\ntransfer the learned inter-class correlations from the source\\ndomain to examples in the target domain. This directly trans-\\nfers useful information from source to target, such as the fact\\nthat bookshelves appear more similar to ﬁling cabinets than\\nto bicycles.\\nOur full method then minimizes the joint loss function\\nL(xS,yS,xT,yT,θD;θrepr,θC) =\\nLC(xS,yS,xT,yT; θrepr,θC)\\n+ λLconf(xS,xT,θD; θrepr)\\n+ νLsoft(xT,yT; θrepr,θC).\\n(2)\\nwhere the hyperparameters λand νdetermine how strongly\\ndomain confusion and soft labels inﬂuence the optimization.\\nOur ideas of domain confusion and soft label loss for task\\ntransfer are generic and can be applied to any CNN classiﬁ-\\ncation architecture. For our experiments and for the detailed\\ndiscussion in this paper we modify the standard Krizhevsky\\narchitecture [22], which has ﬁve convolutional layers (conv1–\\nconv5) and three fully connected layers (fc6–fc8). The rep-\\nresentation parameter θrepr corresponds to layers 1–7 of the\\nnetwork, and the classiﬁcation parameter θC corresponds to\\nlayer 8. For the remainder of this section, we provide further\\ndetails on our novel loss deﬁnitions and the implementation\\nof our model.\\n3.1. Aligning domains via domain confusion\\nIn this section we describe in detail our proposed domain\\nconfusion loss objective. Recall that we introduce the domain\\nconfusion loss as a means to learn a representation that is\\ndomain invariant, and thus will allow us to better utilize a\\nclassiﬁer trained using the labeled source data. We consider\\na representation to be domain invariant if a classiﬁer trained\\nusing that representation can not distinguish examples from\\nthe two domains.\\nTo this end, we add an additional domain classiﬁcation\\nlayer, denoted as fcD in Figure 2, with parameters θD. This\\nlayer simply performs binary classiﬁcation using the domain\\ncorresponding to an image as its label. For a particular fea-\\nture representation, θrepr, we evaluate its domain invariance\\nby learning the best domain classiﬁer on the representation.\\nThis can be learned by optimizing the following objective,\\nwhere yD denotes the domain that the example is drawn\\nfrom:\\nLD(xS,xT,θrepr; θD) =−\\n∑\\nd\\n1 [yD = d] logqd (3)\\nwith qcorresponding to the softmax of the domain classiﬁer\\nactivation: q= softmax(θT\\nDf(x; θrepr)).\\nFor a particular domain classiﬁer, θD, we can now in-\\ntroduce our loss which seeks to “maximally confuse” the\\ntwo domains by computing the cross entropy between the\\noutput predicted domain labels and a uniform distribution\\nover domain labels:\\nLconf(xS,xT,θD; θrepr) =−\\n∑\\nd\\n1\\nDlog qd. (4)\\nThis domain confusion loss seeks to learn domain invari-\\nance by ﬁnding a representation in which the best domain\\nclassiﬁer performs poorly.\\nIdeally, we want to simultaneously minimize Equa-\\ntions (3) and (4) for the representation and the domain clas-\\nsiﬁer parameters. However, the two losses stand in direct\\nopposition to one another: learning a fully domain invariant\\nrepresentation means the domain classiﬁer must do poorly,\\nand learning an effective domain classiﬁer means that the\\nrepresentation is not domain invariant. Rather than globally\\noptimizing θD and θrepr, we instead perform iterative updates\\nfor the following two objectives given the ﬁxed parameters\\nfrom the previous iteration:\\nmin\\nθD\\nLD(xS,xT,θrepr; θD) (5)\\nmin\\nθrepr\\nLconf(xS,xT,θD; θrepr). (6)\\nThese losses are readily implemented in standard deep\\nlearning frameworks, and after setting learning rates properly\\nso that Equation (5) only updates θD and Equation (6) only\\nupdates θrepr, the updates can be performed via standard\\nbackpropagation. Together, these updates ensure that we\\nlearn a representation that is domain invariant.\\n3.2. Aligning source and target classes via soft labels\\nWhile training the network to confuse the domains acts\\nto align their marginal distributions, there are no guarantees\\nabout the alignment of classes between each domain. To\\nensure that the relationships between classes are preserved\\nacross source and target, we ﬁne-tune the network against\\n“soft labels” rather than the image category hard label.\\nWe deﬁne a soft label for category kas the average over\\nthe softmax of all activations of source examples in category\\nk, depicted graphically in Figure 3, and denote this aver-\\nage as l(k). Note that, since the source network was trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Source \\nCNN\\nSource \\nCNN\\nSource \\nCNN\\nBottleMugChairLaptopKeyboard\\nBottle Mug ChairLaptopKeyboard\\nBottle Mug ChairLaptopKeyboard\\nBottle Mug ChairLaptopKeyboard\\n+\\nsoftmax \\nhigh \\ntemp\\nsoftmax \\nhigh \\ntemp\\nsoftmax \\nhigh \\ntemp\\nFigure 3. Soft label distributions are learned by averaging the per-\\ncategory activations of source training examples using the source\\nmodel. An example, with 5 categories, depicted here to demonstrate\\nthe ﬁnal soft activation for the bottle category will be primarily\\ndominated by bottle and mug with very little mass on chair, laptop,\\nand keyboard.\\nBottleMug ChairLaptopKeyboard\\nBottleMug ChairLaptopKeyboard\\nAdapt CNN\\n“Bottle”\\nSource Activations \\nPer Class\\nbackprop\\nCross Entropy Loss\\nsoftmax \\nhigh \\ntemp\\nFigure 4. Depiction of the use of source per-category soft activa-\\ntions with the cross entropy loss function over the current target\\nactivations.\\npurely to optimize a classiﬁcation objective, a simple soft-\\nmax over each zi\\nS will hide much of the useful information\\nby producing a very peaked distribution. Instead, we use a\\nsoftmax with a high temperature τ so that the related classes\\nhave enough probability mass to have an effect during ﬁne-\\ntuning. With our computed per-category soft labels we can\\nnow deﬁne our soft label loss:\\nLsoft(xT,yT; θrepr,θC) =−\\n∑\\ni\\nl(yT )\\ni log pi (7)\\nwhere p denotes the soft activation of the target image,\\np = softmax(θT\\nCf(xT; θrepr)/τ). The loss above corre-\\nsponds to the cross-entropy loss between the soft activation\\nof a particular target image and the soft label corresponding\\nto the category of that image, as shown in Figure 4.\\nTo see why this will help, consider the soft label for a\\nparticular category, such as bottle. The soft label l(bottle) is\\na K-dimensional vector, where each dimension indicates\\nthe similarity of bottles to each of the Kcategories. In this\\nexample, the bottle soft label will have a higher weight on\\nmug than on keyboard, since bottles and mugs are more\\nvisually similar. Thus, soft label training with this particular\\nsoft label directly enforces the relationship that bottles and\\nmugs should be closer in feature space than bottles and\\nkeyboards.\\nOne important beneﬁt of using this soft label loss is that\\nwe ensure that the parameters for categories without any\\nlabeled target data are still updated to output non-zero proba-\\nbilities. We explore this beneﬁt in Section 4, where we train\\na network using labels from a subset of the target categories\\nand ﬁnd signiﬁcant performance improvement even when\\nevaluating only on the unlabeled categories.\\n4. Evaluation\\nTo analyze the effectiveness of our method, we evaluate it\\non the Ofﬁce dataset, a standard benchmark dataset for visual\\ndomain adaptation, and on a new large-scale cross-dataset\\ndomain adaptation challenge.\\n4.1. Adaptation on the Ofﬁce dataset\\nThe Ofﬁce dataset is a collection of images from three\\ndistinct domains, Amazon, DSLR, and Webcam, the largest\\nof which has 2817 labeled images [28]. The 31 categories\\nin the dataset consist of objects commonly encountered in\\nofﬁce settings, such as keyboards, ﬁle cabinets, and laptops.\\nWe evaluate our method in two different settings:\\n•Supervised adaptation Labeled training data for all\\ncategories is available in source and sparsely in target.\\n•Semi-supervised adaptation (task adaptation) La-\\nbeled training data is available in source and sparsely\\nfor a subset of the target categories.\\nFor all experiments we initialize the parameters of conv1–\\nfc7 using the released CaffeNet [20] weights. We then fur-\\nther ﬁne-tune the network using the source labeled data in or-\\nder to produce the soft label distributions and use the learned\\nsource CNN weights as the initial parameters for training\\nour method. All implementations are produced using the\\nopen source Caffe [20] framework, and the network deﬁni-\\ntion ﬁles and cross entropy loss layer needed for training\\nwill be released upon acceptance. We optimize the network\\nusing a learning rate of 0.001 and set the hyper-parameters\\nto λ= 0.01 (confusion) and ν = 0.1 (soft).\\nFor each of the six domain shifts, we evaluate across ﬁve\\ntrain/test splits, which are generated by sampling examples\\nfrom the full set of images per domain. In the source domain,\\nwe follow the standard protocol for this dataset and generate\\nsplits by sampling 20 examples per category for the Amazon\\ndomain, and 8 examples per category for the DSLR and\\nWebcam domains.\\nWe ﬁrst present results for the supervised setting, where\\n3 labeled examples are provided for each category in the\\ntarget domain. We report accuracies on the remaining un-\\nlabeled images, following the standard protocol introduced\\nwith the dataset [28]. In addition to a variety of baselines, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='A→W A →D W →A W →D D →A D →W Average\\nDLID [7] 51.9 – – 89.9 – 78.2 –\\nDeCAF6 S+T [9] 80.7 ±2.3 – – – – 94.8 ±1.2 –\\nDaNN [13] 53.6 ±0.2 – – 83.5 ±0.0 – 71.2 ±0.0 –\\nSource CNN 56.5 ±0.3 64.6 ±0.4 42.7 ±0.1 93.6 ±0.2 47.6 ±0.1 92.4 ±0.3 66.22\\nTarget CNN 80.5 ±0.5 81.8 ±1.0 59.9 ±0.3 81.8 ±1.0 59.9 ±0.3 80.5 ±0.5 74.05\\nSource+Target CNN 82.5 ±0.9 85.2 ±1.1 65.2 ±0.7 96.3 ±0.5 65.8 ±0.5 93.9 ±0.5 81.50\\nOurs: dom confusion only 82.8 ±0.9 85.9 ±1.1 64.9 ±0.5 97.5 ±0.2 66.2 ±0.4 95.6 ±0.4 82.13\\nOurs: soft labels only 82.7 ±0.7 84.9 ±1.2 65.2 ±0.6 98.3 ±0.3 66.0 ±0.5 95.9 ±0.6 82.17\\nOurs: dom confusion+soft labels 82.7 ±0.8 86.1 ±1.2 65.0 ±0.5 97.6 ±0.2 66.2 ±0.3 95.7 ±0.5 82.22\\nTable 1. Multi-class accuracy evaluation on the standard supervised adaptation setting with theOfﬁce dataset. We evaluate on all 31 categories\\nusing the standard experimental protocol from [28]. Here, we compare against three state-of-the-art domain adaptation methods as well as a\\nCNN trained using only source data, only target data, or both source and target data together.\\nA→W A →D W →A W →D D →A D →W Average\\nMMDT [18] – 44.6 ±0.3 – 58.3 ±0.5 – – –\\nSource CNN 54.2 ±0.6 63.2 ±0.4 34.7 ±0.1 94.5 ±0.2 36.4 ±0.1 89.3 ±0.5 62.0\\nOurs: dom confusion only 55.2 ±0.6 63.7 ±0.9 41.1 ±0.0 96.5 ±0.1 41.2 ±0.1 91.3 ±0.4 64.8\\nOurs: soft labels only 56.8 ±0.4 65.2 ±0.9 38.8 ±0.4 96.5 ±0.2 41.7 ±0.3 89.6 ±0.1 64.8\\nOurs: dom confusion+soft labels 59.3 ±0.6 68.0 ±0.5 40.5 ±0.2 97.5 ±0.1 43.1 ±0.2 90.0 ±0.2 66.4\\nTable 2. Multi-class accuracy evaluation on the standard semi-supervised adaptation setting with the Ofﬁce dataset. We evaluate on 16\\nheld-out categories for which we have no access to target labeled data. We show results on these unsupervised categories for the source only\\nmodel, our model trained using only soft labels for the 15 auxiliary categories, and ﬁnally using domain confusion together with soft labels\\non the 15 auxiliary categories.\\nreport numbers for both soft label ﬁne-tuning alone as well\\nas soft labels with domain confusion in Table 1. Because the\\nOfﬁce dataset is imbalanced, we report multi-class accura-\\ncies, which are obtained by computing per-class accuracies\\nindependently, then averaging over all 31 categories.\\nWe see that ﬁne-tuning with soft labels or domain con-\\nfusion provides a consistent improvement over hard label\\ntraining in 5 of 6 shifts. Combining soft labels with do-\\nmain confusion produces marginally higher performance on\\naverage. This result follows the intuitive notion that when\\nenough target labeled examples are present, directly opti-\\nmizing for the joint source and target classiﬁcation objective\\n(Source+Target CNN) is a strong baseline and so using ei-\\nther of our new losses adds enough regularization to improve\\nperformance.\\nNext, we experiment with the semi-supervised adaptation\\nsetting. We consider the case in which training data and\\nlabels are available for some, but not all of the categories in\\nthe target domain. We are interested in seeing whether we\\ncan transfer information learned from the labeled classes to\\nthe unlabeled classes.\\nTo do this, we consider having 10 target labeled exam-\\nples per category from only 15 of the 31 total categories,\\nfollowing the standard protocol introduced with the Ofﬁce\\ndataset [28]. We then evaluate our classiﬁcation performance\\non the remaining 16 categories for which no data was avail-\\nable at training time.\\nIn Table 2 we present multi-class accuracies over the 16\\nheld-out categories and compare our method to a previous\\ndomain adaptation method [ 18] as well as a source-only\\ntrained CNN. Note that, since the performance here is com-\\nputed over only a subset of the categories in the dataset, the\\nnumbers in this table should not be directly compared to the\\nsupervised setting in Table 1.\\nWe ﬁnd that all variations of our method (only soft label\\nloss, only domain confusion, and both together) outperform\\nthe baselines. Contrary to the fully supervised case, here we\\nnote that both domain confusion and soft labels contribute\\nsigniﬁcantly to the overall performance improvement of our\\nmethod. This stems from the fact that we are now evaluat-\\ning on categories which lack labeled target data, and thus\\nthe network can not implicitly enforce domain invariance\\nthrough the classiﬁcation objective alone. Separately, the\\nfact that we get improvement from the soft label training on\\nrelated tasks indicates that information is being effectively\\ntransferred between tasks.\\nIn Figure 5, we show examples for the\\nAmazon→Webcam shift where our method correctly\\nclassiﬁes images from held out object categories and the\\nbaseline does not. We ﬁnd that our method is able to\\nconsistently overcome error cases, such as the notebooks\\nthat were previously confused with letter trays, or the black'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ring binder\\nmonitor\\nlaptop computer\\nmonitor\\nspeaker\\nmonitor\\nscissors\\nmug\\nmouse\\nmug\\nmouse\\nmug\\nlaptop computer\\npaper notebook\\nletter tray\\npaper notebook\\nletter tray\\npaper notebook\\nletter tray\\npaper notebook\\nletter tray\\npaper notebook\\nlaptop computer\\npaper notebook\\ncalculator\\nphone\\ncalculator\\nphone\\nfile cabinet\\nprinter\\nfile cabinet\\nprinter\\nfile cabinet\\nprinter\\nlaptop computer\\nprojector\\nlaptop computer\\nprojector\\nfile cabinet\\nprojector\\nphone\\nprojector\\nkeyboard\\nprojector\\ntape dispenser\\npunchers\\nlaptop computer\\nring binder\\nkeyboard\\nring binder\\nkeyboard\\nring binder\\nletter tray\\nring binder\\nlaptop computer\\nring binder\\nFigure 5. Examples from the Amazon →Webcam shift in the\\nsemi-supervised adaptation setting, where our method (the bot-\\ntom turquoise label) correctly classiﬁes images while the baseline\\n(the top purple label) does not.\\nmugs that were confused with black computer mice.\\n4.2. Adaptation between diverse domains\\nFor an evaluation with larger, more distinct domains, we\\ntest on the recent testbed for cross-dataset analysis [ 30],\\nwhich collects images from classes shared in common among\\ncomputer vision datasets. We use the dense version of this\\ntestbed, which consists of 40 categories shared between\\nthe ImageNet, Caltech-256, SUN, and Bing datasets, and\\nevaluate speciﬁcally with ImageNet as source and Caltech-\\n256 as target.\\nWe follow the protocol outlined in [30] and generate 5\\nsplits by selecting 5534 images from ImageNet and 4366\\nimages from Caltech-256 across the 40 shared categories.\\nEach split is then equally divided into a train and test set.\\nHowever, since we are most interested in evaluating in the\\nsetting with limited target data, we further subsample the\\ntarget training set into smaller sets with only 1, 3, and 5\\nlabeled examples per category.\\nResults from this evaluation are shown in Figure 6. We\\ncompare our method to both CNNs ﬁne-tuned using only\\nsource data using source and target labeled data. Contrary to\\nthe previous supervised adaptation experiment, our method\\nsigniﬁcantly outperforms both baselines. We see that our\\nfull architecture, combining domain confusion with the soft\\nlabel loss, performs the best overall and is able to operate\\nin the regime of no labeled examples in the target (corre-\\nsponding to the red line at point 0 on the x-axis). We ﬁnd\\nthat the most beneﬁt of our method arises when there are\\nfew labeled training examples per category in the target do-\\nmain. As we increase the number of labeled examples in\\nthe target, the standard ﬁne-tuning strategy begins to ap-\\nproach the performance of the adaptation approach. This\\nNumber Labeled Target Examples per Category\\n0 1 3 5\\nMulti-class Accuracy\\n72\\n73\\n74\\n75\\n76\\n77\\n78\\nSource CNN\\nSource+Target CNN\\nOurs: softlabels only\\nOurs: dom confusion+softlabels\\nFigure 6. ImageNet→Caltech supervised adaptation from the Cross-\\ndataset [30] testbed with varying numbers of labeled target exam-\\nples per category. We ﬁnd that our method using soft label loss\\n(with and without domain confusion) outperforms the baselines\\nof training on source data alone or using a standard ﬁne-tuning\\nstrategy to train with the source and target data. Best viewed in\\ncolor.\\nindicates that direct joint source and target ﬁne-tuning is\\na viable adaptation approach when you have a reasonable\\nnumber of training examples per category. In comparison,\\nﬁne-tuning on the target examples alone yields accuracies\\nof 36.6 ±0.6, 60.9 ±0.5, and 67.7 ±0.5 for the cases of 1,\\n3, and 5 labeled examples per category, respectively. All of\\nthese numbers underperform the source only model, indicat-\\ning that adaptation is crucial in the setting of limited training\\ndata.\\nFinally, we note that our results are signiﬁcantly higher\\nthan the 24.8% result reported in [ 30], despite the use of\\nmuch less training data. This difference is explained by their\\nuse of SURF BoW features, indicating that CNN features\\nare a much stronger feature for use in adaptation tasks.\\n5. Analysis\\nOur experimental results demonstrate that our method\\nimproves classiﬁcation performance in a variety of domain\\nadaptation settings. We now perform additional analysis on\\nour method by conﬁrming our claims that it exhibits domain\\ninvariance and transfers information across tasks.\\n5.1. Domain confusion enforces domain invariance\\nWe begin by evaluating the effectiveness of domain con-\\nfusion at learning a domain invariant representation. As\\npreviously explained, we consider a representation to be\\ndomain invariant if an optimal classiﬁer has difﬁculty pre-\\ndicting which domain an image originates from. Thus, for\\nour representation learned with a domain confusion loss, we\\nexpect a trained domain classiﬁer to perform poorly.\\nWe train two support vector machines (SVMs) to clas-\\nsify images into domains: one using the baseline CaffeNet\\nfc7 representation, and the other using our fc7 learned with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Figure 7. We compare the baseline CaffeNet representation to\\nour representation learned with domain confusion by training a\\nsupport vector machine to predict the domains of Amazon and\\nWebcam images. For each representation, we plot a histogram of\\nthe classiﬁer decision scores of the test images. In the baseline\\nrepresentation, the classiﬁer is able to separate the two domains\\nwith 99% accuracy. In contrast, the representation learned with\\ndomain confusion is domain invariant, and the classiﬁer can do no\\nbetter than 56%.\\ndomain confusion. These SVMs are trained using 160 im-\\nages, 80 from Amazon and 80 from Webcam, then tested\\non the remaining images from those domains. We plot the\\nclassiﬁer scores for each test image in Figure 7. It is obvious\\nthat the domain confusion representation is domain invariant,\\nmaking it much harder to separate the two domains—the\\ntest accuracy on the domain confusion representation is only\\n56%, not much better than random. In contrast, on the base-\\nline CaffeNet representation, the domain classiﬁer achieves\\n99% test accuracy.\\n5.2. Soft labels for task transfer\\nWe now examine the effect of soft labels in transfer-\\nring information between categories. We consider the\\nAmazon→Webcam shift from the semi-supervised adapta-\\ntion experiment in the previous section. Recall that in this\\nsetting, we have access to target labeled data for only half\\nof our categories. We use soft label information from the\\nsource domain to provide information about the held-out\\ncategories which lack labeled target examples. Figure 8\\nexamines one target example from the held-out category\\nmonitor. No labeled target monitors were available during\\ntraining; however, as shown in the upper right corner of Fig-\\nure 8, the soft labels for laptop computer was present during\\ntraining and assigns a relatively high weight to the monitor\\nclass. Soft label ﬁne-tuning thus allows us to exploit the fact\\nthat these categories are similar. We see that the baseline\\nmodel misclassiﬁes this image as a ring binder, while our\\nsoft label model correctly assigns the monitor label.\\n6. Conclusion\\nWe have presented a CNN architecture that effectively\\nadapts to a new domain with limited or no labeled data per\\ntarget category. We accomplish this through a novel CNN\\narchitecture which simultaneously optimizes for domain in-\\nback pack\\nbike\\nbike helmetbookcasebottlecalculatordesk chairdesk lamp\\ndesktop computer\\nfile cabinetheadphoneskeyboard\\nlaptop computer\\nletter traymobile phone\\nmonitormousemug\\npaper notebook\\npenphoneprinterprojectorpunchersring binder\\nrulerscissorsspeakerstapler\\ntape dispenser\\ntrash can\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\n0.06\\n0.07\\n0.08\\n0.09\\n0.1 Ours soft label\\nback pack\\nbike\\nbike helmetbookcasebottlecalculatordesk chairdesk lamp\\ndesktop computer\\nfile cabinetheadphoneskeyboard\\nlaptop computer\\nletter traymobile phone\\nmonitormousemug\\npaper notebook\\npenphoneprinterprojectorpunchersring binder\\nrulerscissorsspeakerstapler\\ntape dispenser\\ntrash can\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\n0.06\\n0.07\\n0.08\\n0.09\\n0.1 Baseline soft label\\nback pack bike bike helmet\\nbookcase bottle calculator\\ndesk chair desk lamp desktop computer\\nfile cabinet headphones keyboard\\nlaptop computer letter tray mobile phone\\nring binder\\nmonitor\\nBaseline soft activation Our soft activation\\nSource soft labelsTarget test image\\nFigure 8. Our method (bottom turquoise label) correctly predicts\\nthe category of this image, whereas the baseline (top purple label)\\ndoes not. The source per-category soft labels for the 15 categories\\nwith labeled target data are shown in the upper right corner, where\\nthe x-axis of the plot represents the 31 categories and the y-axis is\\nthe output probability. We highlight the index corresponding to the\\nmonitor category in red. As no labeled target data is available for\\nthe correct category,monitor, we ﬁnd that in our method the related\\ncategory of laptop computer (outlined with yellow box) transfers\\ninformation to the monitor category. As a result, after training, our\\nmethod places the highest weight on the correct category. Probabil-\\nity score per category for the baseline and our method are shown\\nin the bottom left and right, respectively, training categories are\\nopaque and correct test category is shown in red.\\nvariance, to facilitate domain transfer, while transferring\\ntask information between domains in the form of a cross\\nentropy soft label loss. We demonstrate the ability of our\\narchitecture to improve adaptation performance in the super-\\nvised and semi-supervised settings by experimenting with\\ntwo standard domain adaptation benchmark datasets. In the\\nsemi-supervised adaptation setting, we see an average rela-\\ntive improvement of 13% over the baselines on the four most\\nchallenging shifts in the Ofﬁce dataset. Overall, our method\\ncan be easily implemented as an alternative ﬁne-tuning strat-\\negy when limited or no labeled data is available per category\\nin the target domain.\\nAcknowledgements This work was supported by DARPA;\\nAFRL; DoD MURI award N000141110688; NSF awards\\n113629, IIS-1427425, and IIS-1212798; and the Berkeley\\nVision and Learning Center.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] L. T. Alessandro Bergamo. Exploiting weakly-labeled web\\nimages to improve object classiﬁcation: a domain adaptation\\napproach. In Neural Information Processing Systems (NIPS),\\nDec. 2010. 2\\n[2] Y . Aytar and A. Zisserman. Tabula rasa: Model transfer for\\nobject category detection. In Proc. ICCV, 2011. 2\\n[3] J. Ba and R. Caruana. Do deep nets really need to be deep?\\nIn Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and\\nK. Weinberger, editors,Advances in Neural Information Pro-\\ncessing Systems 27 , pages 2654–2662. Curran Associates,\\nInc., 2014. 2, 3\\n[4] A. Berg, J. Deng, and L. Fei-Fei. ImageNet Large Scale\\nVisual Recognition Challenge 2012. 2012. 2\\n[5] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel,\\nB. Sch¨olkopf, and A. J. Smola. Integrating structured biologi-\\ncal data by kernel maximum mean discrepancy. In Bioinfor-\\nmatics, 2006. 2\\n[6] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y . LeCun,\\nC. Moore, E. S¨ackinger, and R. Shah. Signature veriﬁcation\\nusing a siamese time delay neural network. International\\nJournal of Pattern Recognition and Artiﬁcial Intelligence ,\\n7(04):669–688, 1993. 2\\n[7] S. Chopra, S. Balakrishnan, and R. Gopalan. DLID: Deep\\nlearning for domain adaptation by interpolating between do-\\nmains. In ICML Workshop on Challenges in Representation\\nLearning, 2013. 2, 6\\n[8] S. Chopra, R. Hadsell, and Y . LeCun. Learning a similar-\\nity metric discriminatively, with application to face veriﬁ-\\ncation. In Computer Vision and Pattern Recognition, 2005.\\nCVPR 2005. IEEE Computer Society Conference on , vol-\\nume 1, pages 539–546. IEEE, 2005. 2\\n[9] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. In Proc.\\nICML, 2014. 2, 6\\n[10] L. Duan, D. Xu, and I. W. Tsang. Learning with augmented\\nfeatures for heterogeneous domain adaptation. In Proc. ICML,\\n2012. 2\\n[11] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsu-\\npervised visual domain adaptation using subspace alignment.\\nIn Proc. ICCV, 2013. 2\\n[12] Y . Ganin and V . Lempitsky. Unsupervised Domain Adap-\\ntation by Backpropagation. ArXiv e-prints, Sept. 2014. 1,\\n2\\n[13] M. Ghifary, W. B. Kleijn, and M. Zhang. Domain adaptive\\nneural networks for object recognition. CoRR, abs/1409.6041,\\n2014. 2, 6\\n[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\\nfeature hierarchies for accurate object detection and semantic\\nsegmentation. arXiv e-prints, 2013. 1, 2, 3\\n[15] B. Gong, Y . Shi, F. Sha, and K. Grauman. Geodesic ﬂow\\nkernel for unsupervised domain adaptation. In Proc. CVPR,\\n2012. 2\\n[16] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\\nin a neural network. In NIPS Deep Learning and Representa-\\ntion Learning Workshop, 2014. 2, 3\\n[17] J. Hoffman, S. Guadarrama, E. Tzeng, R. Hu, J. Donahue,\\nR. Girshick, T. Darrell, and K. Saenko. LSDA: Large scale de-\\ntection through adaptation. In Neural Information Processing\\nSystems (NIPS), 2014. 3\\n[18] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Darrell.\\nEfﬁcient learning of domain-invariant image representations.\\nIn Proc. ICLR, 2013. 2, 6\\n[19] J. Hoffman, E. Tzeng, J. Donahue, , Y . Jia, K. Saenko, and\\nT. Darrell. One-shot learning of supervised deep convolutional\\nmodels. In arXiv 1312.6204; presented at ICLR Workshop,\\n2014. 2\\n[20] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-\\ntional architecture for fast feature embedding. arXiv preprint\\narXiv:1408.5093, 2014. 5\\n[21] D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in\\ndata streams. In Proc. VLDB, 2004. 2\\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\\nclassiﬁcation with deep convolutional neural networks. In\\nProc. NIPS, 2012. 2, 4\\n[23] B. Kulis, K. Saenko, and T. Darrell. What you saw is not\\nwhat you get: Domain adaptation using asymmetric kernel\\ntransforms. In Proc. CVPR, 2011. 2\\n[24] M. Long and J. Wang. Learning transferable features with\\ndeep adaptation networks. CoRR, abs/1502.02791, 2015. 1, 2\\n[25] Y . Mansour, M. Mohri, and A. Rostamizadeh. Domain adap-\\ntation: Learning bounds and algorithms. In COLT, 2009.\\n2\\n[26] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y .\\nNg. Multimodal deep learning. In Proceedings of the 28th\\nInternational Conference on Machine Learning (ICML-11),\\npages 689–696, 2011. 2\\n[27] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain\\nadaptation via transfer component analysis. In IJCA, 2009. 2\\n[28] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual\\ncategory models to new domains. In Proc. ECCV, 2010. 2, 5,\\n6\\n[29] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks. CoRR,\\nabs/1312.6229, 2013. 1, 2, 3\\n[30] T. Tommasi, T. Tuytelaars, and B. Caputo. A testbed for\\ncross-dataset analysis. In TASK-CV Workshop, ECCV, 2014.\\n2, 7\\n[31] A. Torralba and A. Efros. Unbiased look at dataset bias. In\\nProc. CVPR, 2011. 2, 3\\n[32] J. Yang, R. Yan, and A. Hauptmann. Adapting SVM classi-\\nﬁers to data with shifted distributions. In ICDM Workshops,\\n2007. 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='On the Beneﬁt of Combining Neural, Statistical\\nand External Features for Fake News\\nIdentiﬁcation\\nGaurav Bhatt1, Aman Sharma1, Shivam Sharma1, Ankush Nagpal1,\\nBalasubramanian Raman1, and Ankush Mittal 2\\n1 Indian Institute of Technology, Roorkee, India,\\ngauravbhatt.cs.iitr@gmail.com\\n2 Graphic Era University, India\\nAbstract. Identifying the veracity of a news article is an interesting\\nproblem while automating this process can be a challenging task. Detec-\\ntion of a news article as fake is still an open question as it is contingent\\non many factors which the current state-of-the-art models fail to incor-\\nporate. In this paper, we explore a subtask to fake news identiﬁcation,\\nand that is stance detection. Given a news article, the task is to deter-\\nmine the relevance of the body and its claim. We present a novel idea\\nthat combines the neural, statistical and external features to provide\\nan eﬃcient solution to this problem. We compute the neural embedding\\nfrom the deep recurrent model, statistical features from the weighted\\nn-gram bag-of-words model and hand crafted external features with the\\nhelp of feature engineering heuristics. Finally, using deep neural layer all\\nthe features are combined, thereby classifying the headline-body news\\npair as agree, disagree, discuss, or unrelated. We compare our proposed\\ntechnique with the current state-of-the-art models on the fake news chal-\\nlenge dataset. Through extensive experiments, we ﬁnd that the proposed\\nmodel outperforms all the state-of-the-art techniques including the sub-\\nmissions to the fake news challenge.\\nKeywords: External features, Statistical features, Word embeddings,\\nFake news, Deep learning\\n1 Introduction\\nFake news being a potential threat towards journalism and public discourse\\nhas created a buzz across the internet. With the recent advent of social media\\nplatforms such as Facebook and Twitter, it has become easier to propagate any\\ninformation to the masses within minutes. While the propagation of information\\nis proportional to growth of social media, there has been an aggravation in\\nthe authenticity of these news articles. These days it has become a lot easier\\nto mislead the masses using a single Facebook or Twitter fake post. For an\\ninstance, in the US presidential election of 2016, the fake news has been cited as\\nthe foremost contributing factor that aﬀected the outcome [24].\\narXiv:1712.03935v1  [cs.CL]  11 Dec 2017'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Headline ”Robert Plant Ripped up $800M Led Zeppelin Reunion Contract”Stance\\nBody 1 Led Zeppelin’s Robert Plant turned down 500 MILLION to reform supergroup.Agree\\nBody 2No, Robert Plant did not rip up an $800 million deal to get Led Zeppelin back together.Disagree\\nBody 3 Robert Plant reportedly tore up an $800 million Led Zeppelin reunion deal.Discuss\\nBody 4 Richard Branson’s Virgin Galactic is set to launch SpaceShipTwo today.Unrelated\\nTable 1.Headline-body pairs along with their relative stance.\\nThe root cause of this problem lies in the fact that none of the social net-\\nworking sites use any automatic system that can identify the veracity of news\\nﬂowing across these platforms. A possible reason for this failure is the open do-\\nmain nature of the problem that adds to the intricacies. The recently organized\\nFake News Challenge (FNC-1) [13] is an initiative in this direction. The aim of\\nthis challenge is to build an automatic system that has the capability to iden-\\ntify whether a news article is fake or not. More speciﬁcally, given a news article\\nthe task is to evaluate the relatedness of the news body towards its headline.\\nThe relatedness or stance is the relative perspective of a news article towards a\\nrelative claim (shown in Table 1).\\nThe idea behind building a countermeasure for fake news is to use machine\\nlearning and natural language processing (NLP) tools that can compute semantic\\nand contextual similarity between the headline and the body, and classify the\\npairs into one of four categories. Deep learning models have been eﬃcacious in\\nsolving many NLP problems that share similarities to fake news which includes\\nbut not limited to - computing semantic similarity between sentences [1, 18],\\ncommunity based question answering [31, 32], etc. The basic building blocks\\nof all deep models are recurrent networks such as recurrent neural networks\\n(RNN) [23], long short-term memory networks (LSTM) [16] and gated recurrent\\nunits (GRU) [11], and convolution networks such as convolution neural networks\\n(CNN) [17]. A deep architecture encodes the given sequence of words into ﬁxed\\nlength vector representation which can be used to score the relevance of two\\ntextual entities, in our case, relevance of each headline-body pair.\\nStatistical information related to text can be encoded to vectors using the\\ntraditional bag-of-words (BOW) approach. The BOW approaches are often com-\\nbined with term frequency (TF) and inverse document frequency (IDF), and n-\\ngrams that helps to encode more information related to the text [28, 12]. These\\napproaches, however simple, have been used to ameliorate the performance of\\ndeep models in complex NLP problems such as community question answering\\n[32] and answer sentence selection [33]. Sometimes, it is beneﬁcial to leverage\\nfeature engineering heuristics when combined with statistical approaches. The\\nfeature engineering heuristics or the external features are used to aid the learn-\\ning model to successfully converge to a global solution [31, 32, 34]. The external\\nfeatures includes common observations such as number of n-grams, number of\\nwords match between headline and the body, cosine similarity between the head-\\nline and the body vector, etc. The FNC-1 baseline also includes a combination\\nof feature engineering heuristics that alone achieves a competitive performance,\\neven outperforming several widely used deep learning architectures. In this pa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='per, we combine external features introduced in the baseline with some more\\nheuristics that have been shown to be successful in other NLP tasks.\\nThese days it is common to use pre-trained word embeddings such as Word2vec\\n[20] and GloVe [25] along with deep models for NLP tasks. Similar to word em-\\nbedding, the recurrent models have been used to encode an entire sentence to a\\nvector. Some of the widely used sentence-to-vector models include doc2vec [21],\\nparagraph2vec [27] and skip-thought vectors [18]. These deep recurrent models\\nhelps to capture the semantic and contextual information of the textual pairs,\\nin our case, body and its claim. In our work, we use the skip-thought vector to\\nencode the headline and the body, and combine it with external features and\\nstatistical approaches.\\nFinally, the main contributions of the paper can be summarized as\\n1. We propose an approach that is based on the combination of statistical, neu-\\nral and feature engineering heuristics which achieves state-of-the-art perfor-\\nmance on the task of fake news identiﬁcation.\\n2. We evaluate the proposed approach on FNC-1 challenge, and compare our\\nresults with the top-4 submissions to the challenge. We also analyze the\\napplicability of several state-of-the-art deep models on FNC-1 dataset.\\nThe rest of the paper is organized as follows. In section 2, we brief the previous\\nidea over which our works builds, which is followed by applicability of state-\\nof-the-art deep architectures on the problem of stance detection. In section 4\\nwe describe the proposed approach in detail, followed by the experiment setup\\nin section 5, that includes dataset description, training parameters, evaluation\\nmetrics used and results. Finally, our work is concluded in section 6.\\n2 Related Work\\nIn this section, we discuss some previous work that is in relation to fake news\\nidentiﬁcation such as rumor detection in news articles and hoax news identiﬁca-\\ntion. We also discuss the use of deep learning architecture used by some of the\\nresearchers with whom our work shares some similarity.\\nFake news. From an NLP perspective, researchers have studied numerous\\naspects of credibility of online information. For example, [5] applied the time-\\nsensitive supervised approach by relying on the tweet content to address the\\ncredibility of a tweet in diﬀerent situations. [7] used LSTM in a similar problem\\nof early rumor detection. In an another work, [8] aimed at detecting the stance\\nof tweets and determining the veracity of the given rumor with convolution\\nneural networks. A submission [3] to the SemEval 2016 Twitter Stance Detection\\ntask focuses on creating a bag-of-words auto encoder, and training it over the\\ntokenized tweets.\\nFNC-1 submissions. In their work, [26] achieved a preliminary score of\\n0.8080, slightly above the competition baseline of 0.7950. They experimented\\non four basic models on which the ﬁnal result was evaluated: Bag Of Words\\n(BOW), basic LSTM, LSTM with attention and conditional encoding LSTM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='with attention (CEA LSTM). In our work, instead of using the models separately,\\nwe combine the best of these models.\\nAnother team, [34], combined multiple models in an ensemble providing\\n50/50 weighted average between deep convolution neural network and a gradient-\\nboosted decision trees. Though this work seems to be similar to our work, the\\ndiﬀerence lies in the construction of ensemble of classiﬁers. In a similar attempt,\\na team [2] concatenated various features vectors and passed it through an MLP\\nmodel.\\nThe work by [28], focuses on generating lexical and similarity features using\\n(TF-IDF) representations of bag-of-words (BOW) which are then fed through\\na multi-layer perceptron (MLP) with one hidden layer. In their work, [6] di-\\nvided the problem into two groups: unrelated and related. They were able to\\nachieve 90% accuracy on the related/unrelated task by ﬁnding maximum and\\naverage Jaccard similarity score across all sentences in the article and choosing\\nappropriate threshold values. A similar work of splitting the problem into two\\nsubproblems (related and unrelated) is also performed by [10]. The work by [22]\\nfocuses on the use of recurrent models for fake news stance detection.\\n3 Technique Used\\n3.1 Deep Learning Architectures\\nTo predict the stance for a given sample in FNC-1 dataset, a multi-channel\\ndeep neural network can be used to encode a given headline-body pair, which\\ncan be classiﬁed into one of the four stances. This is achieved by using a multi\\nchannel convolution neural network with softmax layer at the output (shown in\\nFigure 1). Similarly, instead of using the convolution and pooling layers, LSTM\\nand GRU can be used to encode the headline-body pairs. The LSTMs and GRUs\\nencode the given sequence of words into ﬁxed length vector representation which\\ncan be used to score the relevance of headline-body pair. However, for long\\nsequences, such as the body of a news article (which typically contain hundreds\\nof words), the RNN models fail to completely encode the entire information\\ninto a ﬁxed length vector. A solution to this problem is given in the form of\\nattentional mechanism [9] which computes a weighted sum of all the encoder\\nunits that are passed on to the decoder. The decoder is learned in such a way\\nthat it gives importance to only some of the words. The attention mechanism\\nalso alleviates the bottleneck of encoding input sequences to ﬁxed length vector\\nand have been shown to outperform other RNN based encoder-decoder models\\non longer sequences [4]. To alleviate the problem of limited memory we use\\nattention mechanism as described in [4].\\nWe experiment with some of the deep architectures that have been shown to\\nbe successful in NLP tasks (shown in Figure 1). Most of these architectures have\\nbeen proven to be eﬀective for non-factoid based question answering [30, 14].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Fig. 1.Deep architectures used for stance detection on the FNC-1 dataset.\\n4 Proposed Idea\\nThe unrelated headline-body pairs in the FNC-1 dataset are created by ran-\\ndomly assigning a news body to the given headline. This type of data augmen-\\ntation has been successfully used in NLP problems such as non-factoid question\\nanswering where it results in reasonable performance by the deep learning mod-\\nels [31, 19]. However, in the case of FNC-1 challenge, the agree, disagree, and\\ndiscuss headline-body pairs are relatively smaller in quantity than theunrelated\\nstance. This bias leads to a uneven distribution of dataset across the four classes,\\nwith the unrelated category being the least interesting. Interestingness of a\\nheadline-body pair is evaluated in terms of information that it contains; It is\\neasier to evaluate a unrelated pair, while the other three are contingent on\\nexploring contextual relationship between the headline and its body, and are\\nconsidered more interesting.\\nThe uneven distribution of FNC-1 dataset thwarts the performance of deep\\nlearning architectures introduced in Section 3. Moreover, news articles are heav-\\nily inﬂuenced by some words that are generally associated with news to describe\\nits polarity. For example, words likecrime, accident, and scandal are often used\\nwith negative connotation. If such words are present in both the news headline,\\nor are present in one while absent from the other, then, it is easier to identify such\\na pair as agree or disagree. Deep learning models are dependent on a huge train-\\ning corpus (few million headline-body pairs) in order to identify such nuances\\nin patterns. The FNC-1 dataset, though the largest publicly available dataset\\non stance detection, does not satiate this criteria. For this reason, we introduce\\na much simpler strategy that consists of heavy use of feature engineering. We\\nleveraged several widely used state-of-the-art features used in natural language\\nprocessing, and use a feed-forward deep neural network which aggregates all the\\nindividual features and computes a score for each headline-body pair.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Fig. 2.Combining the neural, statistical and external features using deep MLP.\\n4.1 Neural Embeddings\\nWe use skip-thought vectors which encodes sentences to vector embedding of\\nlength 4800 (shown in Figure 2). The skip-thought [18] is a encoder-decoder\\nbased recurrent model that computes the relative occurrence of sentences. In\\nour work, we use the pre-trained skip-thought embedding which is trained on\\nBookCorpus [35]. We make the use of a pre-trained model since the FNC-1\\ndataset is relatively smaller than the dataset required to eﬃciently train a re-\\ncurrent encoder-decoder model like skip-thought.\\nWe follow the work of [18, 1] and compute two features from the skip-thought\\nembeddings. These features have been shown to be eﬀective in evaluating con-\\ntextual similarity between sentences. The task of stance detection is analogous\\nto the computation of contextual similarity between two sentences - headline\\nand its body. We speculate that the features introduced by [18, 1] should be\\neﬀective for stance detection as well. Given the skip-thought encoding of news\\nand headline as unews and vhead, we compute two features\\nfeat1 = unews.vhead (1)\\nfeat2 = |unews −vhead| (2)\\nwhere feat1 is the component-wise product and feat2 is the absolute diﬀer-\\nence between the skip-thought encoding of news and headlines. Both of these\\nfeatures results in a 4800 dimensional vector each.\\n4.2 Statistical Features\\nWe capture the statistical information from the text to vectors with the help of\\nBOW, TF-IDF and n-grams models. We follow the work of [28] and [12], and\\nproduce the following vectors for each headline-body pair\\n1. 1-gram TF vector of the headline.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='2. 1-gram TF vector of the body.\\nThis gives us a vector of 5000 dimension each. We concatenate both of the TF\\nvectors and pass it to a MLP layer (as shown in Figure 2).\\n4.3 External Features\\nThe external features include feature engineering heuristics such as number of\\nsimilar words in the headline and body, cosine similarity between vector encod-\\nings of headline-body pairs, number of n-grams matched between the pairs, etc.\\nWe leveraged ideas for computing the external features from the baseline and\\nadd some extra features, which includes\\n1. Number of characters n-grams match between the headline-body pair, where\\nn = 2, ··· , 16.\\n2. Number of words n-grams match between the headline-body pair, where\\nn = 2, ··· , 6.\\n3. Weighted TF-IDF score between headline and its body using the approach\\nmentioned in [33].\\n4. Sentiment diﬀerence between the headline-body pair, also termed as polarity\\nand is computed using lexicon based approach.\\n5. N-gram refuting feature which is constructed using BOW on a lexicon of n\\npre-deﬁned words. It is similar to polarity based features with an addition\\nof n-gram model.\\nAll the external features adds up to a 50-dimensional feature vector and is passed\\nto a MLP layer similar to neural and statistical features.\\n5 Experimentations\\n5.1 Dataset Description\\nWe use the dataset provided in the FNC-1 challenge which is derived from the\\nEmergent Dataset [15], provided by the fake news challenge administrators. The\\nformer consist of 49972 tuple with each tuple consisting of a headline-body pair\\nfollowed by a corresponding class label stance of either agree, disagree, unrelated\\nor discuss. Word counts roughly ranges between 8 to 40 for headlines and 600\\nto 7000 for article body. The distribution of FNC-1 dataset is shown in Table 2.\\nNews articles unrelated discuss agree disagree\\n49972 73.13 % 17.83 % 7.36 % 1.68 %\\nTable 2.FNC-1 dataset description.\\nThe ﬁnal results are evaluated over a test dataset provided by fake news\\norganization consisting of 25413 samples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Hyperparameter Skip-thought External FeaturesTF-IDF Vectors\\nMLP layers 2 1 2\\nMLP neurons 500 ; 100 50 500 ; 50\\nDropout 0.2 ; - - 0.4 ; -\\nActivation sigmoid ; sigmoid relu relu ; relu\\nRegularization L2 - 0.00000001 ; - - L2 - 0.00005 ; -\\nMLP Layers 1\\nMLP neurons 4\\nActivation Softmax\\nOptimizer Adam\\nLearning rate 0.001\\nBatch size 100\\nLoss Cross-entropy\\nTable 3.Values of hyper-parameters. The ﬁrst half of the table shows the parameters\\nused in architectures for extracting individual features. The second half shows the\\nparameter setting of the feature combination layer that is shown in Figure 2.\\n5.2 Training parameters\\nAs shown in Figure 2, the proposed model computes the feature vectors sepa-\\nrately and then combine these with the help of a MLP layer. We use cross-entropy\\nas the loss function to optimize our architecture with a softmax layer at the out-\\nput which classify the given headline-body pair into agree, disagree, discuss, and\\nunrelated. The hyper-parameter setting is shown in Table 3.\\n5.3 Baselines and compared methods\\nOrganizers of FNC-1 have provided a baseline model that consists of a gradient-\\nboosting classiﬁer over n-gram subsequences between the headline and the body\\nalong with several external features such as word overlap, occurrence of sentiment\\nusing a lexicon of highly-polarized words (like fraud and hoax). With this simple\\nyet elegant baseline it is possible to outperform some of the highly used deep\\nlearning architectures that we have used in our work. Following the work of [26],\\nwe also introduce three new baselines for the FNC-1 dataset: word2vec+external\\nfeatures baseline, skip-thought baseline, and TF-IDF baseline. All these baselines\\nfocuses on performance of neural, statistical, and external features, when used\\nindividually.\\nWe compare our proposed approach with the submissions of top 4 teams at\\nFNC-1 3, which includes the work by [34], [26], [2] and [28]. Apart from the\\ntop submissions at FNC-1, we also compare the proposed architecture with four\\ndeep learning architectures introduced in Section 3, namely, CNN, biLSTM,\\nBiLSTM+Attention and CNN+biLSTM.\\n3 http://www.fakenewschallenge.org/\\nhttps://competitions.codalab.org/competitions/16843#results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='5.4 Evaluation metrics\\nFrom Table 2 it is evident that the FNC-1 dataset shows a heavy bias towards\\nunrelated headline-body pairs. Recognizing this data bias and the simpler na-\\nture of the related/unrelated classiﬁcation problems, the organizers of FNC-1\\nintroduced the following weighted accuracy score as their ﬁnal evaluation metric.\\nScore1 = AccuracyRelated,Unrelated (3)\\nScore2 = AccuracyAgree,Disagree,Discuss (4)\\nScoreFNC = 0.25 ∗Score1 + 0.75 ∗Score2 (5)\\nWe use the ScoreFNC as the main evaluation criteria while comparing the\\nproposed model with other related techniques. We also use the class-wise accu-\\nracy for further evaluation of the performance of all the techniques.\\n5.5 Results\\nThe results on FNC-1 test dataset are shown in Table 4. The ﬁrst part of the ta-\\nble shows the performance of the baselines used in our work. The FNC-1 baseline\\nachieves a score of 75 .2 which is better than the performance of all deep archi-\\ntectures introduced in Section 3. The FNC-1 baseline is comprised of training\\ngradient tree classiﬁer on the hand crafted features (described in Section 4.3).\\nProvided the simplicity of this baseline, it is indeed remarkable to achieve such a\\nhigh score. The FNC-1 baselines achieves approx 7% higher class-wise accuracy\\non unrelated stance as compared to skip-thought baseline, whereas the latter\\nreceiving a higher ScoreFNC . Skip-thought baselines achieves a higher accuracy\\non agree and discuss than the unrelated stance. Since the interestingness of\\nagree and discuss is higher than the unrealted stance, therefore, skip-thought\\nachieves a higher ScoreFNC . This also explains the reason for the introduction\\nof new scoring criterion by the FNC organizers (see Section 5.4). Finally, the\\nScoreFNC by skip-thought, external features, and TF-IDF baselines are higher\\nthan the FNC-1 baseline. Therefore, our speculation to combine these three base-\\nlines models, is guaranteed to achieve a higher score on ScoreFNC evaluation\\nmetric. Moreover, all the baselines achieves very low or zero score on the dis-\\nagree stance. Therefore, apart from the ScoreFNC , the class-wise performance\\nis worth considering as a performance criterion.\\nThe performance of top-4 teams that participated in FNC-1 are shown in the\\nmiddle part of Table 4, with SOLAT in the SWEN[34] winning the challenge\\nachieving a score of 82.05. All the teams achieved higher score and class-wise\\naccuracy on all stances except for the disagree stance. This should be a concern,\\nsince the importance of disagree is equivalent to the agree and discuss stance.\\nWe observed that the news pairs in the disagree category are not only very few,\\nbut also consists of divergent news articles. This is one of the reason for poor\\nperformance of most of the deep models, including the top teams, on identifying\\ndiagree stance.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Method ScoreFNC Agree Disagree Discuss Unrelated Overall\\nFNC-1 baseline 75.20 9.09 1.00 79.65 97.97 85.44\\nWord2vec + External Features 75.78 50.70 9.61 53.38 96.05 82.79\\nSkip-thought baseline 76.18 31.8 0.00 81.20 91.18 82.48\\nTF-IDF baseline 81.72 44.04 6.60 81.38 97.90 88.46\\nSOLAT in the SWEN [34] 82.05 58.50 1.86 76.18 98.70 89.08\\nAthene [2] 81.97 44.72 9.47 80.89 99.25 89.50\\nUCL Machine Reading [28] 81.72 44.04 6.60 81.38 97.90 88.46\\nChips Ahoy! [29] 80.12 55.96 0.28 70.29 98.98 88.01\\nCNN 60.91 35.89 2.10 46.77 88.47 74.84\\nbiLSTM 63.11 38.04 4.59 58.13 78.27 69.88\\nbiLSTM + Attention 63.17 58.74 0.03 63.48 77.49 73.27\\nCNN + biLSTM 64.95 74.09 2.46 57.85 74.87 72.89\\nProposed 83.08 43.82 6.31 85.68 98.04 89.29\\nTable 4.Performance of diﬀerent models on FNC-1 Test Dataset. The ﬁrst half of the\\ntable shows the baselines, followed by the top-4 submissions, and diﬀerent architectures\\nused in our work. Column 2-5 shows the class-wise accuracy in % while the last column\\nshows the overall accuracy.\\nAgree Disagree Discuss Unrelated Overall\\nAgree 834 15 945 109 43.82\\nDisagree 208 44 328 117 6.31\\nDiscuss 401 23 3825 215 85.68\\nUnrelated 22 12 325 17990 98.04\\nTable 5.Confusion matrix for the proposed model on FNC-1 testset.\\nThe lowest section in Table 4 shows the performance of the proposed model\\nalong with other architectures used in our work. The proposed model achieves\\nhighest score and highest class-wise accuracy ondiscuss stance whereas achieving\\nhigh accuracy on other stances that is comparable to top submissions at FNC-1.\\nFrom Table 5, it is evident that the overall accuracy achieved by the proposed\\nmodel is slightly lower than [2], although the proposed model outperformed all\\nthe other techniques by a clear margin (in terms of ScoreFNC ). The possible\\nreason for this deviation is that the [2] gives more focus to the classiﬁcation of\\nunrelated stances rather than the rest, which is the reason for highest overall\\naccuracy. Since unrelated stances are of least interest to us, this results in lower\\nScoreFNC . Finally, a confusion matrix is given in Table 5 that provides in-detail\\nanalysis of the performance of our approach.\\n6 Conclusion\\nIn this paper, we explore the beneﬁt of incorporating neural, statistical and\\nexternal features to deep neural networks on the task of fake news stance de-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='tection. We also presented in-depth analysis of several state-of-the-art recurrent\\nand convolution architectures (shown in Figure 1). The presented idea lever-\\nages features extracted using skip-thought embeddings, n-gram TF-vectors and\\nseveral introduced hand crafted features.\\nWe found that the uneven distribution of FNC-1 dataset undermines the\\nperformance of most deep learning architectures. The fewer training samples\\nadds further to this aggravation. Creating a dataset for a complex NLP problems\\nsuch as fake news identiﬁcation is indeed a cumbersome task, and we appreciate\\nthe work by the FNC organizers, yet, a more detailed and elaborate dataset\\nshould make this challenge more suitable to evaluate.\\nReferences\\n[1] E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W. Guo. sem\\n2013 shared task: Semantic textual similarity, including a pilot on typed-\\nsimilarity. In In* SEM 2013: The Second Joint Conference on Lexical and\\nComputational Semantics. Association for Computational Linguistics. Cite-\\nseer, 2013.\\n[2] B. S. Andreas Hanselowski, Avinesh PVS and F. Caspelherr. Team athene\\non the fake news challenge. 2017.\\n[3] I. Augenstein, A. Vlachos, and K. Bontcheva. Usfd at semeval-2016 task\\n6: Any-target stance detection on twitter with autoencoders. In SemEval@\\nNAACL-HLT, pages 389–393, 2016.\\n[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014. URL http:\\n//arxiv.org/abs/1409.0473.\\n[5] C. Castillo, M. Mendoza, and B. Poblete. Predicting information credibility\\nin time-sensitive social media. Internet Research, 23(5):560–588, 2013.\\n[6] A. K. Chaudhry, D. Baker, and P. Thun-Hohenstein. Stance detection for\\nthe fake news challenge: Identifying textual relationships with deep neural\\nnets. 2017.\\n[7] T. Chen, L. Wu, X. Li, J. Zhang, H. Yin, and Y. Wang. Call attention\\nto rumors: Deep attention based recurrent neural networks for early rumor\\ndetection. arXiv preprint arXiv:1704.05973, 2017.\\n[8] Y.-C. Chen, Z.-Y. Liu, and H.-Y. Kao. Ikm at semeval-2017 task 8: Convo-\\nlutional neural networks for stance detection and rumor veriﬁcation. Pro-\\nceedings of SemEval. ACL, 2017.\\n[9] K. Cho, B. van Merrienboer, C ¸ . G¨ ul¸ cehre, F. Bougares, H. Schwenk, and\\nY. Bengio. Learning phrase representations using RNN encoder-decoder for\\nstatistical machine translation. CoRR, abs/1406.1078, 2014. URL http:\\n//arxiv.org/abs/1406.1078.\\n[10] S. Chopra, S. Jain, and J. M. Sholar. Towards automatic identiﬁcation\\nof fake news: Headline-article stance detection with lstm attention models,\\n2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='[11] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of\\ngated recurrent neural networks on sequence modeling. arXiv preprint\\narXiv:1412.3555, 2014.\\n[12] R. Davis and C. Proctor. Fake news, real consequences: Recruiting neural\\nnetworks for the ﬁght against fake news. 2017.\\n[13] D. R. Dean Pomerleau. Fake news challenge. 2017.\\n[14] M. Feng, B. Xiang, M. R. Glass, L. Wang, and B. Zhou. Applying deep\\nlearning to answer selection: A study and an open task. InAutomatic Speech\\nRecognition and Understanding (ASRU), 2015 IEEE Workshop on, pages\\n813–820. IEEE, 2015.\\n[15] W. Ferreira and A. Vlachos. Emergent: a novel data-set for stance classiﬁca-\\ntion. In Proceedings of the 2016 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Tech-\\nnologies. ACL, 2016.\\n[16] A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with\\nbidirectional lstm and other neural network architectures. Neural Networks,\\n18(5):602–610, 2005.\\n[17] H. He, K. Gimpel, and J. J. Lin. Multi-perspective sentence similarity\\nmodeling with convolutional neural networks. InEMNLP, pages 1576–1586,\\n2015.\\n[18] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba,\\nand S. Fidler. Skip-thought vectors. In Advances in neural information\\nprocessing systems, pages 3294–3302, 2015.\\n[19] T. Mihaylov and P. Nakov. Semanticz at semeval-2016 task 3: Ranking\\nrelevant answers in community question answering using semantic similarity\\nbased on ﬁne-tuned word embeddings. In SemEval@ NAACL-HLT, pages\\n879–886, 2016.\\n[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word\\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed\\nrepresentations of words and phrases and their compositionality. In Ad-\\nvances in neural information processing systems, pages 3111–3119, 2013.\\n[22] K. Miller and A. Oswalt. Fake news headline classiﬁcation using neural\\nnetworks with attention. 2017.\\n[23] P. Neculoiu, M. Versteegh, M. Rotaru, and T. B. Amsterdam. Learning\\ntext similarity with siamese recurrent networks. ACL 2016, page 148, 2016.\\n[24] NYTimes. As fake news spreads lies, more readers shrug at the truth. 2016.\\n[25] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for\\nword representation. In EMNLP, volume 14, pages 1532–1543, 2014.\\n[26] S. Pfohl, O. Triebe, and F. Legros. Stance detection for the fake news\\nchallenge with attention and conditional encoding.\\n[27] R. ˇReh˚ uˇ rek and P. Sojka. Software Framework for Topic Modelling with\\nLarge Corpora. In Proceedings of the LREC 2010 Workshop on New\\nChallenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010.\\nELRA. http://is.muni.cz/publication/884893/en.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='[28] B. Riedel, I. Augenstein, G. P. Spithourakis, and S. Riedel. A simple but\\ntough-to-beat baseline for the fake news challenge stance detection task.\\narXiv preprint arXiv:1707.03264, 2017.\\n[29] J. Shang. Chips ahoy! at fake news challenge. 2017.\\n[30] M. Tan, C. d. Santos, B. Xiang, and B. Zhou. Lstm-based deep learning\\nmodels for non-factoid answer selection. arXiv preprint arXiv:1511.04108,\\n2015.\\n[31] L. Yang, Q. Ai, D. Spina, R.-C. Chen, L. Pang, W. B. Croft, J. Guo, and\\nF. Scholer. Beyond factoid qa: Eﬀective methods for non-factoid answer\\nsentence retrieval. In European Conference on Information Retrieval, pages\\n115–128. Springer, 2016.\\n[32] Y. Yang, W.-t. Yih, and C. Meek. Wikiqa: A challenge dataset for open-\\ndomain question answering. In EMNLP, pages 2013–2018, 2015.\\n[33] L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman. Deep learning for\\nanswer sentence selection. arXiv preprint arXiv:1412.1632, 2014.\\n[34] S. B. Yuxi Pan, Doug Sibley. Talos. http://blog.talosintelligence.\\ncom/2017/06/, 2017.\\n[35] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and\\nS. Fidler. Aligning books and movies: Towards story-like visual explanations\\nby watching movies and reading books. arXiv preprint arXiv:1506.06724,\\n2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nN-BEATS: N EURAL BASIS EXPANSION ANALYSIS FOR\\nINTERPRETABLE TIME SERIES FORECASTING\\nBoris N. Oreshkin\\nElement AI\\nboris.oreshkin@gmail.com\\nDmitri Carpov\\nElement AI\\ndmitri.carpov@elementai.com\\nNicolas Chapados\\nElement AI\\nchapados@elementai.com\\nYoshua Bengio\\nMila\\nyoshua.bengio@mila.quebec\\nABSTRACT\\nWe focus on solving the univariate times series point forecasting problem using\\ndeep learning. We propose a deep neural architecture based on backward and\\nforward residual links and a very deep stack of fully-connected layers. The ar-\\nchitecture has a number of desirable properties, being interpretable, applicable\\nwithout modiﬁcation to a wide array of target domains, and fast to train. We test\\nthe proposed architecture on several well-known datasets, including M3, M4 and\\nTOURISM competition datasets containing time series from diverse domains. We\\ndemonstrate state-of-the-art performance for two conﬁgurations of N-BEATS for\\nall the datasets, improving forecast accuracy by 11% over a statistical benchmark\\nand by 3% over last year’s winner of the M4 competition, a domain-adjusted\\nhand-crafted hybrid between neural network and statistical time series models.\\nThe ﬁrst conﬁguration of our model does not employ any time-series-speciﬁc\\ncomponents and its performance on heterogeneous datasets strongly suggests that,\\ncontrarily to received wisdom, deep learning primitives such as residual blocks are\\nby themselves sufﬁcient to solve a wide range of forecasting problems. Finally, we\\ndemonstrate how the proposed architecture can be augmented to provide outputs\\nthat are interpretable without considerable loss in accuracy.\\n1 I NTRODUCTION\\nTime series (TS) forecasting is an important business problem and a fruitful application area for\\nmachine learning (ML). It underlies most aspects of modern business, including such critical areas as\\ninventory control and customer management, as well as business planning going from production and\\ndistribution to ﬁnance and marketing. As such, it has a considerable ﬁnancial impact, often ranging\\nin the millions of dollars for every point of forecasting accuracy gained (Jain, 2017; Kahn, 2003).\\nAnd yet, unlike areas such as computer vision or natural language processing where deep learning\\n(DL) techniques are now well entrenched, there still exists evidence that ML and DL struggle to\\noutperform classical statistical TS forecasting approaches (Makridakis et al., 2018a;b). For instance,\\nthe rankings of the six “pure” ML methods submitted to M4 competition were 23, 37, 38, 48, 54,\\nand 57 out of a total of 60 entries, and most of the best-ranking methods were ensembles of classical\\nstatistical techniques (Makridakis et al., 2018b).\\nOn the other hand, the M4 competition winner (Smyl, 2020), was based on a hybrid between\\nneural residual/attention dilated LSTM stack with a classical Holt-Winters statistical model (Holt,\\n1957; 2004; Winters, 1960) with learnable parameters. Since Smyl’s approach heavily depends on\\nthis Holt-Winters component, Makridakis et al. (2018b) further argue that “hybrid approaches and\\ncombinations of method are the way forward for improving the forecasting accuracy and making\\nforecasting more valuable”. In this work we aspire to challenge this conclusion by exploring the\\npotential of pure DL architectures in the context of the TS forecasting. Moreover, in the context of\\ninterpretable DL architecture design, we are interested in answering the following question: can we\\n1\\narXiv:1905.10437v4  [cs.LG]  20 Feb 2020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\ninject a suitable inductive bias in the model to make its internal operations more interpretable, in the\\nsense of extracting some explainable driving factors combining to produce a given forecast?\\n1.1 S UMMARY OF CONTRIBUTIONS\\nDeep Neural Architecture:To the best of our knowledge, this is the ﬁrst work to empirically\\ndemonstrate that pure DL using no time-series speciﬁc components outperforms well-established\\nstatistical approaches on M3, M4 and TOURISM datasets (on M4, by 11% over statistical benchmark,\\nby 7% over the best statistical entry, and by 3% over the M4 competition winner). In our view, this\\nprovides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens\\nmotivation to continue advancing the research in this area.\\nInterpretable DL for Time Series:In addition to accuracy beneﬁts, we also show that it is fea-\\nsible to design an architecture with interpretable outputs that can be used by practitioners in very\\nmuch the same way as traditional decomposition techniques such as the “seasonality-trend-level”\\napproach (Cleveland et al., 1990).\\n2 P ROBLEM STATEMENT\\nWe consider the univariate point forecasting problem in discrete time. Given a length- H forecast\\nhorizon a length- T observed series history [y1,..., yT ] ∈RT , the task is to predict the vector of\\nfuture values y ∈RH = [yT +1,yT +2,..., yT +H ]. For simplicity, we will later consider a lookback\\nwindow of length t ≤T ending with the last observed value yT to serve as model input, and denoted\\nx ∈Rt = [yT −t+1,..., yT ]. We denote ˆy the forecast of y. The following metrics are commonly\\nused to evaluate forecasting performance (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000;\\nMakridakis et al., 2018b; Athanasopoulos et al., 2011):\\nsMAPE = 200\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\n|yT +i|+|ˆyT +i|, MAPE = 100\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\n|yT +i| ,\\nMASE = 1\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\n1\\nT +H−m ∑T +H\\nj=m+1 |yj −yj−m|, OWA = 1\\n2\\n[ sMAPE\\nsMAPE Naïve2\\n+\\nMASE\\nMASE Naïve2\\n]\\n.\\nHere m is the periodicity of the data (e.g., 12 for monthly series). MAPE (Mean Absolute Percentage\\nError), sMAPE (symmetric MAPE ) and MASE (Mean Absolute Scaled Error) are standard scale-free\\nmetrics in the practice of forecasting (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000):\\nwhereas sMAPE scales the error by the average between the forecast and ground truth, the MASE\\nscales by the average error of the naïve predictor that simply copies the observation measured m\\nperiods in the past, thereby accounting for seasonality. OWA (overall weighted average) is a M4-\\nspeciﬁc metric used to rank competition entries (M4 Team, 2018b), where sMAPE and MASE metrics\\nare normalized such that a seasonally-adjusted naïve forecast obtains OWA = 1.0.\\n3 N-BEATS\\nOur architecture design methodology relies on a few key principles. First, the base architecture\\nshould be simple and generic, yet expressive (deep). Second, the architecture should not rely on time-\\nseries-speciﬁc feature engineering or input scaling. These prerequisites let us explore the potential\\nof pure DL architecture in TS forecasting. Finally, as a prerequisite to explore interpretability, the\\narchitecture should be extendable towards making its outputs human interpretable. We now discuss\\nhow those principles converge to the proposed architecture.\\n3.1 B ASIC BLOCK\\nThe proposed basic building block has a fork architecture and is depicted in Fig. 1 (left). We focus on\\ndescribing the operation of ℓ-th block in this section in detail (note that the block index ℓ is dropped\\nin Fig. 1 for brevity). The ℓ-th block accepts its respective input xℓ and outputs two vectors, ˆxℓ and\\nˆyℓ. For the very ﬁrst block in the model, its respective xℓ is the overall model input — a history\\nlookback window of certain length ending with the last measured observation. We set the length of\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nFC Stack\\n(4 layers)\\nFC FC\\n( )\\x00\\x00 \\x00\\x00( )\\x00\\x00 \\x00\\x00\\nForecastBackcast\\n\\x00\\x00\\nBlock Input\\n\\x00\\x00\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)\\nLookback window\\n(model input)\\nForecast Period\\nHorizon H\\nLookback\\xa0Period\\nHorizon nH (here n=3)\\nFigure 1: Proposed architecture. The basic building block is a multi-layer FC network with RELU\\nnonlinearities. It predicts basis expansion coefﬁcients both forward, θ f , (forecast) and backward, θb,\\n(backcast). Blocks are organized into stacks using doubly residual stacking principle. A stack may\\nhave layers with shared gb and gf . Forecasts are aggregated in hierarchical fashion. This enables\\nbuilding a very deep neural network with interpretable outputs.\\ninput window to a multiple of the forecast horizon H, and typical lengths of x in our setup range from\\n2H to 7H. For the rest of the blocks, their inputs xℓ are residual outputs of the previous blocks. Each\\nblock has two outputs: ˆyℓ, the block’s forward forecast of lengthH; and ˆxℓ, the block’s best estimate\\nof xℓ, also known as the ‘backcast’, given the constraints on the functional space that the block can\\nuse to approximate signals.\\nInternally, the basic building block consists of two parts. The ﬁrst part is a fully connected network\\nthat produces the forward θ f\\nℓ and the backward θb\\nℓ predictors of expansion coefﬁcients (again, note\\nthat the block index ℓ is dropped for θb\\nℓ , θ f\\nℓ , gb\\nℓ, gf\\nℓ in Fig. 1 for brevity). The second part consists of\\nthe backward gb\\nℓ and the forward gf\\nℓ basis layers that accept the respective forward θ f\\nℓ and backward\\nθb\\nℓ expansion coefﬁcients, project them internally on the set of basis functions and produce the\\nbackcast ˆxℓ and the forecast outputs ˆyℓ deﬁned in the previous paragraph.\\nThe operation of the ﬁrst part of the ℓ-th block is described by the following equations:\\nhℓ,1 = FCℓ,1(xℓ), hℓ,2 = FCℓ,2(hℓ,1), hℓ,3 = FCℓ,3(hℓ,2), hℓ,4 = FCℓ,4(hℓ,3).\\nθb\\nℓ = LINEAR b\\nℓ(hℓ,4), θ f\\nℓ = LINEAR f\\nℓ (hℓ,4).\\n(1)\\nHere LINEAR layer is simply a linear projection layer, i.e. θ f\\nℓ = Wf\\nℓ hℓ,4. The FC layer is a standard\\nfully connected layer with RELU non-linearity (Nair & Hinton, 2010), such that for FCℓ,1 we have,\\nfor example: hℓ,1 = RELU(Wℓ,1xℓ +bℓ,1). One task of this part of the architecture is to predict the\\nforward expansion coefﬁcients θ f\\nℓ with the ultimate goal of optimizing the accuracy of the partial\\nforecast ˆyℓ by properly mixing the basis vectors supplied by gf\\nℓ . Additionally, this sub-network\\npredicts backward expansion coefﬁcients θb\\nℓ used by gb\\nℓ to produce an estimate of xℓ with the ultimate\\ngoal of helping the downstream blocks by removing components of their input that are not helpful for\\nforecasting.\\nThe second part of the network maps expansion coefﬁcients θ f\\nℓ and θb\\nℓ to outputs via basis layers,\\nˆyℓ = gf\\nℓ (θ f\\nℓ ) and ˆxℓ = gb\\nℓ(θb\\nℓ ). Its operation is described by the following equations:\\nˆyℓ =\\ndim(θ f\\nℓ )\\n∑\\ni=1\\nθ f\\nℓ,ivf\\ni , ˆxℓ =\\ndim(θb\\nℓ )\\n∑\\ni=1\\nθb\\nℓ,ivb\\ni .\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nHere vf\\ni and vb\\ni are forecast and backcast basis vectors, θ f\\nℓ,i is the i-th element of θ f\\nℓ . The function\\nof gb\\nℓ and gf\\nℓ is to provide sufﬁciently rich sets {vf\\ni }\\ndim(θ f\\nℓ )\\ni=1 and {vb\\ni }\\ndim(θb\\nℓ )\\ni=1 such that their respective\\noutputs can be represented adequately via varying expansion coefﬁcientsθ f\\nℓ and θb\\nℓ . As shown below,\\ngb\\nℓ and gf\\nℓ can either be chosen to be learnable or can be set to speciﬁc functional forms to reﬂect\\ncertain problem-speciﬁc inductive biases in order to appropriately constrain the structure of outputs.\\nConcrete examples of gb\\nℓ and gf\\nℓ are discussed in Section 3.3.\\n3.2 D OUBLY RESIDUAL STACKING\\nThe classical residual network architecture adds the input of the stack of layers to its output before\\npassing the result to the next stack (He et al., 2016). The DenseNet architecture proposed by Huang\\net al. (2017) extends this principle by introducing extra connections from the output of each stack to\\nthe input of every other stack that follows it. These approaches provide clear advantages in improving\\nthe trainability of deep architectures. Their disadvantage in the context of this work is that they result\\nin network structures that are difﬁcult to interpret. We propose a novel hierarchical doubly residual\\ntopology depicted in Fig. 1 (middle and right). The proposed architecture has two residual branches,\\none running over backcast prediction of each layer and the other one is running over the forecast\\nbranch of each layer. Its operation is described by the following equations:\\nxℓ = xℓ−1 −ˆxℓ−1, ˆy = ∑\\nℓ\\nˆyℓ.\\nAs previously mentioned, in the special case of the very ﬁrst block, its input is the model level\\ninput x, x1 ≡x. For all other blocks, the backcast residual branch xℓ can be thought of as running a\\nsequential analysis of the input signal. Previous block removes the portion of the signal ˆxℓ−1 that\\nit can approximate well, making the forecast job of the downstream blocks easier. This structure\\nalso facilitates more ﬂuid gradient backpropagation. More importantly, each block outputs a partial\\nforecast ˆyℓ that is ﬁrst aggregated at the stack level and then at the overall network level, providing a\\nhierarchical decomposition. The ﬁnal forecast ˆy is the sum of all partial forecasts. In a generic model\\ncontext, when stacks are allowed to have arbitrary gb\\nℓ and gf\\nℓ for each layer, this makes the network\\nmore transparent to gradient ﬂows. In a special situation of deliberate structure enforced in gb\\nℓ and gf\\nℓ\\nshared over a stack, explained next, this has the critical importance of enabling interpretability via the\\naggregation of meaningful partial forecasts.\\n3.3 I NTERPRETABILITY\\nWe propose two conﬁgurations of the architecture, based on the selection of gb\\nℓ and gf\\nℓ . One of them\\nis generic DL, the other one is augmented with certain inductive biases to be interpretable.\\nThe generic architecturedoes not rely on TS-speciﬁc knowledge. We set gb\\nℓ and gf\\nℓ to be a linear\\nprojection of the previous layer output. In this case the outputs of block ℓ are described as:\\nˆyℓ = Vf\\nℓ θ f\\nℓ +bf\\nℓ , ˆxℓ = Vb\\nℓθb\\nℓ +bb\\nℓ.\\nThe interpretation of this model is that the FC layers in the basic building block depicted in Fig. 1 learn\\nthe predictive decomposition of the partial forecast ˆyℓ in the basis Vf\\nℓ learned by the network. Matrix\\nVf\\nℓ has dimensionality H ×dim(θ f\\nℓ ). Therefore, the ﬁrst dimension of Vf\\nℓ has the interpretation of\\ndiscrete time index in the forecast domain. The second dimension of the matrix has the interpretation\\nof the indices of the basis functions, with θ f\\nℓ being the expansion coefﬁcients for this basis. Thus the\\ncolumns of Vf\\nℓ can be thought of as waveforms in the time domain. Because no additional constraints\\nare imposed on the form of Vf\\nℓ , the waveforms learned by the deep model do not have inherent\\nstructure (and none is apparent in our experiments). This leads to ˆyℓ not being interpretable.\\nThe interpretable architecturecan be constructed by reusing the overall architectural approach in\\nFig. 1 and by adding structure to basis layers at stack level. Forecasting practitioners often use the\\ndecomposition of time series into trend and seasonality, such as those performed by theSTL (Cleveland\\net al., 1990) and X13-ARIMA (U.S. Census Bureau, 2013). We propose to design the trend and\\nseasonality decomposition into the model to make the stack outputs more easily interpretable. Note\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nthat for the generic model the notion of stack was not necessary and the stack level indexing was\\nomitted for clarity. Now we will consider both stack level and block level indexing. For example, ˆys,ℓ\\nwill denote the partial forecast of block ℓ within stack s.\\nTrend model.A typical characteristic of trend is that most of the time it is a monotonic function, or\\nat least a slowly varying function. In order to mimic this behaviour we propose to constrain gb\\ns,ℓ and\\ngf\\ns,ℓ to be a polynomial of small degree p, a function slowly varying across forecast window:\\nˆys,ℓ =\\np\\n∑\\ni=0\\nθ f\\ns,ℓ,iti. (2)\\nHere time vector t = [0,1,2,..., H −2,H −1]T /H is deﬁned on a discrete grid running from 0 to\\n(H −1)/H, forecasting H steps ahead. Alternatively, the trend forecast in matrix form will then be:\\nˆytr\\ns,ℓ = Tθ f\\ns,ℓ,\\nwhere θ f\\ns,ℓ are polynomial coefﬁcients predicted by a FC network of layer ℓ of stack s described by\\nequations (1); and T = [1,t,..., tp] is the matrix of powers of t. If p is low, e.g. 2 or 3, it forces ˆytr\\ns,ℓ\\nto mimic trend.\\nSeasonality model. Typical characteristic of seasonality is that it is a regular, cyclical, recurring\\nﬂuctuation. Therefore, to model seasonality, we propose to constrain gb\\ns,ℓ and gf\\ns,ℓ to belong to the\\nclass of periodic functions, i.e. yt = yt−∆, where ∆ is a seasonality period. A natural choice for the\\nbasis to model periodic function is the Fourier series:\\nˆys,ℓ =\\n⌊H/2−1⌋\\n∑\\ni=0\\nθ f\\ns,ℓ,i cos(2πit)+ θ f\\ns,ℓ,i+⌊H/2⌋sin(2πit), (3)\\nThe seasonality forecast will then have the matrix form as follows:\\nˆyseas\\ns,ℓ = Sθ f\\ns,ℓ,\\nwhere θ f\\ns,ℓ are Fourier coefﬁcients predicted by a FC network of layer ℓ of stack s described by\\nequations (1); and S = [1,cos(2πt),... cos(2π⌊H/2−1⌋t)),sin(2πt),..., sin(2π⌊H/2−1⌋t))] is the\\nmatrix of sinusoidal waveforms. The forecast ˆyseas\\ns,ℓ is then a periodic function mimicking typical\\nseasonal patterns.\\nThe overall interpretable architecture consists of two stacks: the trend stack is followed by the\\nseasonality stack. The doubly residual stacking combined with the forecast/backcast principle result\\nin (i) the trend component being removed from the input windowx before it is fed into the seasonality\\nstack and (ii) the partial forecasts of trend and seasonality are available as separate interpretable\\noutputs. Structurally, each of the stacks consists of several blocks connected with residual connections\\nas depicted in Fig. 1 and each of them shares its respective, non-learnable gb\\ns,ℓ and gf\\ns,ℓ. The number\\nof blocks is 3 for both trend and seasonality. We found that on top of sharing gb\\ns,ℓ and gf\\ns,ℓ, sharing all\\nthe weights across blocks in a stack resulted in better validation performance.\\n3.4 E NSEMBLING\\nEnsembling is used by all the top entries in the M4-competition. We rely on ensembling as well\\nto be comparable. We found that ensembling is a much more powerful regularization technique\\nthan the popular alternatives, e.g. dropout or L2-norm penalty. The addition of those methods\\nimproved individual models, but was hurting the performance of the ensemble. The core property of\\nan ensemble is diversity. We build an ensemble using several sources of diversity. First, the ensemble\\nmodels are ﬁt on three different metrics: sMAPE ,MASE and MAPE , a version of sMAPE that has only\\nthe ground truth value in the denominator. Second, for every horizonH, individual models are trained\\non input windows of different length: 2H,3H,..., 7H, for a total of six window lengths. Thus the\\noverall ensemble exhibits a multi-scale aspect. Finally, we perform a bagging procedure (Breiman,\\n1996) by including models trained with different random initializations. We use 180 total models to\\nreport results on the test set (please refer to Appendix B for the ablation of ensemble size). We use\\nthe median as ensemble aggregation function.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 1: Performance on the M4, M3, TOURISM test sets, aggregated over each dataset. Evaluation\\nmetrics are speciﬁed for each dataset; lower values are better. The number of time series in each\\ndataset is provided in brackets.\\nM4 Average (100,000) M3 Average (3,003) TOURISM Average (1,311)\\nsMAPE OWA sMAPE MAPE\\nPure ML 12.894 0.915 Comb S-H-D 13.52 ETS 20.88\\nStatistical 11.986 0.861 ForecastPro 13.19 Theta 20.88\\nProLogistica 11.845 0.841 Theta 13.01 ForePro 19.84\\nML/TS combination 11.720 0.838 DOTM 12.90 Stratometrics 19.52\\nDL/TS hybrid 11.374 0.821 EXP 12.71 LeeCBaker 19.35\\nN-BEATS-G 11.168 0.797 12.47 18.47\\nN-BEATS-I 11.174 0.798 12.43 18.97\\nN-BEATS-I+G 11.135 0.795 12.37 18.52\\n4 R ELATED WORK\\nThe approaches to TS forecasting can be split in a few distinct categories. The statistical model-\\ning approaches based on exponential smoothing and its different ﬂavors are well established and\\nare often considered a default choice in the industry (Holt, 1957; 2004; Winters, 1960). More\\nadvanced variations of exponential smoothing include the winner of M3 competition, the Theta\\nmethod (Assimakopoulos & Nikolopoulos, 2000) that decomposes the forecast into several theta-lines\\nand statistically combines them. The pinnacle of the statistical approach encapsulates ARIMA,\\nauto-ARIMA and in general, the uniﬁed state-space modeling approach, that can be used to ex-\\nplain and analyze all of the approaches mentioned above (see Hyndman & Khandakar (2008) for\\nan overview). More recently, ML/TS combination approaches started inﬁltrating the domain with\\ngreat success, showing promising results by using the outputs of statistical engines as features. In\\nfact, 2 out of top-5 entries in the M4 competition are approaches of this type, including the second\\nentry (Montero-Manso et al., 2019). The second entry computes the outputs of several statistical\\nmethods on the M4 dataset and combines them using gradient boosted tree (Chen & Guestrin, 2016).\\nSomewhat independently, the work in the modern deep learning TS forecasting developed based on\\nvariations of recurrent neural networks (Flunkert et al., 2017; Rangapuram et al., 2018b; Toubeau\\net al., 2019; Zia & Razzaq, 2018) being largely dominated by the electricity load forecasting in the\\nmulti-variate setup. A few earlier works explored the combinations of recurrent neural networks\\nwith dilation, residual connections and attention (Chang et al., 2017; Kim et al., 2017; Qin et al.,\\n2017). These served as a basis for the winner of the M4 competition (Smyl, 2020). The winning\\nentry combines a Holt-Winters style seasonality model with its parameters ﬁtted to a given TS via\\ngradient descent and a unique combination of dilation/residual/attention approaches for each forecast\\nhorizon. The resulting model is a hybrid model that architecturally heavily relies on a time-series\\nengine. It is hand crafted to each speciﬁc horizon of M4, making this approach hard to generalize to\\nother datasets.\\n5 E XPERIMENTAL RESULTS\\nOur key empirical results based on aggregate performance metrics over several datasets—M4 (M4\\nTeam, 2018b; Makridakis et al., 2018b), M3 (Makridakis & Hibon, 2000; Makridakis et al., 2018a)\\nand TOURISM (Athanasopoulos et al., 2011)—appear in Table 1. More detailed descriptions of the\\ndatasets are provided in Section 5.1 and Appendix A. For each dataset, we compare our results with\\nbest 5 entries for this dataset reported in the literature, according to the customary metrics speciﬁc to\\neach dataset (M4: OWA and sMAPE , M3: sMAPE , TOURISM : MAPE ). More granular dataset-speciﬁc\\nresults with data splits over forecast horizons and types of time series appear in respective appendices\\n(M4: Appendix C.1; M3: Appendix C.2; TOURISM : Appendix C.3).\\nIn Table 1, we study the performance of two N-BEATS conﬁgurations: generic (N-BEATS-G) and\\ninterpretable (N-BEATS-I), as well as N-BEATS-I+G (ensemble of all models from N-BEATS-G and\\nN-BEATS-I).On M4 dataset, we compare against 5 representatives from the M4 competition (Makri-\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\ndakis et al., 2018b): each best in their respective model class. Pure ML is the submission by B. Trotta,\\nthe best entry among the 6 pure ML models. Statistical is the best pure statistical model by N.Z.\\nLegaki and K. Koutsouri. ML/TS combination is the model by P. Montero-Manso, T. Talagala, R.J.\\nHyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few statistical time\\nseries models. ProLogistica is the third entry in M4 based on the weighted ensemble of statistical\\nmethods. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020). On the M3 dataset,\\nwe compare against the Theta method (Assimakopoulos & Nikolopoulos, 2000), the winner of M3;\\nDOTA, a dynamically optimized Theta model (Fiorucci et al., 2016); EXP, the most resent statistical\\napproach and the previous state-of-the-art on M3 (Spiliotis et al., 2019); as well as ForecastPro, an\\noff-the-shelf forecasting software that is based on model selection between exponential smoothing,\\nARIMA and moving average (Athanasopoulos et al., 2011; Assimakopoulos & Nikolopoulos, 2000).\\nOn theTOURISM dataset, we compare against 3 statistical benchmarks (Athanasopoulos et al.,\\n2011): ETS, exponential smoothing with cross-validated additive/multiplicative model;Theta method;\\nForePro, same as ForecastProin M3; as well as top 2 entries from the TOURISM Kaggle competi-\\ntion (Athanasopoulos & Hyndman, 2011): Stratometrics, an unknown technique; LeeCBaker (Baker\\n& Howard, 2011), a weighted combination of Naïve, linear trend model, and exponentially weighted\\nleast squares regression trend.\\nAccording to Table 1, N-BEATS demonstrates state-of-the-art performance on three challenging\\nnon-overlapping datasets containing time series from very different domains, sampling frequencies\\nand seasonalities. As an example, on M4 dataset, the OWA gap between N-BEATS and the M4\\nwinner (0.821 −0.795 = 0.026) is greater than the gap between the M4 winner and the second entry\\n(0.838 −0.821 = 0.017). Generic N-BEATS model uses as little prior knowledge as possible, with\\nno feature engineering, no scaling and no internal architectural components that may be considered\\nTS-speciﬁc. Thus the result in Table 1 leads us to the conclusion that DL does not need support\\nfrom the statistical approaches or hand-crafted feature engineering and domain knowledge to perform\\nextremely well on a wide array of TS forecasting tasks. On top of that, the proposed general\\narchitecture performs very well on three different datasets outperforming a wide variety of models,\\nboth generic and manually crafted to respective dataset, including the winner of M4, a model\\narchitecturally adjusted by hand to each forecast-horizon subset of the M4 data.\\n5.1 D ATASETS\\nM4 (M4 Team, 2018b; Makridakis et al., 2018b) is the latest in an inﬂuential series of forecasting\\ncompetitions organized by Spyros Makridakis since 1982 (Makridakis et al., 1982). The 100k-series\\ndataset is large and diverse, consisting of data frequently encountered in business, ﬁnancial and\\neconomic forecasting, and sampling frequencies ranging from hourly to yearly. A table with summary\\nstatistics is presented in Appendix A.1, showing wide variability in TS characteristics.\\nM3 (Makridakis & Hibon, 2000) is similar in its composition to M4, but has a smaller overall scale\\n(3003 time series total vs. 100k in M4). A table with summary statistics is presented in Appendix A.2.\\nOver the past 20 years, this dataset has supported signiﬁcant efforts in the design of more optimal\\nstatistical models, e.g. Theta and its variants (Assimakopoulos & Nikolopoulos, 2000; Fiorucci et al.,\\n2016; Spiliotis et al., 2019). Furthermore, a recent publication (Makridakis et al., 2018a) based on a\\nsubset of M3 presented evidence that ML models are inferior to the classical statistical models.\\nTOURISM (Athanasopoulos et al., 2011) dataset was released as part of the respective Kaggle\\ncompetition conducted by Athanasopoulos & Hyndman (2011). The data include monthly, quarterly\\nand yearly series supplied by both governmental tourism organizations (e.g. Tourism Australia, the\\nHong Kong Tourism Board and Tourism New Zealand) as well as various academics, who had used\\nthem in previous studies. A table with summary statistics is presented in Appendix A.3.\\n5.2 T RAINING METHODOLOGY\\nWe split each dataset into train, validation and test subsets. The test subset is the standard test set\\npreviously deﬁned for each dataset (M4 Team, 2018a; Makridakis & Hibon, 2000; Athanasopoulos\\net al., 2011). The validation and train subsets for each dataset are obtained by splitting their full train\\nsets at the boundary of the last horizon of each time series. We use the train and validation subsets to\\ntune hyperparameters. Once the hyperparameters are determined, we train the model on the full train\\nset and report results on the test set. Please refer to Appendix D for detailed hyperparameter settings\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nat the block level. N-BEATS is implemented and trained in Tensorﬂow (Abadi et al., 2015). We\\nshare parameters of the network across horizons, therefore we train one model per horizon for each\\ndataset. If every time series is interpreted as a separate task, this can be linked back to the multitask\\nlearning and furthermore to meta-learning (see discussion in Section 6), in which a neural network\\nis regularized by learning on multiple tasks to improve generalization. We would like to stress that\\nmodels for different horizons and datasets reuse the same architecture. Architectural hyperparameters\\n(width, number of layers, number of stacks, etc.) are ﬁxed to the same values across horizons and\\nacross datasets (see Appendix D). The fact that we can reuse architecture and even hyperparameters\\nacross horizons indicates that the proposed architecture design generalizes well across time series of\\ndifferent nature. The same architecture is successfully trained on the M4 Monthly subset with 48k\\ntime series and the M3 Others subset with 174 time series. This is a much stronger result than e.g. the\\nresult of S. Smyl (Makridakis et al., 2018b) who had to use very different architectures hand crafted\\nfor different horizons.\\nTo update network parameters for one horizon, we sample train batches of ﬁxed size 1024. We pick\\n1024 TS ids from this horizon, uniformly at random with replacement. For each selected TS id we\\npick a random forecast point from the historical range of length LH immediately preceding the last\\npoint in the train part of the TS. LH is a cross-validated hyperparameter. We observed that for subsets\\nwith large number of time series it tends to be smaller and for subsets with smaller number of time\\nseries it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4LH is\\nequal to 1.5; and in moderate to small Weekly, Daily, Hourly subsets of M4LH is equal to 10. Given\\na sampled forecast point, we set one horizon worth of points following it to be the target forecast\\nwindow y and we set the history of points of one of lengths 2H,3H,..., 7H preceding it to be the\\ninput x to the network. We use the Adam optimizer with default settings and initial learning rate\\n0.001. While optimising the ensemble members relying on the minimization of sMAPE metric, we\\nstop the gradient ﬂows in the denominator to make training numerically stable. The neural network\\ntraining is run with early stopping and the number of batches is determined on the validation set. The\\nGPU based training of one ensemble member for entire M4 dataset takes between 30 min and 2 hours\\ndepending on neural network settings and hardware.\\n5.3 I NTERPRETABILITY RESULTS\\nFig. 2 studies the outputs of the proposed model in the generic and the interpretable conﬁgurations.\\nAs discussed in Section 3.3, to make the generic architecture presented in Fig. 1 interpretable, we\\nconstrain gθ in the ﬁrst stack to have the form of polynomial (2) while the second one has the form\\nof Fourier basis (3). Furthermore, we use the outputs of the generic conﬁguration of N-BEATS as\\ncontrol group (the generic model of 30 residual blocks depicted in Fig. 1 is divided into two stacks)\\nand we plot both generic (sufﬁx “-G”) and interpretable (sufﬁx “-I”) stack outputs side by side in\\nFig. 2. The outputs of generic model are arbitrary and non-interpretable: either trend or seasonality\\nor both of them are present at the output of both stacks. The magnitude of the output (peak-to-peak)\\nis generally smaller at the output of the second stack. The outputs of the interpretable model exhibit\\ndistinct properties: the trend output is monotonic and slowly moving, the seasonality output is\\nregular, cyclical and has recurring ﬂuctuations. The peak-to-peak magnitude of the seasonality output\\nis signiﬁcantly larger than that of the trend, if signiﬁcant seasonality is present in the time series.\\nSimilarly, the peak-to-peak magnitude of trend output tends to be small when no obvious trend\\nis present in the ground truth signal. Thus the proposed interpretable architecture decomposes its\\nforecast into two distinct components. Our conclusion is that the outputs of the DL model can be\\nmade interpretable by encoding a sensible inductive bias in the architecture. Table 1 conﬁrms that\\nthis does not result in performance drop.\\n6 D ISCUSSION : C ONNECTIONS TO META-LEARNING\\nMeta-learning deﬁnes an inner learning procedure and an outer learning procedure. The inner\\nlearning procedure is parameterized, conditioned or otherwise inﬂuenced by the outer learning\\nprocedure (Bengio et al., 1991). The prototypical inner vs. outer learning is individual learning in\\nthe lifetime of an animal vs. evolution of the inner learning procedure itself over many generations\\nof individuals. To see the two levels, it often helps to refer to two sets of parameters, the inner\\nparameters (e.g. synaptic weights) which are modiﬁed inside the inner learning procedure, and the\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n0 1 2 3 4 5\\nt\\n0.8\\n0.9\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90 STACK1-G\\n0 1 2 3 4 5\\nt\\n0.025\\n0.050\\n0.075 STACK2-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90\\n0.95\\nSTACK1-I\\n0 1 2 3 4 5\\nt\\n0.02\\n0.03\\n0.04\\n0.05 STACK2-I\\n0 2 4 6\\nt\\n0.85\\n0.90\\n0.95\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6\\nt\\n0.86\\n0.88\\n0.90 STACK1-G\\n0 2 4 6\\nt\\n0.025\\n0.000\\n0.025\\n0.050\\nSTACK2-G\\n0 2 4 6\\nt\\n0.88\\n0.89\\n0.90\\nSTACK1-I\\n0 2 4 6\\nt\\n0.05\\n0.00\\n0.05\\nSTACK2-I\\n0 5 10 15\\nt\\n0.4\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 5 10 15\\nt\\n0.8\\n0.9 STACK1-G\\n0 5 10 15\\nt\\n0.1\\n0.0\\nSTACK2-G\\n0 5 10 15\\nt\\n0.85\\n0.90\\nSTACK1-I\\n0 5 10 15\\nt\\n0.3\\n0.2\\n0.1\\n0.0 STACK2-I\\n0 2 4 6 8 10 12\\nt\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80\\nSTACK1-G\\n0 2 4 6 8 10 12\\nt\\n0.000\\n0.025\\n0.050\\n0.075\\nSTACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.002\\n0.001\\n STACK2-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.0003\\n0.0002\\n0.0001\\n STACK2-I\\n0 10 20 30 40\\nt\\n0.25\\n0.50\\n0.75\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n(a) Combined\\n0 10 20 30 40\\nt\\n0.2\\n0.4\\n0.6\\nSTACK1-G (b) Stack1-G\\n0 10 20 30 40\\nt\\n0.02\\n0.00\\nSTACK2-G (c) Stack2-G\\n0 10 20 30 40\\nt\\n0.36\\n0.38\\n0.40\\nSTACK1-I (d) StackT-I\\n0 10 20 30 40\\nt\\n0.2\\n0.0\\n0.2\\nSTACK2-I (e) StackS-I\\nFigure 2: The outputs of generic and the interpretable conﬁgurations, M4 dataset. Each row is one\\ntime series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\\nare normalized by the maximal value of the actual time series for convenience. Column (a) shows the\\nactual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\\nforecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\\nrespectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\\nand the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\\nouter parameters or meta-parameters (e.g. genes) which get modiﬁed only in the outer learning\\nprocedure.\\nN-BEATS can be cast as an instance of meta-learning by drawing the following parallels. The outer\\nlearning procedure is encapsulated in the parameters of the whole network, learned by gradient\\ndescent. The inner learning procedure is encapsulated in the set of basic building blocks and modiﬁes\\nthe expansion coefﬁcients θ f that basis gf takes as inputs. The inner learning proceeds through a\\nsequence of stages, each corresponding to a block within the stack of the architecture. Each of the\\nblocks can be thought of as performing the equivalent of an update step which gradually modiﬁes\\nthe expansion coefﬁcients θ f which eventually feed into gf in each block (which get added together\\nto form the ﬁnal prediction). The inner learning procedure takes a single history from a piece of a\\nTS and sees that history as a training set. It produces forward expansion coefﬁcients θ f (see Fig. 1),\\nwhich parametrically map inputs to predictions. In addition, each preceding block modiﬁes the input\\nto the next block by producing backward expansion coefﬁcients θb, thus conditioning the learning\\nand the output of the next block. In the case of the interpretable model, the meta-parameters are only\\nin the FC layers because the gf ’s are ﬁxed. In the case of the generic model, the meta-parameters\\nalso include the V’s which deﬁne thegf non-parametrically. This point of view is further reinforced\\nby the results of the ablation study reported in Appendix B showing that increasing the number of\\nblocks in the stack, as well as the number of stacks improves generalization performance, and can be\\ninterpreted as more iterations of the inner learning procedure.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n7 C ONCLUSIONS\\nWe proposed and empirically validated a novel architecture for univariate TS forecasting. We showed\\nthat the architecture is general, ﬂexible and it performs well on a wide array of TS forecasting prob-\\nlems. We applied it to three non-overlapping challenging competition datasets: M4, M3 andTOURISM\\nand demonstrated state-of-the-art performance in two conﬁgurations: generic and interpretable. This\\nallowed us to validate two important hypotheses: (i) the generic DL approach performs exceptionally\\nwell on heterogeneous univariate TS forecasting problems using no TS domain knowledge, (ii) it is\\nviable to additionally constrain a DL model to force it to decompose its forecast into distinct human\\ninterpretable outputs. We also demonstrated that the DL models can be trained on multiple time series\\nin a multi-task fashion, successfully transferring and sharing individual learnings. We speculate that\\nN-BEATS’s performance can be attributed in part to it carrying out a form of meta-learning, a deeper\\ninvestigation of which should be the subject of future work.\\nREFERENCES\\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\\nKudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike\\nSchuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent\\nVanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,\\nMartin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\\non heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from\\ntensorﬂow.org.\\nV . Assimakopoulos and K. Nikolopoulos. The theta model: a decomposition approach to forecasting.\\nInternational Journal of Forecasting, 16(4):521–530, 2000.\\nGeorge Athanasopoulos and Rob J. Hyndman. The value of feedback in forecasting competitions.\\nInternational Journal of Forecasting, 27(3):845–849, 2011.\\nGeorge Athanasopoulos, Rob J. Hyndman, Haiyan Song, and Doris C. Wu. The tourism forecasting\\ncompetition. International Journal of Forecasting, 27(3):822–844, 2011.\\nLee C. Baker and Jeremy Howard. Winning methods for forecasting tourism time series.International\\nJournal of Forecasting, 27(3):850–852, 2011.\\nYoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. InProceedings\\nof the International Joint Conference on Neural Networks, pp. II–A969, Seattle, USA, 1991.\\nChristoph Bergmeir, Rob J. Hyndman, and José M. Benítez. Bagging exponential smoothing methods\\nusing STL decomposition and Box–Cox transformation. International Journal of Forecasting, 32\\n(2):303–312, 2016.\\nLeo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, Aug 1996.\\nPhil Brierley. Winning methods for forecasting seasonal tourism time series. International Journal of\\nForecasting, 27(3):853–854, 2011.\\nShiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael\\nWitbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.\\nIn NIPS, pp. 77–87, 2017.\\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In ACM SIGKDD, pp.\\n785–794, 2016.\\nRobert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning. STL: A seasonal-\\ntrend decomposition procedure based on Loess (with discussion). Journal of Ofﬁcial Statistics, 6:\\n3–73, 1990.\\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.\\nuci.edu/ml.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, and Anne B. Koehler.\\nModels for optimising the Theta method and their relationship to state space models. International\\nJournal of Forecasting, 32(4):1151–1161, 2016.\\nValentin Flunkert, David Salinas, and Jan Gasthaus. DeepAR: Probabilistic forecasting with autore-\\ngressive recurrent networks. CoRR, abs/1704.04110, 2017.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In CVPR, pp. 770–778. IEEE Computer Society, 2016.\\nC. C. Holt. Forecasting trends and seasonals by exponentially weighted averages. Technical Report\\nONR memorandum no. 5, Carnegie Institute of Technology, Pittsburgh, PA, 1957.\\nCharles C. Holt. Forecasting seasonals and trends by exponentially weighted moving averages.\\nInternational Journal of Forecasting, 20(1):5–10, 2004.\\nGao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected\\nconvolutional networks. In CVPR, pp. 2261–2269. IEEE Computer Society, 2017.\\nRob Hyndman and Anne B. Koehler. Another look at measures of forecast accuracy. International\\nJournal of Forecasting, 22(4):679–688, 2006.\\nRob J Hyndman and Yeasmin Khandakar. Automatic time series forecasting: the forecast package\\nfor R. Journal of Statistical Software, 26(3):1–22, 2008.\\nChaman L. Jain. Answers to your forecasting questions. Journal of Business Forecasting, 36, Spring\\n2017.\\nKenneth B. Kahn. How to measure the impact of a forecast error on an enterprise? The Journal of\\nBusiness Forecasting Methods & Systems, 22(1), Spring 2003.\\nJaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. Residual lstm: Design of a deep recurrent\\narchitecture for distant speech recognition. In Interspeech 2017, pp. 1591–1595, 2017.\\nM4 Team. M4 dataset, 2018a. URLhttps://github.com/M4Competition/M4-methods/tree/\\nmaster/Dataset.\\nM4 Team. M4 competitor’s guide: prizes and rules, 2018b. URL www.m4.unic.ac.cy/\\nwp-content/uploads/2018/03/M4-CompetitorsGuide.pdf.\\nS Makridakis, E Spiliotis, and V Assimakopoulos. Statistical and machine learning forecasting\\nmethods: Concerns and ways forward. PLoS ONE, 13(3), 2018a.\\nSpyros Makridakis and Michèle Hibon. The M3-Competition: results, conclusions and implications.\\nInternational Journal of Forecasting, 16(4):451–476, 2000.\\nSpyros Makridakis, A Andersen, Robert Carbone, Robert Fildes, Michele Hibon, Rudolf\\nLewandowski, Joseph Newton, Emanuel Parzen, and Robert Winkler. The accuracy of extrapo-\\nlation (time series) methods: Results of a forecasting competition. Journal of forecasting, 1(2):\\n111–153, 1982.\\nSpyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The M4-Competition:\\nResults, ﬁndings, conclusion and way forward. International Journal of Forecasting , 34(4):\\n802–808, 2018b.\\nPablo Montero-Manso, George Athanasopoulos, Rob J Hyndman, and Thiyanga S Talagala.\\nFFORMA: Feature-based Forecast Model Averaging. International Journal of Forecasting, 2019.\\nto appear.\\nVinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\\nICML, pp. 807–814, 2010.\\nYao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison W. Cottrell. A\\ndual-stage attention-based recurrent neural network for time series prediction. In IJCAI-17, pp.\\n2627–2633, 2017.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 11, 'page_label': '12', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nSyama Sundar Rangapuram, Matthias Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim\\nJanuschowski. Deep state space models for time series forecasting. In NeurIPS, 2018a.\\nSyama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and\\nTim Januschowski. Deep state space models for time series forecasting. In NeurIPS 31, pp.\\n7785–7794, 2018b.\\nSlawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time\\nseries forecasting. International Journal of Forecasting, 36(1):75 – 85, 2020.\\nSlawek Smyl and Karthik Kuber. Data preprocessing and augmentation for multiple short time series\\nforecasting with recurrent neural networks. In 36th International Symposium on Forecasting, 2016.\\nEvangelos Spiliotis, Vassilios Assimakopoulos, and Konstantinos Nikolopoulos. Forecasting with a\\nhybrid method utilizing data smoothing, a variation of the theta method and shrinkage of seasonal\\nfactors. International Journal of Production Economics, 209:92–102, 2019.\\nA. A. Syntetos, J. E. Boylan, and J. D. Croston. On the categorization of demand patterns. Journal of\\nthe Operational Research Society, 56(5):495–503, 2005.\\nJ. Toubeau, J. Bottieau, F. Vallée, and Z. De Grève. Deep learning-based multivariate probabilistic\\nforecasting for short-term scheduling in power markets. IEEE Transactions on Power Systems, 34\\n(2):1203–1215, March 2019.\\nU.S. Census Bureau. Reference manual for the X-13ARIMA-SEATS Program, version 1.0, 2013.\\nURL http://www.census.gov/ts/x13as/docX13AS.pdf.\\nYuyang Wang, Alex Smola, Danielle C. Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.\\nDeep factors for forecasting. In ICML, 2019.\\nPeter R. Winters. Forecasting sales by exponentially weighted moving averages. Management\\nScience, 6(3):324–342, 1960.\\nHsiang-Fu Yu, Nikhil Rao, and Inderjit S. Dhillon. Temporal regularized matrix factorization for\\nhigh-dimensional time series prediction. In NIPS, 2016.\\nTehseen Zia and Saad Razzaq. Residual recurrent highway networks for learning deep sequence\\nprediction models. Journal of Grid Computing, Jun 2018.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 2: Composition of the M4 dataset: the number of time series based on their sampling frequency\\nand type.\\nFrequency / Horizon\\nType Yearly/6 Qtly/8 Monthly/18 Wkly/13 Daily/14 Hrly/48 Total\\nDemographic 1,088 1,858 5,728 24 10 0 8,708\\nFinance 6,519 5,305 10,987 164 1,559 0 24,534\\nIndustry 3,716 4,637 10,017 6 422 0 18,798\\nMacro 3,903 5,315 10,016 41 127 0 19,402\\nMicro 6,538 6,020 10,975 112 1,476 0 25,121\\nOther 1,236 865 277 12 633 414 3,437\\nTotal 23,000 24,000 48,000 359 4,227 414 100,000\\nMin. Length 19 24 60 93 107 748\\nMax. Length 841 874 2812 2610 9933 1008\\nMean Length 37.3 100.2 234.3 1035.0 2371.4 901.9\\nSD Length 24.5 51.1 137.4 707.1 1756.6 127.9\\n% Smooth 82% 89% 94% 84% 98% 83%\\n% Erratic 18% 11% 6% 16% 2% 17%\\nA D ATASET DETAILS\\nA.1 M4 D ATASET DETAILS\\nTable 2 outlines the composition of the M4 dataset across domains and forecast horizons by listing the\\nnumber of time series based on their frequency and type (M4 Team, 2018b). The M4 dataset is large\\nand diverse: all forecast horizons are composed of heterogeneous time series types (with exception of\\nHourly) frequently encountered in business, ﬁnancial and economic forecasting. Summary statistics\\non series lengths are also listed, showing wide variability therein, as well as a characterization (smooth\\nvs erratic) that follows Syntetos et al. (2005), and is based on the squared coefﬁcient of variation of\\nthe series. All series have positive observed values at all time-steps; as such, none can be considered\\nintermittent or lumpy per Syntetos et al. (2005).\\nA.2 M3 D ATASET DETAILS\\nTable 3 outlines the composition of the M3 dataset across domains and forecast horizons by listing\\nthe number of time series based on their frequency and type (Makridakis & Hibon, 2000). The\\nM3 is smaller than the M4, but it is still large and diverse: all forecast horizons are composed\\nof heterogeneous time series types frequently encountered in business, ﬁnancial and economic\\nforecasting. Summary statistics on series lengths are also listed, showing wide variability in length,\\nas well as a characterization ( smooth vs erratic) that follows Syntetos et al. (2005), and is based\\non the squared coefﬁcient of variation of the series. All series have positive observed values at all\\ntime-steps; as such, none can be considered intermittent or lumpy per Syntetos et al. (2005).\\nA.3 TOURISM DATASET DETAILS\\nTable 4 outlines the composition of the TOURISM dataset across forecast horizons by listing the\\nnumber of time series based on their frequency. Summary statistics on series lengths are listed,\\nshowing wide variability in length. All series have positive observed values at all time-steps. In\\ncontrast to M4 and M3 datasets, TOURISM includes a much higher fraction of erratic series.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 13, 'page_label': '14', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 3: Composition of the M3 dataset: the number of time series based on their sampling frequency\\nand type.\\nFrequency / Horizon\\nType Yearly/6 Quarterly/8 Monthly/18 Other/8 Total\\nDemographic 245 57 111 0 413\\nFinance 58 76 145 29 308\\nIndustry 102 83 334 0 519\\nMacro 83 336 312 0 731\\nMicro 146 204 474 4 828\\nOther 11 0 52 141 204\\nTotal 645 756 1,428 174 3,003\\nMin. Length 20 24 66 71\\nMax. Length 47 72 144 104\\nMean Length 28.4 48.9 117.3 76.6\\nSD Length 9.9 10.6 28.5 10.9\\n% Smooth 90% 99% 98% 100%\\n% Erratic 10% 1% 2% 0%\\nTable 4: Composition of the TOURISM dataset: the number of time series based on their sampling\\nfrequency.\\nFrequency / Horizon\\nYearly/4 Quarterly/8 Monthly/24 Total\\n518 427 366 1,311\\nMin. Length 11 30 91\\nMax. Length 47 130 333\\nMean Length 24.4 99.6 298\\nSD Length 5.5 20.3 55.7\\n% Smooth 77% 61% 49%\\n% Erratic 23% 39% 51%\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 5: sMAPE on the validation set, generic ar-\\nchitecture. sMAPE for varying number of stacks,\\neach having one residual block.\\nStacks s MAPE\\n1 11.154\\n3 11.061\\n9 10.998\\n18 10.950\\n30 10.937\\nTable 6: sMAPE on the validation set, inter-\\npretable architecture. Ablation of the synergy\\nof the layers with different basis functions and\\nmulti-block stack gain.\\nDetrend Seasonality s MAPE\\n0 2 11.189\\n2 0 11.572\\n1 1 11.040\\n3 3 10.986\\nB A BLATION STUDIES\\nB.1 L AYER STACKING AND BASIS SYNERGY\\nWe performed an ablation study on the validation set, using sMAPE metric as performance criterion.\\nWe addressed two speciﬁc questions with this study. First, Is stacking layers helpful? Second, Does\\nthe architecture based on the combination of layers with different basis functions results in better\\nperformance than the architecture using only one layer type?\\nLayer stacking. We start our study with the generic architecture that consists of stacks of one\\nresidual block of 5 FC layers each of the form Fig. 1 and we increase the number of stacks. Results\\npresented in Table 5 conﬁrm that increasing the number of stacks decreases error and at certain point\\nthe gain saturates. We would like to mention that the network having 30 stack of depth 5 is in fact a\\nvery deep network of total depth 150 layers.\\nBasis synergy. Stacking works well for the interpretable architecture as can be seen in Table 6\\ndepicting the results of ablating the interpretable architecture conﬁguration. Here we experiment\\nwith the architecture that is composed of 2 stacks, stack one is trend model and stack two is the\\nseasonality model. Each stack has variable number of residual blocks and each residual block has 5\\nFC layers. We found that this architecture works best when all weights are shared within stack. We\\nclearly see that increasing the number of layers improves performance. The largest network is 60\\nlayers deep. On top of that, we observe that the architecture that consists of stacks based on different\\nbasis functions wins over the architecture based on the same stack. It looks like chaining stacks of\\ndifferent nature results in synergistic effects. This is logical as function classes that can be modelled\\nby trend and seasonality stacks have small overlap.\\nB.2 E NSEMBLE SIZE\\nFigure 3 demonstrates that increasing the ensemble size results in improved performance. Most\\nimportantly, according to Figure 3, N-BEATS achieves state-of-the-art performance even if compara-\\ntively small ensemble size of 18 models is used. Therefore, computational efﬁciency of N-BEATS\\ncan be traded very effectively for performance and there is no over-reliance of the results on large\\nensemble size.\\nB.3 D OUBLY RESIDUAL STACKING\\nIn Section 3.2 we described the proposed doubly residual stacking (DRESS) principle, which is the\\ntopological foundation of N-BEATS. The topology is based on both (i) running a residual backcast\\nconnection and (ii) producing partial block-level forecasts that are further aggregated at stack and\\nmodel levels to produce the ﬁnal model-level forecast. In this section we conduct a study to conﬁrm\\nthe accuracy effectiveness of this topology compared to several alternatives. The methodology\\nunderlying this study is that we remove either the backcast or partial forecast links or both and track\\nhow this affects the forecasting metrics. We keep the number of parameters in the network for each\\nof the architectural alternatives ﬁxed by using the same number of layers in the network (we used\\ndefault hyperparameter settings reported in Table 18). The architectural alternatives are depicted in\\nFigure 4 and described in detail below.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 15, 'page_label': '16', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n18 36 90 180\\nEnsemble Size\\n0.797\\n0.798\\n0.799\\n0.800\\n0.801\\n0.802OWA\\nFigure 3: M4 test performance (OWA) as a function of ensemble size, based on N-BEATS-G. This\\nﬁgure shows that N-BEATS loses less than 0.5% in terms of OWA performance even if 10 times\\nsmaller ensemble size is used.\\nN-BEATS-DRESSis depicted in Fig. 4a. This is the default conﬁguration of N-BEATS using doubly\\nresidual stacking described in Section 3.2.\\nPARALLEL is depicted in Fig. 4b. This is the alternative where the backward residual connection is\\ndisabled and the overall model input is fed to every block. The blocks then forecast in parallel using\\nthe same input and their individual outputs are summed to make the ﬁnal forecast.\\nNO-RESIDUAL is depicted in Fig. 4c. This is the alternative where the backward residual connection\\nis disabled. Unlike PARALLEL, in this case the backcast forecast of the previous block is fed as input\\nto the next block. Unlike the usual feed-forward network, in the NO-RESIDUAL architecture, each\\nblock makes a partial forecast and their individual outputs are summed to make the ﬁnal forecast.\\nLAST-FORWARDis depicted in Fig. 4d. This is the alternative where the backward residual\\nconnection is active, however the model level forecast is derived only from the last block. So, the\\npartial forward forecasts are disabled. This is the architecture that is closest to the classical residual\\nnetwork.\\nNO-RESIDUAL-LAST-FORWARDis depicted in Fig. 4f. This is the alternative where both\\nbackward residual and the partial forward connections are disabled. This is therefore a simple\\nfeed-forward network, but very deep.\\nThe quantitative ablation study results on the M4 dataset are reported in Tables 7–10. N-BEATS-\\nDRESS model is essentially N-BEATS model in this study. For this study we used ensemble size\\nof 18. Since the ensemble size is 18 for N-BEATS-DRESS, as opposed to 180 used for N-BEATS,\\nthe OWA metric reported in Table 9 for N-BEATS-DRESS is higher than the OWA reported for\\nN-BEATS-G in Table 12. Note that both results align well withOWA reported in Figure 3 for different\\nensemble sizes, as part of the ensemble size ablation conducted in Section B.2.\\nThe results presented in Tables 7–10 demonstrate that the doubly residual stacking topology provides\\na clear overall advantage over the alternative architectures in which either backcast residual links or\\nthe partial forward forecast links are disabled.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 16, 'page_label': '17', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input\\nStack output\\n(to next stack)\\n(a) N-BEATS-DRESS\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n+\\nStack output\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input (b) PARALLEL\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n+\\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input (c) NO-RESIDUAL\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\nGlobal forecast\\n(model output)Model Input\\n(d) LAST-FORW ARD\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\nGlobal forecast\\n(model output)Model Input (e) NO-RESIDUAL-LAST-\\nFORW ARD\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input\\nStack output\\n(to next stack)\\nModel Input\\n(f) RESIDUAL-INPUT\\nFigure 4: The architectural conﬁgurations used in the ablation study of the doubly residual stack.\\nSymbol ⋄denotes unconnected output.\\nTable 7: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\\nthe ensemble of 18 generic models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-G 13.279 9.558 12.510 3.691 11.538\\nNO-RESIDUAL-G 13.195 9.555 12.451 3.759 11.493\\nLAST-FORW ARD-G 13.200 9.322 12.352 3.703 11.387\\nNO-RESIDUAL-LAST-FORW ARD-G 15.386 11.346 15.282 6.673 13.931\\nRESIDUAL-INPUT-G 13.264 9.545 12.316 3.692 11.438\\nN-BEATS-DRESS-G 13.211 9.217 12.122 3.636 11.251\\nTable 8: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\\nthe ensemble of 18 interpretable models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-I 13.207 9.530 12.500 3.710 11.510\\nNO-RESIDUAL-I 13.075 9.707 12.708 4.007 11.637\\nLAST-FORW ARD-I 13.168 9.547 12.111 3.599 11.313\\nNO-RESIDUAL-LAST-FORW ARD-I 13.067 10.207 15.177 4.912 12.986\\nRESIDUAL-INPUT-I 13.104 9.716 12.814 4.005 11.697\\nN-BEATS-DRESS-I 13.155 9.286 12.009 3.642 11.201\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 17, 'page_label': '18', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 9: Performance on the M4 test set, OWA. Lower values are better. The results are obtained on\\nthe ensemble of 18 generic models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-G 0.780 0.832 0.852 0.844 0.822\\nNO-RESIDUAL-G 0.774 0.831 0.851 0.853 0.819\\nLAST-FORW ARD-G 0.774 0.808 0.840 0.846 0.811\\nNO-RESIDUAL-LAST-FORW ARD-G 0.948 1.029 1.095 1.296 1.030\\nRESIDUAL-INPUT-G 0.779 0.831 0.840 0.844 0.817\\nN-BEATS-DRESS-G 0.776 0.800 0.823 0.835 0.803\\nTable 10: Performance on the M4 test set, OWA. Lower values are better. The results are obtained on\\nthe ensemble of 18 interpretable models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-I 0.776 0.831 0.857 0.845 0.821\\nNO-RESIDUAL-I 0.769 0.848 0.886 0.886 0.833\\nLAST-FORW ARD-I 0.773 0.836 0.825 0.817 0.808\\nNO-RESIDUAL-LAST-FORW ARD-I 0.771 0.900 1.085 1.016 0.922\\nRESIDUAL-INPUT-I 0.771 0.848 0.892 0.887 0.836\\nN-BEATS-DRESS-I 0.771 0.805 0.819 0.836 0.800\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 11: Performance on the M4 test set, sMAPE . Lower values are better. Red – second best.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nBest pure ML 14.397 11.031 13.973 4.566 12.894\\nBest statistical 13.366 10.155 13.002 4.682 11.986\\nBest ML/TS combination 13.528 9.733 12.639 4.118 11.720\\nDL/TS hybrid, M4 winner 13.176 9.679 12.126 4.014 11.374\\nN-BEATS-G 13.023 9.212 12.048 3.574 11.168\\nN-BEATS-I 12.924 9.287 12.059 3.684 11.174\\nN-BEATS-I+G 12.913 9.213 12.024 3.643 11.135\\nTable 12: Performance on the M4 test set, OWA and M4 rank. Lower values are better. Red – second\\nbest.\\nYearly Quarterly Monthly Others Average Rank\\n(23k) (24k) (48k) (5k) (100k)\\nBest pure ML 0.859 0.939 0.941 0.991 0.915 23\\nBest statistical 0.788 0.898 0.905 0.989 0.861 8\\nBest ML/TS combination 0.799 0.847 0.858 0.914 0.838 2\\nDL/TS hybrid, M4 winner 0.778 0.847 0.836 0.920 0.821 1\\nN-BEATS-G 0.765 0.800 0.820 0.822 0.797\\nN-BEATS-I 0.758 0.807 0.824 0.849 0.798\\nN-BEATS-I+G 0.758 0.800 0.819 0.840 0.795\\nC D ETAILED EMPIRICAL RESULTS\\nC.1 D ETAILED RESULTS : M4 D ATASET\\nTables 11 and 12 present our key quantitative empirical results showing that the proposed model\\nachieves the state of the art performance on the challenging M4 benchmark. We study the performance\\nof two model conﬁgurations: generic (Ours-G) and interpretable (Ours-I), as well as Ours-I+G\\n(ensemble of all models from Ours-G and Ours-I). We compare against 4 representatives from the\\nM4 competition: each best in their respective model class. Best pure ML is the submission by B.\\nTrotta, the best entry among the 6 pure ML models. Best statistical is the best pure statistical model\\nby N.Z. Legaki and K. Koutsouri. Best ML/TS combination is the model by P. Montero-Manso, T.\\nTalagala, R.J. Hyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few\\nstatistical time series models. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020).\\nN-BEATS outperforms all other approaches on all the studied subsets of time series. The average\\nOWA gap between our generic model and the M4 winner (0.821 −0.795 = 0.026) is greater than the\\ngap between the M4 winner and the second entry (0.838 −0.821 = 0.017).\\nA more granular and detailed statistical analysis of our results on M4 is provided in Table 13. This\\ntable ﬁrst presents the sMAPE for N-BEATS, decomposed by M4 time series sub-type and sampling\\nfrequency (upper part). Then (lower part), it shows the average sMAPE difference between the\\nN-BEATS results and the M4 winner (TS/DL hybrid by S. Smyl), adding the standard error of that\\ndifference (in parentheses); bold entries indicate statistical signiﬁcance at the 99% level based on a\\ntwo-sided paired t-test.\\nWe note that each cross-section of the M4 dataset into horizon and type may be regarded as an\\nindependent mini-dataset. We observe that over those mini-datasets there is a preponderance of\\nstatistically signiﬁcant differences between N-BEATS and Smyl (18 cases out of 31) to the advantage\\nof N-BEATS. This provides evidence that (i) the improvement observed on average in Tables 11\\nand 12 is statistically signiﬁcant and consistent over smaller subsets of M4 and (ii) N-BEATS\\ngeneralizes well over time series of different types and sampling frequencies.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 19, 'page_label': '20', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 13: Performance decomposition on non-overlapping subsets of the M4 test set and comparison\\nwith the Smyl model results.\\nDemographic Finance Industry Macro Micro Other\\nsMAPE per M4 series type and sampling frequency\\nYearly 8 .931 13 .741 16 .317 13 .327 10 .489 13 .320\\nQuarterly 9 .219 10 .787 8 .628 8 .576 9 .264 6 .250\\nMonthly 4 .357 13 .353 12 .657 12 .571 13 .627 11 .595\\nWeekly 4 .580 3 .004 9 .258 7 .220 10 .425 6 .183\\nDaily 6 .351 3 .467 3 .835 2 .525 2 .299 2 .885\\nHourly 8 .197\\nAverage sMAPE difference vs Smyl model, computed as N-BEATS – Smyl.\\nStandard error of the mean displayed in parenthesis.\\nBold entries are signiﬁcant at the 99% level (2-sided paired t-test).\\nYearly −0.749 −0.337 −0.065 −0.386 −0.168 −0.157\\n(0.119) ( 0.065) ( 0.087) ( 0.085) ( 0.056) ( 0.140)\\nQuarterly −0.651 −0.281 −0.328 −0.712 −0.523 −0.029\\n(0.085) ( 0.047) ( 0.043) ( 0.060) ( 0.051) ( 0.083)\\nMonthly −0.185 −0.379 −0.419 0.089 0.338 −0.279\\n(0.023) ( 0.034) ( 0.036) ( 0.039) ( 0.034) ( 0.162)\\nWeekly −0.336 −1.075 −0.937 −1.627 −3.029 −1.193\\n(0.270) ( 0.221) ( 1.399) ( 0.770) ( 0.378) ( 0.772)\\nDaily 0 .191 −0.098 −0.124 −0.026 −0.367 −0.037\\n(0.231) ( 0.018) ( 0.025) ( 0.057) ( 0.013) ( 0.015)\\nHourly −1.132\\n(0.163)\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 14: Performance on the M3 test set, Average sMAPE , aggregate over all forecast horizons\\n(Yearly: 1-6, Quarterly: 1-8, Monthly: 1-18, Other: 1-8, Average: 1-18). Lower values are better.\\nRed – second best. †Numbers are computed by us.\\nYearly Quarterly Monthly Others Average\\n(645) (756) (1428) (174) (3003)\\nNaïve2 17.88 9.95 16.91 6.30 15.47\\nARIMA (B–J automatic) 17.73 10.26 14.81 5.06 14.01\\nComb S-H-D 17.07 9.22 14.48 4.56 13.52\\nForecastPro 17.14 9.77 13.86 4.60 13.19\\nTheta 16.90 8.96 13.85 4.41 13.01\\nDOTM (Fiorucci et al., 2016) 15.94 9.28 13.74 4.58 12.90\\nEXP (Spiliotis et al., 2019) 16.39 8.98 13.43 5.46 12 .71†\\nLGT (Smyl & Kuber, 2016) 15.23 n/a n/a 4.26 n/a\\nBaggedETS.BC (Bergmeir et al., 2016) 17.49 9.89 13.74 n/a n/a\\nN-BEATS-G 16.2 8.92 13.19 4.19 12.47\\nN-BEATS-I 15.84 9.03 13.15 4.30 12.43\\nN-BEATS-I+G 15.93 8.84 13.11 4.24 12.37\\nC.2 D ETAILED RESULTS : M3 D ATASET\\nResults for M3 dataset are provided in Table 14. The performance metric is calculated using the\\nearlier version of sMAPE , deﬁned speciﬁcally for the M3 competition:1\\nsMAPE = 200\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\nyT +i + ˆyT +i\\n. (4)\\nFor some of the methods, either average sMAPE was not reported or sMAPE for some of the splits was\\nnot reported in their respective publications. Below, we list those cases. BaggedETS.BC (Bergmeir\\net al., 2016) has not reported numbers on Others. LGT (Smyl & Kuber, 2016) did not report results on\\nMonthly and Quarterly data. According to the authors, the underlying RNN had problems dealing with\\nraw seasonal data, the ETS based pre-processing was not effective and the LGT pre-processing was\\nnot computationally feasible given comparatively large number of time series and their comparatively\\nlarge length (Smyl & Kuber, 2016). Finally, EXP (Spiliotis et al., 2019) reported average performance\\ncomputed using a different methodology than the default M3 and M4 methodology (source: personal\\ncommunication with the authors). For the latter method we recomputed the Average sMAPE based on\\nthe previously reported Yearly, Quarterly and Monthly splits. To calculate it, we follow the M3, M4\\nand TOURISM competition methodology and compute the average metric as the average over all time\\nseries and over all forecast horizons. Given the performance metric values aggregated over Yearly,\\nQuarterly and Monthly splits, the average can be computed straightforwardly as:\\nsMAPE Average = NYear\\nNTot\\nsMAPE Year+NQuart\\nNTot\\nsMAPE Quart +NMonth\\nNTot\\nsMAPE Month +NOthers\\nNTot\\nsMAPE Others .\\n(5)\\nHere NTot = NYear +NQuart +NMonth +NOthers and NYear = 6 ×645,NQuart = 8 ×756,NMonth = 18 ×\\n1428,NOthers = 8 ×174. It is clear that for each split, its N is the product of its respective number of\\ntime series and its largest forecast horizon.\\n1With minor differences compared to the sMAPE deﬁnition used for M4. Please refer to Appendix A\\nin (Makridakis & Hibon, 2000) for the mathematical deﬁnition.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 15: Performance on the TOURISM test set, Average MAPE , aggregate over all forecast horizons\\n(Yearly: 1-4, Quarterly: 1-8, Monthly: 1-24, Average: 1-24). Lower values are better. Red – second\\nbest.\\nYearly Quarterly Monthly Average\\n(518) (427) (366) (1311)\\nStatistical benchmarks (Athanasopoulos et al., 2011)\\nSNaïve 23.61 16.46 22.56 21.25\\nTheta 23.45 16.15 22.11 20.88\\nForePro 26.36 15.72 19.91 19.84\\nETS 27.68 16.05 21.15 20.88\\nDamped 28.15 15.56 23.47 22.26\\nARIMA 28.03 16.23 21.13 20.96\\nKaggle competitors (Athanasopoulos & Hyndman, 2011)\\nSaliMali n/a 14.83 19.64 n/a\\nLeeCBaker 22.73 15.14 20.19 19.35\\nStratometrics 23.15 15.14 20.37 19.52\\nRobert n/a 14.96 20.28 n/a\\nIdalgo n/a 15.07 20.55 n/a\\nN-BEATS-G (Ours) 21.67 14.71 19.17 18.47\\nN-BEATS-I (Ours) 21.55 15.22 19.82 18.97\\nN-BEATS-I+G (Ours) 21.44 14.78 19.29 18.52\\nC.3 D ETAILED RESULTS : TOURISM DATASET\\nDetailed results for the TOURISM competition dataset are provided in Table 15. The respective Kaggle\\ncompetition was divided into two parts: (i) Yearly time series forecasting and (ii) Quarterly/Monthly\\ntime series forecasting (Athanasopoulos & Hyndman, 2011). Some of the participants chose to\\ntake part only in the second part. Therefore, In addition to entries present in Table 1, we report\\ncompetitors from (Athanasopoulos & Hyndman, 2011) that have missing results in Yearly compe-\\ntition. In particular, SaliMali team is the winner of the Quarterly/Monthly time series forecasting\\ncompetition (Brierley, 2011). Their approach is based on a weighted ensemble of statistical methods.\\nTeams Robert and Idalgo used unknown approaches. We can see from Table 15 that N-BEATS\\nachieves state-of-the-art performance on all subsets of TOURISM dataset. On average, it is state of the\\nart and it gains 4.2% over the best-known approach LeeCBaker, and 11.5% over auto-ARIMA.\\nThe average metrics have not been reported in the original competition results (Athanasopoulos et al.,\\n2011; Athanasopoulos & Hyndman, 2011). Therefore, in Table 15, we present the Average MAPE\\nmetric calculated by us based on the previously reported Yearly, Quarterly and Monthly splits. To\\ncalculate it, we follow the M4 competition methodology and compute the average metric as the\\naverage over all time series and over all forecast horizons. Given the performance metric values\\naggregated over Yearly, Quarterly and Monthly splits, the average can be computed straightforwardly\\nas:\\nMAPE Average = NYear\\nNTot\\nMAPE Year+NQuart\\nNTot\\nMAPE Quart +NMonth\\nNTot\\nMAPE Month . (6)\\nHere NTot = NYear +NQuart +NMonth and NYear = 4 ×518,NQuart = 8 ×427,NMonth = 24 ×366. It is\\nclear that for each split, its N is the product of its respective number of time series and its largest\\nforecast horizon.\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nC.4 D ETAILED RESULTS : ELECTRICITY AND TRAFFIC DATASETS\\nIn this experiment we are comparing the performances of MatFact (Yu et al., 2016), DeepAR (Flunkert\\net al., 2017) (Amazon Labs), Deep State (Rangapuram et al., 2018a) (Amazon Labs), Deep Fac-\\ntors (Wang et al., 2019) (Amazon Labs), and N-BEATS models on ELECTRICITY 2 (Dua & Graff,\\n2017) and TRAFFIC 3 (Dua & Graff, 2017) datasets. The results are presented in in Table 16.\\nBoth datasets are aggregated to hourly data, but using different aggregation operations: sum for\\nELECTRICITY and mean for TRAFFIC . The hourly aggregation is done so that all the points available\\nin (h −1 : 00,h : 00] hours are aggregated to hour h, thus if original dataset starts on 2011-01-01\\n00:15 then the ﬁrst time point after aggregation will be 2011-01-01 01:00. For the ELECTRICITY\\ndataset we removed the ﬁrst year from training set, to match the training set used in (Yu et al., 2016),\\nbased on the aggregated dataset downloaded from, presumable authors’, github repository4. We also\\nmade sure that data points for both ELECTRICITY and TRAFFIC datasets after aggregation match\\nthose used in (Yu et al., 2016). The authors of MatFact model were using the last 7 days of datasets\\nas test set, but papers from Amazon are using different splits, where the split points are provided by a\\ndate. Changing split points without a well grounded reason adds uncertainties to the comparability of\\nthe models performances and creates challenges to the reproducibility of the results, thus we were\\ntrying to match all different splits in our experiments. It was especially challenging on TRAFFIC\\ndataset, where we had to use some heuristics to ﬁnd records dates; the dataset authors state: “ The\\nmeasurements cover the period from Jan. 1st 2008 to Mar. 30th 2009” and “ We remove public\\nholidays from the dataset, as well as two days with anomalies (March 8th 2009 and March 9th 2008)\\nwhere all sensors were muted between 2:00 and 3:00 AM. ” , but we failed to match a part of the\\nprovided labels of week days to actual dates. Therefore, we had to assume that the actual list of gaps,\\nwhich include holidays and anomalous days, is the following:\\n1. Jan. 1, 2008 (New Year’s Day)\\n2. Jan. 21, 2008 (Martin Luther King Jr. Day)\\n3. Feb. 18, 2008 (Washington’s Birthday)\\n4. Mar. 9, 2008 (Anomaly day)\\n5. May 26, 2008 (Memorial Day)\\n6. Jul. 4, 2008 (Independence Day)\\n7. Sep. 1, 2008 (Labor Day)\\n8. Oct. 13, 2008 (Columbus Day)\\n9. Nov. 11, 2008 (Veterans Day)\\n10. Nov. 27, 2008 (Thanksgiving)\\n11. Dec. 25, 2008 (Christmas Day)\\n12. Jan. 1, 2009 (New Year’s Day)\\n13. Jan. 19, 2009 (Martin Luther King Jr. Day)\\n14. Feb. 16, 2009 (Washington’s Birthday)\\n15. Mar. 8, 2009 (Anomaly day)\\nThe ﬁrst 6 gaps were conﬁrmed by the gaps in labels, but the rest were more than 1 day apart from any\\npublic holiday of years 2008 and 2009 in San Francisco, California and US. More over the number of\\ngaps we found in the labels provided by dataset authors is 10, while the number of days between Jan.\\n1st 2008 and Mar. 30th 2009 is 455, assuming that Jan. 1st 2008 was skipped from the values and\\nlabels we should end up with either 454 −10 = 444 instead of 440 days or different end date.\\nThe metric is reported in Normalized deviation (ND) as in (Yu et al., 2016) which is equal to p50\\nloss used in DeepAR, Deep State, and Deep Factors papers.\\n2https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\\n3https://archive.ics.uci.edu/ml/datasets/PEMS-SF\\n4https://github.com/rofuyu/exp-trmf-nips16/blob/master/python/exp-scripts/datasets/download-data.sh\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 23, 'page_label': '24', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nND = ∑i,t |ˆYit −Yit|\\n∑i,t |Yit| (7)\\nTable 16: ND Performance on the ELECTRICITY and TRAFFIC test sets.\\n1 Split used in DeepAR (Flunkert et al., 2017) and Deep State (Rangapuram et al., 2018a).\\n2 Split used in Deep Factors (Wang et al., 2019).\\n†Numbers reported by (Flunkert et al., 2017), which are different from the original MatFact paper,\\nhypothetically due to changed split point.\\nELECTRICITY TRAFFIC\\n2014-09-011 2014-03-312 last 7 days 2008-06-15 1 2008-01-142 last 7 days\\nMatFact 0.16 † n/a 0.255 0.20† n/a 0.187\\nDeepAR 0.07 0.272 n/a 0.17 0.296 n/a\\nDeep State 0.083 n/a n/a 0.167 n/a n/a\\nDeep Factors n/a 0.112 n/a n/a 0.225 n/a\\nN-BEATS-G (ours) 0.064 0.065 0.171 0.114 0.230 0.112\\nN-BEATS-I (ours) 0.073 0.072 0.185 0.114 0.231 0.110\\nN-BEATS-I+G (ours) 0.067 0.067 0.178 0.114 0.230 0.111\\nContrary to Amazon models N-BEATS does not use any covariates, like day-of-week, hour-of-day,\\netc.\\nThe N-BEATS architecture used in this experiment is exactly the same as used in M4, M3 and\\nTOURISM datasets, the only difference is history size and the number of iterations. These parameters\\nwere chosen based on performance on validation set. Where the validation set consists of 7 consecutive\\ndays right before the test set. After the parameters are chosen the model is retrained on training set\\nwhich includes the validation set, then tested on test set. The model is trained once and tested on test\\nset using rolling window operation described in (Yu et al., 2016).\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 24, 'page_label': '25', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nC.5 D ETAILED RESULTS : COMPARE TO DEEPAR, D EEP STATE SPACE MODELS\\nTable 17 compares ND (7) performance of DeepAR, DeepState models published in (Rangapuram\\net al., 2018a) and N-BEATS.\\nTable 17: ND Performance of DeepAR, Deep State Space, and N-BEATS models on M4-Hourly and\\nTOURISM datasets\\nM4 (Hourly) TOURISM (Monthly) TOURISM (Quarterly)\\nDeepAR 0.09 0.107 0.11\\nDeepState 0.044 0.138 0.098\\nN-BEATS-G (ours) 0.023 0.097 0.080\\nN-BEATS-I (ours) 0.027 0.103 0.079\\nN-BEATS-I+G (ours) 0.025 0.099 0.077\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 25, 'page_label': '26', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 18: Settings of hyperparameters across subsets of M4, M3, TOURISM datasets.\\nM4 M3 TOURISM\\nYly Qly Mly Wly Dly Hly Yly Qly Mly Other Yly Qly Mly\\nParameter N-BEATS-I\\nLH 1.5 1.5 1.5 10 10 10 20 5 5 20 20 10 20\\nIterations 15K 15K 15K 5K 5K 5K 50 6K 6K 250 30 500 300\\nLosses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\\nS-width 2048\\nS-blocks 3\\nS-block-layers 4\\nT-width 256\\nT-degree 2\\nT-blocks 3\\nT-block-layers 4\\nSharing STACK LEVEL\\nLookback period 2 H,3H,4H,5H,6H,7H\\nBatch 1024\\nParameter N-BEATS-G\\nLH 1.5 1.5 1.5 10 10 10 20 20 20 10 5 10 20\\nIterations 15K 15K 15K 5K 5K 5K 20 250 10K 250 30 100 100\\nLosses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\\nWidth 512\\nBlocks 1\\nBlock-layers 4\\nStacks 30\\nSharing NO\\nLookback period 2 H,3H,4H,5H,6H,7H\\nBatch 1024\\nD H YPER -PARAMETER SETTINGS\\nTable 18 presents the hyperparameter settings used to train models on different subsets of M4, M3\\nand TOURISM datasets. A brief discussion of ﬁeld names in the table is warranted.\\nSubset names Yly, Qly, Mly, Wly, Dly, Hly, Othercorrespond to yearly, quarterly, monthly, weekly,\\ndaily, hourly and other frequency subsets deﬁned in the original datasets.\\nN-BEATS-I and N-BEATS-G correspond to the interpretable and generic model conﬁgurations\\ndeﬁned in Section 3.3.\\nD.1 C OMMON PARAMETERS\\nLH is the coefﬁcient deﬁning the length of training history immediately preceding the last point in\\nthe train part of the TS that is used to generate training samples. For example, if for M4 Yearly the\\nforecast horizon is 6 and LH is 1.5, then we consider 1.5 ·6 = 9 most recent points in the train dataset\\nfor each time series to generate training samples. A training sample from a given TS in M4 Yearly is\\nthen generated by choosing one of the most recent 9 points as an anchor. All the points preceding the\\nanchor are used to create the input to N-BEATS, while the points following and including the anchor\\nbecome training target. Target and history points that fall outside of the time series limits given the\\nanchor position are ﬁlled with zeros and masked during the training. We observed that for subsets\\nwith large number of time series LH tends to be smaller and for subsets with smaller number of time\\nseries it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4LH is\\nequal to 1.5; and in moderate to small Weekly, Daily, Hourly subsets of M4LH is equal to 10.\\nIterations is the number of batches used to train N-BEATS.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nLosses is the set of loss functions that is used to build ensemble. We observed on the respective\\nvalidation sets that for M4 and M3 mixing models trained on a variety of metrics resulted in\\nperformance gain. In the case of TOURISM dataset training only on MAPE led to the best validation\\nscores.\\nSharing deﬁnes whether the coefﬁcients in the fully-connected layers are shared. We observed that\\nthe interpretable model works best when weights are shared across stack, while generic model works\\nbest when none of the weights are shared.\\nLookback periodis the length of the history window forming the input to the model (please refer to\\nFigure 1). This is the function of the forecast horizon length, H. In our experiments we mixed models\\nwith lookback periods 2H,3H,4H,5H,6H,7H in one ensemble. As an example, for a forecast\\nhorizon length H = 8 and a lookback period 7H, the model’s input will consist of the history window\\nof 7 ·8 = 56 samples.\\nBatch is the batch size. We used batch size of 1024. We observed that the training was faster with\\nlarger batch sizes, however in our setup little gain was observed with batch sizes beyond 1024.\\nD.2 N-BEATS-I PARAMETERS\\nS-width is the width of the fully connected layers in the blocks comprising the seasonality stack of\\nthe interpretable model (please refer to Figure 1).\\nS-blocks is the number of blocks comprising the seasonality stack of the interpretable model (please\\nrefer to Figure 1).\\nS-block-layers is the number of fully-connected layers comprising one block in the seasonality\\nstack of the interpretable model (preceding the ﬁnal fully-connected projection layers forming the\\nbackcast/forecast fork, please refer to Figure 1).\\nT-width is the width of the fully connected layers in the blocks comprising the trend stack of the\\ninterpretable model (please refer to Figure 1).\\nT-degree is the degree p of polynomial in the trend stack of the interpretable model (please refer to\\nequation (2)).\\nT-blocks is the number of blocks comprising the trend stack of the interpretable model (please refer\\nto Figure 1).\\nT-block-layers is the number of fully-connected layers comprising one block in the trend stack\\nof the interpretable model (preceding the ﬁnal fully-connected projection layers forming the back-\\ncast/forecast fork, please refer to Figure 1).\\nD.3 N-BEATS-G PARAMETERS\\nWidth is the width of the fully connected layers in the blocks comprising the stacks of the generic\\nmodel (please refer to Figure 1).\\nBlocks is the number of blocks comprising the stack of the generic model (please refer to Figure 1).\\nBlock-layers is the number of fully-connected layers comprising one block in the stack of the generic\\nmodel (preceding the ﬁnal fully-connected projection layers forming the backcast/forecast fork,\\nplease refer to Figure 1).\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n0 1 2 3 4 5\\nt\\n0.8\\n0.9\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90 STACK1-G\\n0 1 2 3 4 5\\nt\\n0.025\\n0.050\\n0.075 STACK2-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90\\n0.95\\nSTACK1-I\\n0 1 2 3 4 5\\nt\\n0.02\\n0.03\\n0.04\\n0.05 STACK2-I\\n0 2 4 6\\nt\\n0.85\\n0.90\\n0.95\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6\\nt\\n0.86\\n0.88\\n0.90 STACK1-G\\n0 2 4 6\\nt\\n0.025\\n0.000\\n0.025\\n0.050\\nSTACK2-G\\n0 2 4 6\\nt\\n0.88\\n0.89\\n0.90\\nSTACK1-I\\n0 2 4 6\\nt\\n0.05\\n0.00\\n0.05\\nSTACK2-I\\n0 5 10 15\\nt\\n0.4\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 5 10 15\\nt\\n0.8\\n0.9 STACK1-G\\n0 5 10 15\\nt\\n0.1\\n0.0\\nSTACK2-G\\n0 5 10 15\\nt\\n0.85\\n0.90\\nSTACK1-I\\n0 5 10 15\\nt\\n0.3\\n0.2\\n0.1\\n0.0 STACK2-I\\n0 2 4 6 8 10 12\\nt\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80\\nSTACK1-G\\n0 2 4 6 8 10 12\\nt\\n0.000\\n0.025\\n0.050\\n0.075\\nSTACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.002\\n0.001\\n STACK2-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.0003\\n0.0002\\n0.0001\\n STACK2-I\\n0 10 20 30 40\\nt\\n0.25\\n0.50\\n0.75\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n(a) Combined\\n0 10 20 30 40\\nt\\n0.2\\n0.4\\n0.6\\nSTACK1-G (b) Stack1-G\\n0 10 20 30 40\\nt\\n0.02\\n0.00\\nSTACK2-G (c) Stack2-G\\n0 10 20 30 40\\nt\\n0.36\\n0.38\\n0.40\\nSTACK1-I (d) StackT-I\\n0 10 20 30 40\\nt\\n0.2\\n0.0\\n0.2\\nSTACK2-I (e) StackS-I\\nFigure 5: The outputs of generic and the interpretable conﬁgurations, M4 dataset. Each row is one\\ntime series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\\nare normalized by the maximal value of the actual time series for convenience. Column (a) shows the\\nactual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\\nforecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\\nrespectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\\nand the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\\nE D ETAILED SIGNAL TRACES OF INTERPRETABLE INPUTS PRESENTED IN\\nFIGURE 2\\nThe goal of this section is to show the detailed traces (numeric values) of signals visualized in Fig. 2.\\nThis is to demonstrate that even though the StackT-I (Fig. 2 (d)) and StackS-I (Fig. 2 (e)) provide\\nresponse lines different from the counterparts in Stack1-G (Fig. 2 (b)) and Stack2-G (Fig. 2 (c)), the\\nsummations in the combined line (Fig. 2 (a)) can still be very similar.\\nFirst, we reproduce Fig. 5 for the convenience of the reader. Second, for each row in the ﬁgure, we\\nproduce a table showing the numeric values of each signal depicted in corresponding plots (please\\nrefer to Tables 19– 24). We make sure that the names of signals in ﬁgure legends and in the table\\ncolumns match, such that they can easily be cross-referenced. It can be clearly seen in Tables 19– 24\\nthat (i) traces STACK1-I and STACK2-I sum up to trace FORECAST-I, (ii) traces STACK1-G and\\nSTACK2-G sum up to trace FORECAST-G, (iii) traces FORECAST-I and FORECAST-G are overall\\nvery similar even though their components may signiﬁcantly differ from each other.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 19: Detailed traces of signals depicted in row 1 of Fig. 5, corresponding to the time series\\nYearly: id Y3974.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.780182 0.802068 0.806608 0.781290 0.020778 0.801294 0.005314\\n1 0.802337 0.829223 0.841406 0.798422 0.030801 0.825271 0.016135\\n2 0.840317 0.863683 0.883136 0.820196 0.043487 0.853114 0.030022\\n3 0.889376 0.905962 0.929258 0.850250 0.055712 0.880833 0.048425\\n4 0.930521 0.947028 0.967846 0.892221 0.054807 0.904393 0.063453\\n5 0.976414 0.982307 1.000000 0.949748 0.032559 0.921360 0.078640\\nTable 20: Detailed traces of signals depicted in row 2 of Fig. 5, corresponding to the time series\\nQuarterly: id Q11588.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.830068 0.835964 0.829417 0.880435 -0.044471 0.852018 -0.022601\\n1 0.927155 0.898949 0.891168 0.881626 0.017324 0.880124 0.011044\\n2 0.979204 0.957379 0.948799 0.882549 0.074831 0.907149 0.041650\\n3 0.857250 0.900612 0.891967 0.883830 0.016782 0.877959 0.014008\\n4 0.895082 0.857230 0.847029 0.886096 -0.028866 0.852232 -0.005204\\n5 0.981590 0.923832 0.911001 0.889972 0.033860 0.881140 0.029861\\n6 1.000000 0.978128 0.965236 0.896085 0.082043 0.907475 0.057761\\n7 0.910528 0.920632 0.915460 0.905062 0.015571 0.886941 0.028519\\nTable 21: Detailed traces of signals depicted in row 3 of Fig. 5, corresponding to the time series\\nMonthly: id M19006.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 1.000000 0.923394 0.928279 0.944660 -0.021266 0.922835 0.005444\\n1 0.865248 0.822588 0.829924 0.937575 -0.114987 0.867619 -0.037695\\n2 0.638298 0.693820 0.717119 0.930295 -0.236475 0.810818 -0.093699\\n3 0.531915 0.594375 0.612377 0.922890 -0.328515 0.757199 -0.144823\\n4 0.468085 0.579403 0.595221 0.915428 -0.336025 0.747151 -0.151930\\n5 0.539007 0.602615 0.620809 0.907977 -0.305362 0.755078 -0.134269\\n6 0.581560 0.653387 0.682669 0.900606 -0.247219 0.774561 -0.091891\\n7 0.666667 0.747440 0.765814 0.893385 -0.145945 0.799594 -0.033781\\n8 0.737589 0.817883 0.835577 0.886382 -0.068498 0.817218 0.018359\\n9 0.765957 0.862568 0.856962 0.879665 -0.017097 0.822099 0.034862\\n10 0.851064 0.873448 0.880074 0.873304 0.000145 0.833473 0.046601\\n11 0.893617 0.878186 0.871103 0.867367 0.010819 0.829537 0.041566\\n12 0.858156 0.834448 0.853549 0.861923 -0.027475 0.816527 0.037022\\n13 0.695035 0.785341 0.776687 0.857040 -0.071699 0.782536 -0.005850\\n14 0.446809 0.662443 0.697788 0.852789 -0.190345 0.745623 -0.047835\\n15 0.382979 0.623196 0.624614 0.849236 -0.226040 0.711553 -0.086939\\n16 0.453901 0.598511 0.625150 0.846451 -0.247941 0.712130 -0.086980\\n17 0.539007 0.668231 0.652175 0.844504 -0.176272 0.716925 -0.064750\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 29, 'page_label': '30', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 22: Detailed traces of signals depicted in row 4 of Fig. 5, corresponding to the time series\\nWeekly: id W246.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.630056 0.629703 0.625108 0.639236 -0.009534 0.625416 -0.000309\\n1 0.607536 0.643509 0.639846 0.647549 -0.004039 0.639592 0.000254\\n2 0.641731 0.656171 0.652584 0.656696 -0.000526 0.643665 0.008919\\n3 0.628783 0.669636 0.661163 0.666739 0.002897 0.652107 0.009056\\n4 0.816799 0.687287 0.683860 0.677738 0.009549 0.662176 0.021683\\n5 0.817020 0.709211 0.717187 0.689752 0.019459 0.686589 0.030598\\n6 0.766724 0.731732 0.742824 0.702841 0.028891 0.705234 0.037590\\n7 0.770320 0.750834 0.755154 0.717066 0.033768 0.716986 0.038167\\n8 0.794113 0.769671 0.778460 0.732487 0.037184 0.731113 0.047347\\n9 0.874011 0.793373 0.810332 0.749164 0.044209 0.750939 0.059392\\n10 1.000000 0.816386 0.847545 0.767157 0.049229 0.776405 0.071140\\n11 0.979251 0.834532 0.858604 0.786526 0.048006 0.783939 0.074665\\n12 0.933160 0.850010 0.866116 0.807332 0.042678 0.792134 0.073982\\nTable 23: Detailed traces of signals depicted in row 5 of Fig. 5, corresponding to the time series\\nDaily: id D404.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.968704 0.972314 0.971950 0.972589 -0.000275 0.972964 -0.001014\\n1 0.954319 0.972637 0.972131 0.972808 -0.000171 0.972822 -0.000690\\n2 0.954599 0.972972 0.972188 0.973060 -0.000088 0.973798 -0.001610\\n3 0.959959 0.973230 0.972140 0.973341 -0.000112 0.973686 -0.001546\\n4 0.975472 0.973481 0.972125 0.973649 -0.000168 0.974060 -0.001934\\n5 0.970391 0.973715 0.972174 0.973979 -0.000264 0.974800 -0.002626\\n6 0.977728 0.974056 0.972403 0.974328 -0.000272 0.974368 -0.001965\\n7 0.985624 0.974445 0.972428 0.974693 -0.000248 0.973870 -0.001442\\n8 0.979695 0.974823 0.972567 0.975069 -0.000246 0.974870 -0.002303\\n9 0.985345 0.975079 0.973089 0.975455 -0.000376 0.975970 -0.002881\\n10 0.983088 0.975547 0.973881 0.975845 -0.000298 0.975796 -0.001915\\n11 0.983368 0.975991 0.974537 0.976238 -0.000247 0.976757 -0.002220\\n12 0.998312 0.976365 0.974924 0.976628 -0.000263 0.977579 -0.002655\\n13 1.000000 0.976821 0.975291 0.977013 -0.000193 0.977213 -0.001922\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 24: Detailed traces of signals depicted in row 6 of Fig. 5, corresponding to the time series\\nHourly: id H344.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.226804 0.256799 0.277159 0.346977 -0.090179 0.280489 -0.003329\\n1 0.175258 0.228913 0.234605 0.347615 -0.118701 0.241790 -0.007185\\n2 0.164948 0.209208 0.207347 0.348265 -0.139057 0.218575 -0.011228\\n3 0.164948 0.197360 0.193084 0.348928 -0.151568 0.208458 -0.015374\\n4 0.216495 0.190397 0.186586 0.349606 -0.159209 0.205701 -0.019115\\n5 0.195876 0.194204 0.189433 0.350297 -0.156094 0.214399 -0.024966\\n6 0.319588 0.221026 0.216221 0.351004 -0.129978 0.241574 -0.025353\\n7 0.226804 0.279857 0.276414 0.351726 -0.071869 0.293580 -0.017167\\n8 0.371134 0.357292 0.359372 0.352464 0.004828 0.364392 -0.005020\\n9 0.536082 0.438540 0.446126 0.353218 0.085322 0.442703 0.003423\\n10 0.711340 0.511441 0.519928 0.353989 0.157452 0.510142 0.009787\\n11 0.752577 0.571604 0.578186 0.354777 0.216827 0.571596 0.006590\\n12 0.783505 0.617085 0.618778 0.355584 0.261501 0.613425 0.005353\\n13 0.773196 0.651777 0.655123 0.356409 0.295368 0.649259 0.005864\\n14 0.618557 0.670202 0.676814 0.357253 0.312950 0.669555 0.007260\\n15 0.793814 0.679884 0.692592 0.358116 0.321768 0.684208 0.008384\\n16 0.793814 0.672488 0.696440 0.359000 0.313488 0.684764 0.011676\\n17 0.680412 0.648851 0.677696 0.359904 0.288947 0.662714 0.014983\\n18 0.525773 0.602496 0.630922 0.360828 0.241667 0.620368 0.010554\\n19 0.505155 0.537698 0.552296 0.361775 0.175923 0.552599 -0.000304\\n20 0.701031 0.463760 0.466442 0.362743 0.101016 0.477429 -0.010987\\n21 0.484536 0.395795 0.390958 0.363734 0.032061 0.408708 -0.017750\\n22 0.247423 0.337809 0.338500 0.364748 -0.026939 0.354028 -0.015528\\n23 0.371134 0.292452 0.303902 0.365786 -0.073334 0.312588 -0.008686\\n24 0.216495 0.254359 0.258435 0.366848 -0.112489 0.270568 -0.012133\\n25 0.412371 0.227557 0.224291 0.367934 -0.140377 0.237846 -0.013555\\n26 0.237113 0.207962 0.201250 0.369046 -0.161084 0.219420 -0.018169\\n27 0.206186 0.196049 0.189439 0.370183 -0.174133 0.209743 -0.020304\\n28 0.206186 0.189030 0.182843 0.371346 -0.182316 0.207727 -0.024884\\n29 0.237113 0.194524 0.185734 0.372536 -0.178011 0.213194 -0.027460\\n30 0.206186 0.220227 0.215444 0.373753 -0.153526 0.242485 -0.027041\\n31 0.329897 0.279614 0.274624 0.374998 -0.095383 0.292834 -0.018210\\n32 0.371134 0.355078 0.358020 0.376270 -0.021193 0.365332 -0.007312\\n33 0.494845 0.437103 0.445832 0.377572 0.059531 0.441323 0.004510\\n34 0.690722 0.509515 0.520006 0.378903 0.130612 0.512064 0.007942\\n35 0.989691 0.570761 0.579003 0.380263 0.190497 0.569851 0.009152\\n36 1.000000 0.615868 0.623981 0.381654 0.234214 0.617254 0.006728\\n37 0.845361 0.651487 0.656782 0.383076 0.268411 0.650336 0.006446\\n38 0.742268 0.670664 0.678412 0.384528 0.286136 0.673055 0.005357\\n39 0.721649 0.680534 0.691961 0.386013 0.294521 0.684347 0.007614\\n40 0.567010 0.671607 0.692853 0.387530 0.284078 0.683297 0.009555\\n41 0.546392 0.648851 0.672476 0.389079 0.259771 0.660613 0.011863\\n42 0.432990 0.599785 0.621940 0.390662 0.209123 0.615426 0.006514\\n43 0.391753 0.537520 0.544543 0.392279 0.145241 0.549961 -0.005417\\n44 0.443299 0.462772 0.457700 0.393930 0.068842 0.471080 -0.013380\\n45 0.422680 0.397098 0.380324 0.395616 0.001482 0.401229 -0.020905\\n46 0.381443 0.342213 0.325583 0.397337 -0.055124 0.347827 -0.022244\\n47 0.257732 0.297711 0.287130 0.399094 -0.101384 0.304270 -0.017140\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware\\nSequential Autoencoder\\nTiexin Qin1 Shiqi Wang1 Haoliang Li1\\nAbstract\\nDomain generalization aims to improve the gen-\\neralization capability of machine learning sys-\\ntems to out-of-distribution (OOD) data. Exist-\\ning domain generalization techniques embark\\nupon stationary and discrete environments to\\ntackle the generalization issue caused by OOD\\ndata. However, many real-world tasks in non-\\nstationary environments ( e.g., self-driven car\\nsystem, sensor measures) involve more complex\\nand continuously evolving domain drift, which\\nraises new challenges for the problem of do-\\nmain generalization. In this paper, we formu-\\nlate the aforementioned setting as the problem of\\nevolving domain generalization. Speciﬁcally, we\\npropose to introduce a probabilistic framework\\ncalled Latent Structure-aware Sequential Autoen-\\ncoder (LSSAE) to tackle the problem of evolving\\ndomain generalization via exploring the under-\\nlying continuous structure in the latent space of\\ndeep neural networks, where we aim to identify\\ntwo major factors namely covariate shift and con-\\ncept shift accounting for distribution shift in non-\\nstationary environments. Experimental results on\\nboth synthetic and real-world datasets show that\\nLSSAE can lead to superior performances based\\non the evolving domain generalization setting.\\n1. Introduction\\nThe success of machine learning techniques typically lies on\\nthe assumption that training data and test data are sampled\\nindependently and identically from similar distributions.\\nHowever, this assumption does not hold when deploying the\\ntrained model in many real-world environments where the\\ndistribution of test data varies from training data. This dis-\\ntribution discrepancy, so-called distribution shift, can lead\\n1City University of Hong Kong, Hong Kong. Correspondence\\nto: Haoliang Li <haoliang.li1991@gmail.com>.\\nProceedings of the 39 th International Conference on Machine\\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-\\nright 2022 by the author(s).\\nto the dramatic performance decrease of machine learning\\nmodels (Torralba & Efros, 2011). To mitigate this issue,\\ndomain generalization (DG) has been proposed to learn a\\nmore robust model which can be better generalized to OOD\\ndata (Muandet et al., 2013; Balaji et al., 2018; Li et al.,\\n2018b).\\nWhile some progress is being achieved so far, existing DG\\nmethods are limited to the setting of generalization among\\ndiscrete and stationary environments. This setting can be\\nproblematic in some real-world applications where we re-\\nquire that model can be generalized among continuous do-\\nmains (Hoffman et al., 2014). For example, a self-driving\\ncar system, when deployed in the real world, struggles to\\nperform under an open environment where the accepted data\\nchanges naturally according to the geographic location, time\\nintervals, and other factors in a gradual manner (Hoffman\\net al., 2014). For another example, the measures of sen-\\nsors can also drift over time due to the outer environments\\nand inter factors, such as aging (Vergara et al., 2012). In\\nthese scenarios, treating each domain in a separate manner\\nis unlikely to yield the desired performance as it does not\\nconsider the property of continuous domain structure.\\nAnother limitation of most of the existing DG methods is\\nthat they did not take “concept shift” into consideration.\\nSuch concept shift can also lead to performance drop (Fed-\\nerici et al., 2021). A typical example of concept shift would\\nbe that the incidence rate of a particular disease in cer-\\ntain groups may change over time due to the development\\nof treatments and preventive measures. Therefore, exist-\\ning DG techniques may fail to be applied to some other\\ncomplex real-world applications in non-stationary environ-\\nments (Sugiyama et al., 2013; Tahmasbi et al., 2021).\\nIn this paper, we propose to focus on the problem of domain\\ngeneralization based on the non-stationary setting, where\\ndata can evolve gradually with both covariate shift and con-\\ncept shift. Particularly, we formulate this non-stationary\\nscenario as evolving domain generalization where we only\\nhave access to adequate labeled examples from the sequen-\\ntial source domains. Our objective is to develop algorithms\\nthat can explore the underlying continuous structure of dis-\\ntribution shift and generalize well to evolving target domains\\nwhere the samples are unavailable during the training stage.\\narXiv:2205.07649v2  [cs.LG]  16 Jun 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nUnlike existing DG methods that only focus on covariate\\nshift based on the stationary environment, in this paper, we\\npropose a novel framework called Latent Structure-aware\\nSequential Autoencoder (LSSAE), a dynamic probabilistic\\nframework to model the underlying latent variables across\\ndomains. More speciﬁcally, we propose to use two latent\\nvariables to represent the sampling bias in data sample space\\n(i.e., covariate shift) and data category space (i.e., concept\\nshift), and propose a domain-related module and a category-\\nrelated module to infer their dynamic transition functions\\nbased on different time stamps. We conduct extensive ex-\\nperiments to verify that our framework can successfully\\ncapture the underlying covariate shift and interpret the con-\\ncept shift simultaneously. Last but not least, we show that\\nour proposed LSSAE has a promising generation capability\\nto predict unseen target domains, which can be helpful to\\nthe problem related to sequential data generation. The main\\ncontributions of this paper are summarized as follows.\\n• We propose to focus on the problem of non-stationary\\nevolving domain generalization where both covariate\\nshift and concept shift may exist in the setting.\\n• We propose a novel probabilistic framework LSSAE\\nwhich incorporates variational inference to identify the\\ncontinuous latent structures of these two shifts sepa-\\nrately and simultaneously.\\n• We provide empirical results to show that the proposed\\napproach yield better results than other DG methods\\nacross scenarios. Besides, it presents a powerful gener-\\nation ability of predicting unseen evolving domains.\\n2. Related Work\\nDomain Generalization (DG). The goal of DG is to learn\\nrobust models which can generalize well towards the out-\\nof-distribution samples from unseen domains. Existing DG\\nmethods commonly rely on multiple source domains to learn\\nrepresentative features that can be better generalized. Ac-\\ncording to various strategies used to learn these representa-\\ntions, we can roughly categorize them into three catogories.\\nThe ﬁrst type is feature-based methods, which aim to learn\\ndomain-invariant representation which can be better gener-\\nalized to target domains. Speciﬁcally, it can be achieved by\\naligning the distribution of representations from all source\\ndomains (Blanchard et al., 2011; Li et al., 2018b; Albu-\\nquerque et al., 2021) and feature disentanglement (Ilse et al.,\\n2020; Wang et al., 2021; Nguyen et al., 2021). The second\\ncategory is meta-learning based methods, which utilize the\\nmodel agnostic training procedure to stimulate the train/test\\nshift for acquiring generalized models (Li et al., 2018a; Bal-\\naji et al., 2018; Dou et al., 2019). Last but not least, data\\naugmentation-based techniques, which aim to manipulate\\nthe perturbation both in original images and features to stim-\\nulate the unseen target domains, can also beneﬁt the problem\\nof domain generalization (V olpi et al., 2018; Shankar et al.,\\n2018; Zhou et al., 2021).\\nContinuous Domain Adaptation (CDA).The problem of\\ncontinuous domain adaptation (i.e., evolving domain adap-\\ntation) has attracted increasing attention recently, where\\nthe CDA methods can be categorized into the intermediate-\\ndomains based methods (Kumar et al., 2020; Gong et al.,\\n2019; Chen & Chao, 2021), domain manifold based meth-\\nods (Hoffman et al., 2014; Li et al., 2017), adversary-based\\napproaches (Wang et al., 2020; Wulfmeier et al., 2018), and\\nmeta-learning based methods (Liu et al., 2020; Lao et al.,\\n2020). More or less, they require some samples from tar-\\nget domains for adaptation. In Mancini et al. (2019), they\\npropose to substitute this reliance with some metadata from\\ntarget domains as additional supervision. Instead, our focus\\nis continuous domain generalization where no information\\nfrom target domains is accessible for model learning, which\\nis a more challenging but realistic task for real-world appli-\\ncations.\\nSequential Data Generation.Recent process in unsuper-\\nvised sequence generation (Yingzhen & Mandt, 2018; Han\\net al., 2021; Park et al., 2021) suggests the importance of\\ndecoupling time-invariant and time-variant information dur-\\ning the representation learning procedure. However, these\\napproaches only take sequence generation tasks into consid-\\neration and fail to consider the category-related information,\\nwhich is important for the problem of domain generalization.\\nUnlike these approaches, we propose jointly focusing on\\nthe dynamic modeling of time-variant information on both\\ndata sample space and category space across domains.\\n3. Methodology\\nIn this section, we ﬁrst formalize the problem of evolving\\ndomain generalization based on the non-stationary environ-\\nment and then describe our framework LSSAE for address-\\ning this problem. After that, we will provide theoretical\\nanalysis for the proposed framework in Sec. 3.3 and imple-\\nmentation in Sec. 3.4.\\n3.1. Problem Formulation\\nSuppose we are given T sequentially arriving source do-\\nmains S = {D1,D2,..., DT }, where each domain Dt =\\n{(xt,i,yt,i)}nt\\ni=1 is comprised of nt labeled samples for\\nt ∈ {1,2,...,T }. The goal of our problem setting is\\nto train a classiﬁcation model on S which can be gen-\\neralized to M following arriving target domains T =\\n{DT+1,DT+2,..., DT+M }, Dt = {(xt,i)}nt\\ni=1 (t ∈{T +\\n1,T + 2,...,T + M}), which are not available during\\ntraining stage. For simplicity, we omit the index iwhen-\\never xi refers to a single data point. To further quantify\\nthe continuously evolving nature of domains, we denote'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nX Y\\nW\\nX Y\\nW\\nV\\nXt- 1 Yt- 1\\nWt- 1\\nVt- 1\\nXt Yt\\nWt\\nVt /gid00473/gid00473/gid00473\\n(a)\\n(b)\\n(c)\\n(d)\\n/gid01141/gid01141/gid01141\\nXt- 1 Yt- 1\\nWt- 1\\nX t Yt\\nWt /gid00473/gid00473/gid00473\\n/gid00473/gid00473/gid00473\\n/gid00473/gid00473/gid00473 \\n/gid00473/gid00473/gid00473\\nFigure 1.Comparison of causality diagram for stationary and non-\\nstationary domain generalization scenarios. (a) represents the\\nstandard stationary DG settings which only contains covariate shift\\n(P(X) varies for source and target domains). (b) is an extension\\nversion of (a) which contains both covariate shift and concept shift\\n(P(Y |X) varies). (c) and (d) are corresponding non-stationary\\nDG settings where there exist evolving patterns among adjacent\\ndomains.\\n0 ≤D(Dt,Dt+1) ≤ϵ for two consecutive domains un-\\nder some distribution distance function D (e.g., Kullback-\\nLeibler distance). In other words, the discrepancy between\\ntwo consecutive domains is bounded.\\nConventional DG setting only assumes that P(X)\\nvaries (i.e., covariate shift) for different domains (See\\nFig. 1 (a)), which may not be ideal since both P(X) and\\nP(Y|X) can be non-stationary (i.e., P(X) and P(Y|X)\\nvary over time which lead to evolving covariate shift and\\nconcept shift, respectively). To tackle this problem, in this\\npaper, we aim to explore the evolving patterns of covariate\\nshift and concept shift across domains.\\n3.2. LSSAE: Latent Structure-aware Sequential\\nAutoencoder\\nTo model the dynamic in non-stationary systems, we con-\\nsider two independent factors W and V (i.e., W ⊥ ⊥V)\\nwhich account for the distribution drift in data sample space\\n(i.e., covariate shift) and data category space (i.e., con-\\ncept shift) respectively according to different time stamps\\n(See Fig. 1 (d)). For data (xt,yt) collected at time stamp t,\\nwe denote zw\\nt and zv\\nt as the latent variables of W and V at\\ntime stamp t. For completeness, we further consider time-\\ninvariant latent code zc to capture the static information of\\nxt. We thus can deﬁne a probabilistic generative model for\\nthe joint distribution of all source domains as\\np(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\n= p(x1:T ,zw\\n1:T ,zc)p(y1:T ,zv\\n1:T |zc). (1)\\nwhere the ﬁrst term p(x1:T ,zw\\n1:T ,zc) and second term\\np(y1:T ,zv\\n1:T |x1:T ) can be formulated by using Markov\\nchain model as\\np(x1:T ,zw\\n1:T ,zc) =p(zc)\\nT∏\\nt=1\\np(zw\\nt |zw\\n<t) p(xt|zc,zw\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\ncovariate shift\\n,\\n(2)\\np(y1:T ,zv\\n1:T |zc) =\\nT∏\\nt=1\\np(zv\\nt |zv\\n<t) p(yt|zc,zv\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\nconcept shift\\n, (3)\\nand p(xt|zc,zw\\nt ) and p(yt|zc,zv\\nt ) denote covariate shift and\\nconcept shift, respectively. Eq. 2 shows that the generation\\nprocess of domains data xt at time stamp tdepends on the\\ncorresponding dynamic latent code zw\\nt and static code zc,\\nand Eq. 3 shows that the inference process (i.e., classiﬁer to\\nproduce yt) rely on the corresponding zv\\nt and zc. Our ob-\\njective is to learn the classiﬁer p(yt|zc,zv\\nt ) which disposes\\nof covariate shift through zc and concept shift with dynamic\\nzv\\nt for the problem of evolving domain generalization.\\nDomain-related module for covariate shift.To model\\np(x1:T ,zw\\n1:T ,zc) where covariate shift involved, we set\\nthe prior distribution as p(zc) = N(0,I), p(zw\\nt |zw\\n<t) =\\nN(µ(zw\\nt ),σ2(zw\\nt )) which can be parameterized by some re-\\ncurrent neural networks (e.g., LSTM (Hochreiter & Schmid-\\nhuber, 1997)) by setting zw\\n0 = 0 for initial state when t= 0,\\nand p(xt|zc,zw\\nt ) as a conditional decoder for the reconstruc-\\ntion of input data xt. To approximate the prior distributions\\np(zw\\nt |zw\\n<t), we propose to use variational inference to learn\\nan approximate posterior distribution qover latent variables\\ngiven data which can be formulated as\\nq(zw\\n1:T ,zc|x1:T ) =q(zc|x1:T )\\nT∏\\nt=1\\nq(zw\\nt |zw\\n<t,xt), (4)\\nwhere q(zc|x1:T ) and q(zw\\nt |zw\\n<t,xt) can be also parameter-\\nized by neural networks. The objective function for latent\\nfeature representation learning can be derived based on the\\nevidence lower bound (ELBO) form (Kingma & Welling,\\n2014) given as\\nLd =\\nT∑\\nt=1\\nEq(zc|xt)q(zw\\nt |zw\\n<t,xt)\\n[\\nlog p(xt|zc,zw\\nt )\\n]\\n−λ1DKL(q(zc|x1:T ),p(zc))\\n−λ2\\nT∑\\nt=1\\nDKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t)),\\n(5)\\nwhere the ﬁrst term denotes the reconstruction term for input\\ndata xt, the second and third term denote KL divergence\\nwhich are to align the posterior distributions zc and zw\\nt with\\nthe corresponding prior distributions.\\nCategory-related module for concept shift. To model\\np(y1:T ,zv\\n1:T |zc) for classiﬁcation purpose where concept'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nshift is involved, we propose to introduce another module\\nwhich can be easily integrated into our uniﬁed framework\\nunder domain generalization scenarios. Speciﬁcally, we\\npropose to model p(y1:T ,zv\\n1:T |zc) with a dynamic distri-\\nbution p(yt,zv\\nt |zc) where the zv\\nt is varied and encode the\\nshift information in the data category space ( e.g., the pro-\\nportion of each category). The module can be optimized\\nvia maximizing the distribution p(y1:T ,zv\\n1:T |zc) given a se-\\nquence of domains. In practise, we represent p(zv\\nt |zv\\n<t) as\\np(zv\\nt |zv\\n<t) =Cat(π(zv\\n<t)) which is a learnable categorical\\ndistribution. In a similar vein with zw\\nt , we utilize a distribu-\\ntion qto model the posterior distribution and approximate\\nthe prior distribution of zv\\nt by adopting variational inference\\ngiven as\\nq(zv\\n1:T |y1:T ) =\\nT∏\\nt=1\\nq(zv\\nt |zv\\n<t,yt), (6)\\nwhere q(zv\\nt |zv\\n<t,yt) can be parameterized by a recurrent\\nneural network with categorical distribution as output. We\\nset zv\\n0 = 0 for initial state when t = 0. The proposed\\nmodule can be jointly trained with the inference process in\\nEq. 6 as well as classiﬁcation loss by maximizing\\nLc =\\nT∑\\nt=1\\nEq(zv\\nt |zv\\n<t,yt)\\n[\\nlog p(yt|zc,zv\\nt )\\n]\\n−λ3\\nT∑\\nt=1\\nDKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t)).\\n(7)\\nHere, the ﬁrst term denotes the classiﬁcation loss (i.e., max-\\nimizing log likelihood) and the second term denotes KL\\ndivergence which aims to align the posterior distribution of\\nzv\\nt with its prior distribution.\\nTemporal smooth constraint for better stability. We\\nempirically ﬁnd that the estimation of conditional den-\\nsity (i.e., p(zw\\nt |zw\\n<t) and p(zv\\nt |zv\\n<t)) which is to model\\nthe complex dynamics over temporal transition may not\\nbe stable based on our formulation above. We conjecture\\nthe reason that some of the static information can be dis-\\ntorted by the dynamic inference module q(zw\\nt |zw\\n<t,xt) and\\nq(zv\\nt |zv\\n<t,yt) for better reconstruction quality, which fur-\\nther yields sub-optimal results for recognition tasks. Intu-\\nitively, one can tackle this limitation by reducing the dimen-\\nsion of latent codes zw\\nt and zv\\nt or decreasing the learning\\nrate of corresponding inference modules and prior modules\\nmanually to achieve better decoupling effect. However, we\\nﬁnd that such strategy may not lead to desired performance.\\nIn our work, we propose to employ Lipschitz constrain over\\nthe temporal domain to stabilize the learning of the dynamic\\ninference modules as follows\\n|q(zw\\nt |zw\\n<t,xt) −q(zw\\nt−1|zw\\n<t−1,xt−1)|≤ α,\\n|q(zv\\nt |zv\\n<t,yt) −q(zv\\nt−1|zv\\n<t−1,yt−1)|≤ α, (8)\\nwhere αis referred to as a Lipschitz constant. The above\\nregularization term is denoted as TS constraint for simplicity.\\nWe expect it can help with reducing the potential for volatile\\ntraining.\\nObjective function.Given training data S, our proposed\\nframework can be optimized through the objective function\\nLLSSAE = Ld + Lc with the temporal smooth constrains\\nin Eq. 8, where the ﬁrst term Ld and second term Lc aim\\nto tackle the problem of covariate shift and concept shift,\\nrespectively.\\nDiscussion. It is worth noting that there exists some works\\nusing probabilistic graph model for future video frame gen-\\neration task (Yingzhen & Mandt, 2018; Han et al., 2021),\\nwhich are similar to our proposed method at a high level.\\nNevertheless, our method is different since 1) our proposed\\nframework can be applied not only to generation task (as\\nwe show in the ablation study of experimental section) but\\nalso to evolving domain generalization task (which is the\\nmain focus in our paper); 2) in order to ﬁt our framework\\nto the non-stationary recognition task, we introduce a novel\\ncategory-related module to capture the concept shift, as\\nsuch, better generalization performance can be achieved.\\n3.3. Theoretical Analysis\\nIn this section, we aim to give a theoretical insight on our\\nproposed method by extending variational inference from\\nstationary environments to non-stationary environments.\\nProbabilistic model for stationary environments. We\\nﬁrst elaborate our proposed framework based on the sta-\\ntionary condition, where zc, zw and zv are introduced to\\ncapture the domain-invariant category information, domain-\\nspeciﬁc and category information, respectively. We thus\\nhave the following theorem.\\nTheorem 3.1. For the data log likelihood log p(x,y) in a\\nstationary environment, we have the evidence lower bound\\nmax\\np,q\\nEzc,zw,zv\\n[\\nlog p(x|zc,zw)p(y|zc,zv)\\n]\\n−DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw))\\n−DKL(q(zv|y),p(zv)),\\n(9)\\nwhere zc ∼q(zc|x),zw ∼q(zw|x),zv ∼q(zv|y).\\nProof. We consider the data generation procedure\\np(x,y,zc,zw,zv). We have\\nlog p(x,y) =DKL(q(zc,zw,zv|x,y),p(zc,zw,zv|x,y))\\n+ Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y) ,\\n(10)\\nwhere the ﬁrst term is the KL divergence of the approximate\\nfrom the true posterior. Since this term is non-negative,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nthe second term is called the evidence lower bound on the\\nmarginal likelihood of data (x,y). This can be written as\\nlog p(x,y) ≥Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y)\\n= Ezc,zw,zv\\n[\\nlog p(x|zc,zw)p(y|zc,zv)\\n]\\n−DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw))\\n−DKL(q(zv|y),p(zv)).\\n(11)\\nThis completes the proof.\\nTheorem 3.1 shows that for a speciﬁed domain, the latent\\nspace can be decoupled into a domain invariant subspace, a\\ndomain related subspace and a category related subspace.\\nProbabilistic model for non-stationary environ-\\nments. We now extend the aforementioned analysis to\\nthe non-stationary environment. Speciﬁcally, for the\\nnon-stationary environment, we can cast the dynamic\\nvariational inference framework via modeling the sequence\\nof latent variables zw and zv as two parallel Markov\\nchains (i.e., p(zw) = p(zw\\nt |zw\\n<t) and p(zv) = p(zv\\nt |zv\\n<t)).\\nWe thus have the following theorem.\\nTheorem 3.2. By denoting p(zw) = p(zw\\nt |zw\\n<t) and\\np(zv) = p(zv\\nt |zv\\n<t)), LLSSAE is equivalent to the ELBO\\nof the data log likelihood log p(x1:T ,y1:T ) based on the\\nnon-stationary environment setting.\\nProof. We can reformulate the lower bound in Eq. 9 for a\\nnon-stationary environment as\\nlog p(x1:T ,y1:T ) ≥Eq log p(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\nq(zc,zw\\n1:T ,zv\\n1:T |x1:T ,y1:T ) ,\\n(12)\\nwhich can be written as\\nlog p(x1:T ,y1:T )\\n≥\\nT∑\\nt=1\\nEzc,zw\\nt ,zv\\nt\\n[\\nlog p(xt|zc,zw\\nt )p(y|xc,zv\\nt )\\n]\\n−DKL(q(zc|x1:T ),p(zc))\\n−\\nT∑\\nt=1\\nDKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t))\\n−\\nT∑\\nt=1\\nDKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t)),\\n(13)\\nwhere zc ∼ q(zc|x1:T ),zw\\nt ∼ q(zw\\nt |zw\\n<t,xt) and zv\\nt ∼\\nq(zv\\nt |zv\\n<t,yt). The detailed derivation procedure is pro-\\nvided in App. A. We can see that the reconstruction part for\\nxt in the ﬁrst term together with the second and third terms\\ncan form our objective Ld for domain-related module (i.e.,\\ncovariate shift), and the combination of the reconstruction\\npart for yt in the ﬁrst term and the last term can form Lc for\\ncategory-related module (i.e., concept shift). As a result, the\\nformulation above is equivalent to our objective LLSSAE .\\nE\\nE\\n...\\n...\\n...\\n...\\nD\\nC\\nE ... ...\\nx\\ny/gid00466/gid00507/gid00021 \\n/gid00466/gid00507/gid00021 \\ny/gid00466/gid00507/gid00021 \\nx/gid00466/gid00507/gid00021 \\n/gid00031/gid00052/gid00041/gid00028/gid00040/gid00036/gid00030/gid00001 /gid00043/gid00535 z/gid00001/gid00001/gid00536\\n/gid00031/gid00052/gid00041/gid00028/gid00040/gid00036/gid00030/gid00001 /gid00043/gid00535 z/gid00001/gid00001/gid00001/gid00536\\n/gid00046/gid00047/gid00028/gid00047/gid00036/gid00030/gid00001 /gid00043/gid00535 z /gid00536\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00466\\n~\\n~\\nTS constraint \\nTS constraint \\n/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032 \\n/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032 \\n/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032 \\n/gid00030\\n/gid00049\\n/gid00050\\n/gid00050\\n/gid00049\\n/gid00030\\nz/gid00050\\n/gid00466/gid00507/gid00021 \\nz /gid00049\\n/gid00466/gid00507/gid00021 \\nz/gid00030\\n/gid00049\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00467\\n/gid00049\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00021\\n/gid00049\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00466\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00467\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00021\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 \\nF\\n/gid00030\\n/gid00050\\nF\\n/gid00049\\n/gid00043/gid00042/gid00046/gid00047/gid00032/gid00045/gid00036/gid00042/gid00045 \\n/gid00043/gid00042/gid00046/gid00047/gid00032/gid00045/gid00036/gid00042/gid00045 \\n/gid00043/gid00042/gid00046/gid00047/gid00032/gid00045/gid00036/gid00042/gid00045 \\nFigure 2.An overview of network architecture for LSSAE. Our\\nframework consists of the static variational encoding network Ec,\\ndynamic variational encoding networksEw and Ev, dynamic prior\\nnetworks Fw and Fv, a decoder D and a classiﬁer C. It is worth\\nnoting that we do not requireEv (i.e., only data from target domain\\nTavailable) during inference stage.\\n3.4. Implementation\\nThe implementation of network architecture for LSSAE is\\ndepicted in Fig. 2. It is composed of two parts: (1) the\\ndomain-related module (middle and bottom region); (2) the\\ncategory-related module (top region).\\nThe domain-related module consists of a static variational\\nencoding network Ec for q(zc|x1:T ), a dynamic variational\\nencoding network Ew associated with q(zw\\nt |zw\\n<t,xt), a dy-\\nnamic prior network Fw working for p(zw\\nt |zw\\n<t) and a de-\\ncoder D corresponding to p(xt|zc,zw\\nt ). Similar to V AE\\n(Kingma & Welling, 2014), we can apply the reparameteri-\\nzation trick (Kingma & Welling, 2014) to optimize param-\\neters of Ec, Ew and Fw. Speciﬁcally, we implement Ec\\nby a feature extractor which will be also utilized to extract\\nfeatures during test time. Ew can be implemented by a\\nfeature extractor with same architecture of Ec but not shar-\\ning network parameters, and followed by a LSTM network.\\nFw is implemented as a one-layer LSTM network. For the\\ncategory-related module, we design a dynamic inference\\nnetwork Ew which takes the one-hot code of label yt as\\nthe input and with the output of the categorical distribution\\nq(zv\\nt |zv\\n<t,yt), and a classiﬁer C which takes zc and zv\\nt as\\nthe input for p(yt|zc,zv\\nt ). Similarly, the prior network Fv\\nfor p(zv\\nt |zv\\n<t) is a LSTM network with a categorical distri-\\nbution as the output. After that, we use Gumbel-Softmax\\nreparameterization trick (Jang et al., 2017; Maddison et al.,\\n2017) to sample zv\\nt for optimization. Regarding the clas-\\nsiﬁer C, we utilize a linear layer by following Gulrajani\\n& Lopez-Paz (2021). The details of our architecture and\\nhyperparameters for objective function can be found in the\\nsupplementary material.\\nOptimization. We ﬁrst initialize zw and zv as zw\\n0 = 0 and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nzv\\n0 = 0, respectively. To train our framework, we generate\\nthe dynamic prior distributions p(zw\\nt |zw\\n<t) and p(zv\\nt |zv\\n<t)\\nthrough Fw and Fv respectively based onT source domains\\nS. For time stamp t, we sample a batch of data xt and take\\nit as input for Ec and Ew to obtain the parameters of their\\nposterior distributions. After that, the latent features zc\\nand zw\\nt are resampled separately through reparameterization\\ntrick and then concatenated together. Finally the decoder\\nD outputs the reconstruction data of xt. Meanwhile, Ev\\ntakes the corresponding labels yt of xt in one-hot format\\nas input and outputs the latent features, and then the latent\\nfeatures are also resampled through reparameterization trick\\nto obtain zv\\nt . The classiﬁer C takes zc and zv\\nt as the input\\nto predict the labels of xt. During training, we sample a\\nmini-batch from each single domain with the same number\\nof data sample to form a large batch in order to suit our\\nframework to the temporal smooth constraint for stable\\ntraining. This optimization procedure of LSSAE is depicted\\nin Algorithm 1.\\nInference. To predict the label of xt sampled from one\\nof the target domains in Dt in T, we adopt Fv to infer the\\nlatent code zv\\nt and Ec to extract the latent features zc, and\\nthen apply the classiﬁer Cfor the prediction purpose. It is\\nworth noting that we do not require Ev (i.e., only data from\\ntarget domain Tavailable) during inference stage. We show\\nthis procedure in Algorithm 2.\\n4. Experiments\\nIn this section, we present experimental results to validate\\nthe effectiveness of our proposed LSSAE based on the set-\\nting of evolving domain generalization.\\n4.1. Experimental Setup\\nWe compare the proposed LSSAE with other DG methods\\non two synthetic datesets (Circle and Sine) and four real-\\nworld datasets (Rotated MNIST, Portraits, Caltran, Power-\\nSupply). We also evaluate the results on two variants named\\nCircle-C and Sine-C derived from Circle and Sine via syn-\\nthesizing concept shift manually. We split the domains into\\nsource domains, intermediate domains and target domains\\nwith the ratio of {1/2 : 1/6 : 1/3}. The intermediate do-\\nmains are utilized as validation set. More details of dataset\\nconstruction can be found in the supplementary material.\\n(1) Circle/-C (Pesaranghader & Viktor, 2016). This dataset\\ncontains evolving 30 domains where the instance are sam-\\npled from 30 2D Gaussian distributions. For Circle-C, con-\\ncept shift is introduced via changing the center and radius\\nof decision boundary in a gradual manner over time.\\n(2) Sine/-C (Pesaranghader & Viktor, 2016). We rearrange\\nthis dataset by extending it to 24 evolving domains. To\\nsimulate concept drift for Sine-C, labels are reversed (i.e.,\\nAlgorithm 1Optimization procedure for LSSAE\\nInput: sequential source labeled datasetsS; static feature\\nextractor Ec; dynamic inference networks Ew, Ev and\\ntheir corresponding prior networks Fw, Fv; decoder D\\nand classiﬁer C.\\nRandomly initialize Ec,Ew,Ev,Fw,Fv,D,C\\nAssign zw\\n0 ,zv\\n0 ←0\\nfor t= 1,2,...,K do\\nGenerate prior distribution p(zw\\nt |zw\\n<t) via Fw\\nGenerate prior distribution p(zv\\nt |zv\\n<t) via Fv\\nfor i= 1,2,... do\\nSample a batch of data (xt,yt) from St\\n⊿Calculate Ld by Eq. 5 for Ec, Ew, Dand Fw\\n⊿Calculate Lc by Eq. 7 for Ec, Ev, Cand Fv\\n⊿Calculate temporal smooth constriction by Eq. 8\\nUpdate all modules by the summary of these loss\\nend for\\nend for\\nAlgorithm 2Inference procedure for LSSAE\\nInput: sequential target datasets T; static feature extrac-\\ntor Ec; dynamic prior network Fv and classiﬁer C.\\nAssign zv\\n0 ←0\\nfor t= 1,2,... do\\nSample zv\\nt ∼p(zv\\nt |zv\\n<t) via Fv\\nfor i= 1,2,... do\\n⊿Extract the feature for data xt via Ec\\n⊿Generate the prediction ˜ yt via C\\nend for\\nend for\\nfrom 0 to 1 or from 1 to 0) from the 6-th domain to the last\\none.\\n(3) Rotated MNIST (RMNIST)(Ghifary et al., 2015). Ro-\\ntated MNIST (RMNIST) is composed of MNIST digits\\nof various rotations. We extend it to 19 evolving do-\\nmains via applying the rotations with degree of R =\\n{0◦,15◦,30◦,..., 180◦}in order.\\n(4) Portraits (Ginosar et al., 2015). This dataset comprises\\nphotos of high-school seniors from the 1905s to the 2005s\\nfor gender classiﬁcation. We split the dataset into 34 do-\\nmains by a ﬁxed internal over time.\\n(5) Caltran (Hoffman et al., 2014). Caltran is a real-world\\nsurveillance dataset comprising images captured from a\\nﬁxed trafﬁc camera deployed in an intersection. The task is\\nto predict the type of scene based on continuously evolving\\ndata. We divide it into 34 domains based on different times.\\n(6) PowerSupply (Dau et al., 2019). PowerSupply is con-\\nstructed for the time-section prediction of current power\\nsupply based on the hourly records of an Italy electricity\\ncompany. The concept shift may raise from the change in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 1.The comparison of accuracy (%) between LSSAE and other DG baselines on various datasets. As DG baselines do not take\\nconcept shift into consideration, we report the results on datasets without concept shift and with concept shift separately.\\nAlgorithm Circle Sine RMNIST Portraits Caltran Avg Circle-C Sine-C PowerSupply Avg\\nERM 49.9 63.0 43.6 87.8 66.3 62.1 34.0 61.5 71.0 55.5\\nMixup 48.4 62.9 44.9 87.8 66.0 62.0 33.9 60.9 70.8 55.2\\nMMD 50.7 55.8 44.8 87.3 57.1 59.1 33.7 52.7 70.9 52.4\\nMLDG 50.8 63.2 43.1 88.5 66.2 62.3 34.6 62.0 70.8 55.8\\nIRM 51.3 63.2 39.0 85.4 64.1 60.6 38.5 61.2 70.8 56.8\\nRSC 48.0 61.5 41.7 87.3 67.0 61.1 33.7 61.5 70.9 55.4\\nMTL 51.2 62.9 41.7 89.0 68.2 62.6 33.9 61.4 70.7 55.3\\nFish 48.8 62.3 44.2 88.8 68.6 62.5 34.3 62.7 70.8 55.9\\nCORAL 53.9 51.6 44.5 87.4 65.7 60.6 34.1 59.0 71.0 54.7\\nAndMask 47.9 69.3 42.8 70.3 56.9 57.4 37.7 52.7 70.7 53.7\\nDIV A 67.9 52.9 42.7 88.2 69.2 64.2 33.9 52.9 70.8 55.1\\nLSSAE (Ours) 73.8 71.4 46.4 89.1 70.6 70.3 44.8 60.8 71.1 58.9\\nseason, weather or price. We form 30 domains according to\\ndays.\\nThe methods for comparison include: (1) ERM (Vapnik,\\n1998); (2) Mixup (Yan et al., 2020); (3) MMD (Li et al.,\\n2018b); (4) MLDG (Li et al., 2018a); (5) IRM (Rosen-\\nfeld et al., 2021); (6) RSC (Huang et al., 2020);\\n(7) MTL (Blanchard et al., 2021); (8) Fish (Shi et al., 2021);\\n(9) CORAL (Sun & Saenko, 2016); (10) AndMask (Paras-\\ncandolo et al., 2021); (11) DIV A (Ilse et al., 2020). All of our\\nexperiments are implemented in the PyTorch platform based\\non DomainBed package (Gulrajani & Lopez-Paz, 2021). For\\na fair comparison, we keep the neural network architecture\\nof encoding part and classiﬁcation part to be same for all\\nbaselines for different benchmarks.\\n4.2. Quantitative Results\\nThe results of of our proposed LSSAE and baselines are\\npresented in Table 1. As conventional DG methods focus\\nupon covariate shift only, we separate the datasets according\\nto with or without concept shift into two parts for fairness.\\nWe can see that LSSAE consistently outperforms other base-\\nlines over all datasets, it achieves 70.3% accuracy when\\nthere exists covariate shift only (Circle, Sine, RMNIST, Por-\\ntraits and Caltran), and achieves58.9% accuracy when there\\nexist concept shift (Circle-C, Sine-C, PowerSupply). The\\nresults are signiﬁcantly better than the compared DG ap-\\nproaches, which are reasonable since existing DG methods\\ncannot deal with distribution shift well in non-stationary\\nenvironments but our proposed LSSAE can properly cap-\\nture the evolving patterns to gain better performance. More\\nresults can be found in the supplementary materials.\\nTo better understand the rationality of our method, we vi-\\nsualize the decision boundaries of our method and ERM\\nbaseline on two synthetic datasets: Circle and Sine by com-\\nparing our proposed method with ERM baseline. The vi-\\nsualization results are depicted in Fig. 3 and Fig. 4. As\\n(a) Data\\nEvolving P(X)\\n(b) Ground Truth (c) ERM (d) LSSAE\\nStatic P(Y|X)\\nEvolving P(Y|X)\\nCircle Circle-C \\nFigure 3.Decision boundary visualization of Circle and Circle-C\\ndatasets each with 30 domains. (a) presents the original data in\\ndifferent domains by color, where the right half part are source\\ndomains. (b) shows the positive and negative labels in red and\\nblue dots. (c) and (d) are decision boundaries learned by ERM and\\nLSSAE, respectively.\\nEvolving P(X)\\n(a) Data (b) Ground Truth (c) ERM (d) LSSAE\\nStatic P(Y|X)\\nEvolving P(Y|X)\\nSine Sine-C \\nFigure 4.Decision boundary visualization of Sine and Sine-C\\ndatasets. (a) presents 24 domains indexed by different colors,\\nwhere the left half part are source domains. (b) shows the positive\\nand negative labels in red and blue dots. (c) and (d) are decision\\nboundaries learned by ERM and LSSAE, respectively.\\nshown in Fig. 3, both of ERM and LSSAE can ﬁt the source\\ndomains well. However, different from ERM which only ﬁt\\nthe source domains, LSSAE shows a desired generalization\\nability to unseen target domains. This validates that our\\nLSSAE can capture the underlying evolving patterns across\\ndomains to achieve better results. As for Circle-C where we\\nintroduce evolving concept shift via modifying the center\\nand radius of decision boundary over a period of time, we\\nobserve that our proposed LSSAE can still produce a more\\naccurate decision boundary compared with ERM. Similar\\nobservation can be found in Fig. 4, where LSSAE can be\\nbetter generalized to evolving domains compared with ERM.\\nHowever, we ﬁnd that our proposed LSSAE may not be able'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\n(a) random data sequences \\n(b) reconstructions\\n(c) generated sequences with fixed\\n(d) generated sequences with fixed\\nzc\\nzwt\\n/gid00083\\nFigure 5.Visualisation of generated and reconstructed data se-\\nquences on RMNIST dataset.\\nto obtain a desired boundary based on the setting of Sine-C,\\nwhere we introduce concept shift by reversing the labels.\\nWe conjecture the reason that our proposed LSSAE may\\nnot be able to well handle the abrupt concept shift where no\\ncontinuous evolving pattern exists.\\n4.3. Ablation Study\\nAnalysis on domain-related module.To better evaluate\\nwhether our proposed LSSAE can capture domain spe-\\nciﬁc information, we conduct experiments by evaluating\\nthe reconstruction and generation capability of LSSAE on\\nRMNIST dataset. The reconstruction and generation re-\\nsults are shown in Fig. 5. Each subﬁgure shows two se-\\nquences which follow the domain order (i.e., rotation degree\\ngradually evolves) from the left to the right. Subﬁgure (a)\\nshows the original data sequence sampled from source do-\\nmains (green bounding box), intermediate domains (orange\\nbounding box) and target domains (pink bounding box) in\\norder. Noted that we are only interested in degree of rota-\\ntion thus the category of images from different domains may\\nnot be consistent. Subﬁgure (b) shows the reconstructions\\nfor each sample in Subﬁgure (a). Based on the results, we\\nﬁnd that we can achieve a desired reconstruction quality,\\nwhich suggests that the domain-related information can be\\ncaptured even we random the category related information.\\nTo further show that our proposed method is capable for\\ngeneration task, in subﬁgure (c), we visualize the randomly\\ngenerated samples using zw\\nt ∼p(zw\\nt |zw\\n<t) while keeping zc\\nthe same for all domains. We can see that the category infor-\\nmation (i.e., digit information) remains unchanged when we\\nﬁx zc, and can generated samples for unseen future domains\\nwhere the rotation degree of digit is gradually changed with\\nevolving prior p(zw\\nt |zw\\n<t). This observation suggests that\\nour proposed method can be used for data augmentation\\nwhen adapting to the unseen target domains. However, we\\nalso observe that there exists image quality distortion when\\ngenerating new domains. We conjecture the reason that the\\nTable 2.Comparison of different prior distributions for category-\\nrelated module on PowerSupply dataset.\\nPrior Type Accuracy (%)\\nWithout zv 70.7\\nGaussian 70.1\\nUniform 71.0\\nCategorical (Ours) 71.1\\nrange of degrees for training may not be sufﬁciently diverse,\\nwhich limits the performance of generation to unseen digit\\nrotation degrees. Subﬁgure (d) shows randomly generated\\ndata via sampling zc ∼p(zc) while keeping zw\\nt the same for\\nall domains at different time stamp t(i.e., zw\\nt = zw\\n1 ), where\\nwe ﬁnd that the generated digit images belong to different\\ncategories but with the same rotation degree, which further\\nindicates that our proposed method can successfully extract\\ndomain related information.\\nAnalysis on category-related module.We then evaluate\\nthe effectiveness of our proposed category-related module\\non PowerSupply dataset. To this end, we consider three\\ndifferent ablation studies by 1) removing category-related\\nmodule, 2) replacing the learnable categorical prior with a\\nlearnable Gaussian prior, 3) replacing the learnable categor-\\nical prior with a ﬁx Uniform prior. The results are shown\\nin Table 2. As we can see, our proposed LSSAE based on\\ncategorical prior can achieve the best performance among\\nother baselines, which shows the effectiveness of our pro-\\nposed method. However, we observe that the performance\\ndrops to some extent by replacing the categorical prior with\\nGaussian prior, which we conjecture the reason that cate-\\ngorical distribution can better capture the category related\\ninformation which is discrete. We also observe that better\\nperformance can be achieved by comparing with the results\\nusing Uniform distribution as prior, which is reasonable\\nsince Uniform distribution does not change over time.\\nAnalysis on temporal smooth constraint. The smooth\\nconstraint is mainly designed for stabilizing the training\\nprocedure. To verify this, we conduct experiments by con-\\nsidering “with & without this constraint” on RMNIST and\\nCalTran datasets while keeping other hyperparameters the\\nsame. The results are reported as the variance of the test\\naccuracy in the last ﬁve epochs (See Table 3). For RMNIST,\\nwhen training with constraint, the variance is 0.4; without\\nconstraint, the variance is 3.6. For CalTran, when training\\nwith constraint, the variance is 4.7, and without constraint,\\nthe variance is 10.0. We can see that there exists an obvious\\ndecrease in terms of variance when incorporating with our\\nproposed TS constraint. Besides, as our smooth constraint\\nhelps stabilize the training process, better performance can\\nbe expected. For RMNIST, training with our smooth con-\\nstraint can achieve 0.9% performance gain; For CalTran,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 3.Ablation study on temporal smooth constraint.\\nTS Constraint RMNIST CalTran\\nVar ↓ Acc (%)↑ Var ↓ Acc (%)↑\\n× 3.6 45.5 10.0 69.9\\n✓ 0.4 46.4 4.7 70.6\\nthe performance gain is 0.7%.\\n5. Conclusion\\nIn this paper, we propose to focus on the problem of evolv-\\ning domain generalization, where the covariate shift and\\nconcept shift vary over time. To tackle this problem, we\\npropose a novel framework LSSAE to model the dynamics\\nof distribution shift (i.e., covariate shift and concept shift).\\nWe also provide theoretical analysis, which shows that our\\nproposed method is equivalent to maximizing the ELBO\\nbased on the non-stationary environment setting, and justi-\\nﬁes the rationality of our proposed method for the problem\\nof evolving domain generalization. Experimental results\\non both toy data and real-world datasets across multiple\\ndomains further indicate the signiﬁcance of our proposed\\nmethod based on this setting.\\nAcknowledgements\\nThis work was supported in part by CityU New Research\\nInitiatives/Infrastructure Support from Central (APRC\\n9610528), CityU Applied Research Grant (ARG 9667244)\\nand Hong Kong Innovation and Technology Commission\\n(InnoHK Project CIMDA). Besides, this work was also sup-\\nported in part by the National Natural Science Foundation of\\nChina under 62022002, in part by the Hong Kong Research\\nGrants Council General Research Fund (GRF) under Grant\\n11203220.\\nReferences\\nAlbuquerque, I., Monteiro, J., Darvishi, M., Falk, T. H.,\\nand Mitliagkas, I. Generalizing to unseen domains via\\ndistribution matching. arXiv preprint arXiv:1911.00804,\\n2021.\\nBalaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg:\\nTowards domain generalization using meta-regularization.\\nAdvances in Neural Information Processing Systems, 31:\\n998–1008, 2018.\\nBlanchard, G., Lee, G., and Scott, C. Generalizing from sev-\\neral related classiﬁcation tasks to a new unlabeled sample.\\nAdvances in Neural Information Processing Systems, 24:\\n2178–2186, 2011.\\nBlanchard, G., Deshmukh, A. A., Dogan, ¨U., Lee, G., and\\nScott, C. Domain generalization by marginal transfer\\nlearning. J. Mach. Learn. Res., 22:2–1, 2021.\\nChen, H.-Y . and Chao, W.-L. Gradual domain adaptation\\nwithout indexed intermediate domains. In Thirty-Fifth\\nConference on Neural Information Processing Systems,\\n2021.\\nDau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu,\\nY ., Gharghabi, S., Ratanamahatana, C. A., and Keogh,\\nE. The ucr time series archive. IEEE/CAA Journal of\\nAutomatica Sinica, 6(6):1293–1305, 2019.\\nDou, Q., Coelho de Castro, D., Kamnitsas, K., and Glocker,\\nB. Domain generalization via model-agnostic learning\\nof semantic features. Advances in Neural Information\\nProcessing Systems, 32:6450–6461, 2019.\\nFederici, M., Tomioka, R., and Forr ´e, P. An information-\\ntheoretic approach to distribution shifts. In Thirty-Fifth\\nConference on Neural Information Processing Systems,\\n2021.\\nGhifary, M., Kleijn, W. B., Zhang, M., and Balduzzi, D. Do-\\nmain generalization for object recognition with multi-task\\nautoencoders. In Proceedings of the IEEE International\\nConference on Computer Vision, pp. 2551–2559, 2015.\\nGinosar, S., Rakelly, K., Sachs, S., Yin, B., and Efros, A. A.\\nA century of portraits: A visual historical record of amer-\\nican high school yearbooks. In 2015 IEEE International\\nConference on Computer Vision Workshop (ICCVW), pp.\\n652–658, 2015.\\nGong, R., Li, W., Chen, Y ., and Gool, L. V . Dlow: Domain\\nﬂow for adaptation and generalization. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pp. 2477–2486, 2019.\\nGulrajani, I. and Lopez-Paz, D. In search of lost domain\\ngeneralization. In International Conference on Learning\\nRepresentations, 2021.\\nHan, J., Min, M. R., Han, L., Li, L. E., and Zhang, X.\\nDisentangled recurrent wasserstein autoencoder. In Inter-\\nnational Conference on Learning Representations, 2021.\\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\\nNeural Computation, 9(8):1735–1780, 1997.\\nHoffman, J., Darrell, T., and Saenko, K. Continuous man-\\nifold based adaptation for evolving visual domains. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 867–874, 2014.\\nHuang, Z., Wang, H., Xing, E. P., and Huang, D. Self-\\nchallenging improves cross-domain generalization. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nComputer Vision–ECCV 2020: 16th European Confer-\\nence, Glasgow, UK, August 23–28, 2020, Proceedings,\\nPart II 16, pp. 124–140, 2020.\\nIlse, M., Tomczak, J. M., Louizos, C., and Welling, M. Diva:\\nDomain invariant variational autoencoders. In Medical\\nImaging with Deep Learning, pp. 322–348. PMLR, 2020.\\nJang, E., Gu, S., and Poole, B. Categorical reparameteriza-\\ntion with gumbel-softmax. International Conference on\\nLearning Representations, 2017.\\nKingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization. In International Conference on Learning\\nRepresentations, 2015.\\nKingma, D. P. and Welling, M. Auto-encoding variational\\nbayes. International Conference on Learning Represen-\\ntations, 2014.\\nKumar, A., Ma, T., and Liang, P. Understanding self-training\\nfor gradual domain adaptation. In International Confer-\\nence on Machine Learning, pp. 5468–5479. PMLR, 2020.\\nLao, Q., Jiang, X., Havaei, M., and Bengio, Y . Continu-\\nous domain adaptation with variational domain-agnostic\\nfeature replay. arXiv preprint arXiv:2003.04382, 2020.\\nLi, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Learn-\\ning to generalize: Meta-learning for domain generaliza-\\ntion. In Thirty-Second AAAI Conference on Artiﬁcial\\nIntelligence, 2018a.\\nLi, H., Pan, S. J., Wang, S., and Kot, A. C. Domain general-\\nization with adversarial feature learning. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp. 5400–5409, 2018b.\\nLi, W., Xu, Z., Xu, D., Dai, D., and Van Gool, L. Domain\\ngeneralization and adaptation using low rank exemplar\\nsvms. IEEE Transactions on Pattern Analysis and Ma-\\nchine Intelligence, 40(5):1114–1127, 2017.\\nLiu, H., Long, M., Wang, J., and Wang, Y . Learning to adapt\\nto evolving domains. In Advances in Neural Information\\nProcessing Systems, volume 33, pp. 22338–22348, 2020.\\nMaddison, C. J., Mnih, A., and Teh, Y . W. The concrete\\ndistribution: A continuous relaxation of discrete random\\nvariables. International Conference on Learning Repre-\\nsentations, 2017.\\nMancini, M., Bulo, S., Caputo, B., and Ricci, E. Adagraph:\\nUnifying predictive and continuous domain adaptation\\nthrough graphs. In 2019 IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pp. 6561–\\n6570, jun 2019.\\nMuandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain\\ngeneralization via invariant feature representation. In\\nInternational Conference on Machine Learning, pp. 10–\\n18, 2013.\\nNguyen, A. T., Tran, T., Gal, Y ., and Baydin, A. G. Do-\\nmain invariant representation learning with domain den-\\nsity transformations. arXiv preprint arXiv:2102.05082,\\n2021.\\nParascandolo, G., Neitz, A., ORVIETO, A., Gresele, L., and\\nSch¨olkopf, B. Learning explanations that are hard to vary.\\nIn International Conference on Learning Representations,\\n2021.\\nPark, S. W., Shu, D. W., and Kwon, J. Generative adversarial\\nnetworks for markovian temporal dynamics: Stochastic\\ncontinuous data generation. In Proceedings of the 38th\\nInternational Conference on Machine Learning, volume\\n139, pp. 8413–8421, 18–24 Jul 2021.\\nPesaranghader, A. and Viktor, H. L. Fast hoeffding\\ndrift detection method for evolving data streams. In\\nECML/PKDD, 2016.\\nRosenfeld, E., Ravikumar, P. K., and Risteski, A. The\\nrisks of invariant risk minimization. In International\\nConference on Learning Representations, 2021.\\nShankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S.,\\nJyothi, P., and Sarawagi, S. Generalizing across domains\\nvia cross-gradient training. In International Conference\\non Learning Representations, 2018.\\nShi, Y ., Seely, J., Torr, P. H., Siddharth, N., Hannun, A.,\\nUsunier, N., and Synnaeve, G. Gradient matching for\\ndomain generalization. arXiv preprint arXiv:2104.09937,\\n2021.\\nSugiyama, M., Yamada, M., and du Plessis, M. C. Learning\\nunder nonstationarity: covariate shift and class-balance\\nchange. Wiley Interdisciplinary Reviews: Computational\\nStatistics, 5(6):465–477, 2013.\\nSun, B. and Saenko, K. Deep coral: Correlation alignment\\nfor deep domain adaptation. In Hua, G. and J ´egou, H.\\n(eds.), Computer Vision – ECCV 2016 Workshops , pp.\\n443–450, Cham, 2016.\\nTahmasbi, A., Jothimurugesan, E., Tirthapura, S., and Gib-\\nbons, P. B. Driftsurf: Stable-state/reactive-state learning\\nunder concept drift. In International Conference on Ma-\\nchine Learning, pp. 10054–10064. PMLR, 2021.\\nTorralba, A. and Efros, A. A. Unbiased look at dataset bias.\\nIn CVPR 2011, pp. 1521–1528. IEEE, 2011.\\nVapnik, V . Statistical learning theory wiley.New York, 1998.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nVergara, A., Vembu, S., Ayhan, T., Ryan, M. A., Homer,\\nM. L., and Huerta, R. Chemical gas sensor drift compen-\\nsation using classiﬁer ensembles. Sensors and Actuators\\nB: Chemical, 166:320–329, 2012.\\nV olpi, R., Namkoong, H., Sener, O., Duchi, J. C., Murino,\\nV ., and Savarese, S. Generalizing to unseen domains via\\nadversarial data augmentation. In Advances in Neural\\nInformation Processing Systems, volume 31, 2018.\\nWang, H., He, H., and Katabi, D. Continuously indexed\\ndomain adaptation. In III, H. D. and Singh, A. (eds.),\\nProceedings of the 37th International Conference on Ma-\\nchine Learning, volume 119, pp. 9898–9907, 13–18 Jul\\n2020.\\nWang, Y ., Li, H., Chau, L.-P., and Kot, A. C. Variational dis-\\nentanglement for domain generalization. arXiv preprint\\narXiv:2109.05826, 2021.\\nWulfmeier, M., Bewley, A., and Posner, I. Incremental\\nadversarial domain adaptation for continually changing\\nenvironments. In 2018 IEEE International Conference on\\nRobotics and Automation (ICRA), pp. 4489–4495. IEEE,\\n2018.\\nYan, S., Song, H., Li, N., Zou, L., and Ren, L. Improve un-\\nsupervised domain adaptation with mixup training. arXiv\\npreprint arXiv:2001.00677, 2020.\\nYingzhen, L. and Mandt, S. Disentangled sequential autoen-\\ncoder. In International Conference on Machine Learning,\\npp. 5670–5679. PMLR, 2018.\\nZhou, K., Yang, Y ., Qiao, Y ., and Xiang, T. Domain general-\\nization with mixstyle. arXiv preprint arXiv:2104.02008,\\n2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nA. Proofs\\nA.1. Derivation of ELBO for Stationary Environments\\nAs we introduce two latent variables to account for the two types of distribution shift, the data generating procedure for one\\ndomain can be expressed as:\\np(x,y,zc,zw,zv) =p(zc)p(zw)p(zv)p(x|zc,zw)p(y|zc,zv)\\n= p(zw)p(zv)\\nN∏\\ni=1\\np(zc\\ni )p(xi|zc\\ni ,zw)p(yi|zc\\ni ,zv)\\n(14)\\nWe let (zc,zw) be the latent variables for x, and zv be latent variable for y, thus the distribution of these three latent\\nvariables can be inferred from the observable datapoints as p(zc|x),p(zw|x), p(zv|y), respectively. The joint distribution of\\nlatent variables is:\\np(zc,zw,zv|x,y) =p(zc|x)p(zw|x)p(zv|y) (15)\\nFor our approximating distribution in Eq 15, we can choose the form q(zc,zw,zv|x,y) =q(zc|x)q(zw|x)q(zv|y). Thus,\\nwe can get:\\nDKL(q,p) =Eq log q(zc,zw,zv|x,y)\\np(zc,zw,zv|x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv)p(x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv) + Eq log p(x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv) + logp(x,y)\\n(16)\\nWe can get:\\nlog p(x,y) =DKL(q,p) +Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y) (17)\\nAs DKL(q,p) ≥0, the variational lower bound for log p(x,y) is:\\nL= Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y) . (18)\\nThis formulation can be reorganized as:\\nL= Eq log p(x,y|zc,zw,zv)p(zc)p(zw)p(zv)\\nq(zc|x)q(zw|x)q(zv|y)\\n= Eq log p(x,y|zc,zw,zv) +Eq log p(zc)\\nq(zc|x) + Eq log p(zw)\\nq(zw|x) + Eq log p(zv)\\nq(zv|y)\\n= Eq log p(x,y|zc,zw,zv) −DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw)) −DKL(q(zv|y),p(zv)).\\n(19)\\nAs p(x,y|zc,zw,zv) =p(x|zc,zw)p(y|zc,zv), the above formulation can be rewrote as:\\nL≥ Eq log p(x|zc,zw)p(y|zc,zv) −DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw)) −DKL(q(zv|y),p(zv)). (20)\\nA.2. Derivation of ELBO for Non-stationary Environments\\nWe assume the prior distribution of latent variableszw and zv satisfy Markov property. This means each of them relies on\\nthe value of their previous states:\\np(zw) =p(zw\\nt |zw\\n<t),p(zv) =p(zv\\nt |zv\\n<t), (21)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nThe joint distribution of data and latent variables is:\\np(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\n=\\nT∏\\nt=1\\np(xt,yt|zc,zw\\nt ,zv\\nt )p(zc)p(zw\\nt |zw\\n<t)p(zv\\nt |zv\\n<t)\\n=\\nT∏\\nt=1\\np(zw\\nt |zw\\n<t)p(zv\\nt |zv\\n<t)\\nNt∏\\ni=1\\np(zc\\ni )p(xi|zc\\ni ,zw\\nt )p(yi|zc\\ni ,zv\\nt )\\n(22)\\nwhere p(zw\\n1 ) =p(zw\\n1 |zw\\n0 ) and p(zv\\n1) =p(zv\\n1|zv\\n0).\\nFor a non-stationary environment, we assume that (zc,zw\\nt ) are the latent variables for xt and zv\\nt is the latent variable for yt\\nat t-th time stamp. Thus the distribution of the latent variables for t-th time stamp can be express as p(zc|xt), p(zw\\nt |xt) and\\np(zv\\nt |yt), respectively. We employq(zc|xt), q(zw\\nt |zw\\n<t,xt) and q(zv\\nt |zv\\n<t,yt) to approximate the prior distributions here.\\nSimilar to Eq 18, we can draw the variational lower bound for log p(x1:T ,y1:T ) as:\\nL= Eq log p(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\nq(zc,zw\\n1:T ,zv\\n1:T |x1:T ,y1:T ) . (23)\\nWhen incorporating with Eq 22, this can be reorganized as:\\nL= Eq log\\n∏T\\nt=1 p(xt, yt|zc, zw\\nt , zv\\nt )p(zc)p(zw\\nt |zw\\n<t)p(zv\\nt |zv\\n<t)∏T\\nt=1 q(zc|xt)q(zw\\nt |zw\\n<t, xt)q(zv\\nt |zv\\n<t, yt)\\n= Eq\\n[\\nlog\\n∏T\\nt=1 p(zw\\nt |zw\\n<t)∏T\\nt=1 q(zw\\nt |zw\\n<t, xt)\\n+ log\\n∏T\\nt=1 p(zv\\nt |zv\\n<t)∏T\\nt=1 q(zv\\nt |zv\\n<t, yt)\\n+ log\\n∏T\\nt=1 p(zc)∏T\\nt=1 q(zc|xt)\\n+ log\\nT∏\\nt=1\\np(xt|zc\\nt, zw\\nt )p(yt|zc\\nt, zv\\nt )\\n]\\n= Eq\\n[\\n−\\nT∑\\nt=1\\nlog q(zw\\nt |zw\\n<t, xt)\\np(zw\\nt |zw\\n<t) −\\nT∑\\nt=1\\nlog q(zv\\nt |zv\\n<t, yt)\\np(zv\\nt |zv\\n<t) −\\nT∑\\nt=1\\nlog q(zc\\nt|xt)\\np(zc\\nt)\\n+\\nT∑\\nt=1\\nlog p(xt|zc, zw\\nt )p(yt|zc, zv\\nt )\\n]\\n(24)\\nBy Jensen’s inequality, the above formulation can be rewrote as:\\nL≥ Eq\\n[ T∑\\nt=1\\nlog p(xt|zc,zw\\nt )p(yt|zc,zv\\nt ) −DKL(q(zc|xt),p(zc)) −DKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t))\\n−DKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t))\\n] (25)\\nThis complements the proof.\\nB. Probabilistic Generative Model of LSSAE\\nIn Fig. 6, we present the complete probabilistic generative graph of our LSSAE. The left part is the DAG of LSSAE presented\\nin Fig. 1(d) where we did not include zc in this ﬁgure as the main focus of Fig. 1 is the dynamic factors (i.e., W and V) for\\npresenting our main idea of evolving dynamics at a high level instead of DG. In the right part, we illustrate the relationship\\nbetween our DAG and probabilistic generative model (with zc). Here, the dependence of Xt on Yt (the dotted blue line) is\\nsubstituted with zc (the solid red line) which mainly captures the static category information in data sample space.\\nC. Additional Details on the Experimental Setup\\nC.1. Datasets\\nOur experiments are conducted on 2 synthetic and 4 real-world datasets presented in Table 4. More detailed description are\\ngiven below.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nVt-1 Vt ...\\n(d)\\nXt-1 Yt-1 \\nWt-1 \\nX t Yt\\nWt ......\\n... zv\\nt-1 zv\\nt ...\\nProbabilisitic Generative Model of (d)\\nXt-1 Yt-1 \\nzw\\nt-1 \\nX t Yt\\nzw\\nt ......\\n...\\nzc zc\\nFigure 6.The probabilistic generative graph of LSSAE with zc presented. Here, the dependence of Xt on Yt in Fig. 1(d) is substituted\\nwith zc which mainly captures the static category information in data sample space.\\nTable 4.Brief description of employed benchmarks in this work.\\nDataset Type Number Source Domains Intermediate Domains Target Domains Total Domains\\nCircle/-C Digital 3,000 15 5 10 30\\nSine/-C Digital 2,280 12 4 8 24\\nRMNIST Image 70,000 10 3 6 19\\nPortraits Image 37,921 19 5 10 34\\nCalTran Image 5,450 19 5 10 34\\nPowerSupply Digital 29,928 15 5 10 30\\n• Circle (Pesaranghader & Viktor, 2016): Each data in this dataset owns two attributes (x,y),x,y ∈[0,1]. The label is\\nassigned using a circle curve as the decision boundary following (x−x0)2 + (y−y0)2 ≤r2, where (x0,y0) are the\\nlocation of the center and ris the radius of this circle. To generate Circle-C, we inject gradual shift via modifying the\\nvalue of x0 continuously throughout the domains.\\n• Sine (Pesaranghader & Viktor, 2016): Each data in this dataset owns two attributes(x,y),x,y ∈[0,1]. The label is\\nassigned using a sine curve as the decision boundary following y≤sin(x). To simulate abrupt concept drift for Sine-C,\\nlabels are reversed (i.e., from 0 to 1 or from 1 to 0) from the 6-th domain to the last one.\\n• RMNIST (Ghifary et al., 2015): This dataset is composed of MNIST digits of various rotations. We generate 19\\ndomains via applying the rotations with degree of R= {0◦,15◦,30◦,..., 180◦}on each domain. Note that each image\\nis seen at exactly one angle, so the training procedure cannot track a single image across different angles.\\n• Portraits (Ginosar et al., 2015): This is a real-world dataset of photos collected in American high school seniors. The\\nportraits are taken over 108 years (1905-2013) across 26 states. The goal is to classify the gender for each photo. We\\nsplit the dataset into 34 domains by a ﬁxed internal along time.\\n• CalTran (Hoffman et al., 2014): This dataset contains real-world images captured by a ﬁxed trafﬁc camera deployed in\\nan intersection over time. Frames were updated at 3 minute intervals each with a resolution 320 ×320. We divide\\nit into 34 domains by time. This is a scene classiﬁcation task to determine whether one or more cars are present in,\\nor approaching the intersection. The challenge mainly raise from the continually evolving domain shift as changes\\ninclude time, illumination, weather, etc.\\n• PowerSupply (Dau et al., 2019): This dataset is comprised of records of hourly power supply collected by an Italy\\nelectricity company. We form 30 domains according to days. Each data is assigned by a binary class label which\\nrepresents which time of day the current power supply belongs to (i.e., am or pm). The concept shift may results from\\nthe change in season, weather, price or the differences between working days and weekend.\\nC.2. Model Architecture & Hyperparameters\\nNeural network architectures used for each dataset:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 5.Model architectures for different datasets.\\nDataset Encoder Decoder\\nCircle/-C\\nNon-linear Encoder Non-linear Decoder Sine/-C\\nPowerSupply\\nRMNIST MNIST ConvNet MNIST ConvTranNet\\nPortraits ResNet-18 ConvTranNetCalTran\\nNeural network architecture for digital experiments (Circle/-C, Sine/-C, PowerSupply):\\nTable 6.Implementation of Non-linear Encoder.\\n# Layer\\n1 Linear(in= d, output=512)\\n2 ReLU\\n3 Linear(in=512, output=512)\\n4 ReLU\\n5 Linear(in=512, output=512)\\n6 ReLU\\n7 Linear(in=512, output=512)\\nTable 7.Implementation of Non-linear Decoder.\\n# Layer\\n1 Linear(in= d, output=16)\\n2 BatchNorm\\n3 LeakyReLU(0.2)\\n4 Linear(in=16, output=64)\\n5 BatchNorm\\n6 LeakyReLU(0.2)\\n7 Linear(in=64, output=128)\\n8. BatchNorm\\n9 LeakyReLU(0.2)\\n10 Linear(in=128, output= d)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nNeural network architecture for RMNIST experiments:\\nTable 8.Implementation of MNIST ConvNet.\\n# Layer\\n1 Conv2D(in= d, output=64)\\n2 ReLU\\n3 GroupNorm(groupds=8)\\n4 Conv2D(in=64, output=128, stride=2)\\n5 ReLU\\n6 GroupNorm(groupds=8)\\n7 Conv2D(in=128, output=128)\\n8 ReLU\\n9 GroupNorm(groupds=8)\\n10 Conv2D(in=128, output=128)\\n11 ReLU\\n12 GroupNorm(groupds=8)\\n13 Global average-pooling\\nTable 9.Implementation of MNIST ConvTranNet.\\n# Layer\\n1 Linear(in= d, output=1024)\\n2 BatchNorm\\n3 ReLU\\n4 Upsample(8)\\n5 ConvTransposed2D(in=64, output=128, kernel=5)\\n6 BatchNorm\\n7 ReLU\\n8 Upsample(24)\\n9 ConvTransposed2D(in=128, output=256, kernel=5)\\n10 BatchNorm\\n11 ReLU\\n12 Conv2D(in=256, output=1, kernel=1)\\n13 Sigmoid'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nNeural network architecture for Portraits and CalTran experiments:\\nTable 10.Implementation of ConvTranNet.\\n# Layer\\n1 Linear(in= d, output=1024)\\n2 BatchNorm\\n3 ReLU\\n4 Upsample(16)\\n5 ConvTransposed2D(in=64, output=128, kernel=5)\\n6 BatchNorm\\n7 ReLU\\n8 Upsample(40)\\n9 ConvTransposed2D(in=128, output=256, kernel=5)\\n10 BatchNorm\\n11 ReLU\\n12 Upsample(80)\\n13 ConvTransposed2D(in=256, output=3, kernel=5)\\n14 BatchNorm\\n15 ReLU\\n16 Sigmoid\\nThe model architecture of the encoder is ResNet-18, we replace the ﬁnal softmax layer of the ofﬁcial version following Gul-\\nrajani & Lopez-Paz (2021). Besides, a dropout layer before the ﬁnal linear layer is inserted. This network is initialized by\\nrandom rather than loading the pretrained parameters on ImageNet.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nWe list the values of hyperparameters for different datasets below. All models are optimized by Adam (Kingma & Ba, 2015).\\nIn our experiments, we found that keeping the balance of the three KL divergence terms for zc, zw and zv via adjusting the\\nvalue of λ1, λ2 and λ3 is beneﬁcial for the ﬁnal results.\\nTable 11.Hyperparametes and their default values.\\nDataset Parameters Value\\nCircle\\nlearning rate for Ec,D,C 5e-5\\nlearning rate for Ew,Fw,Ev,Fv 5e-6\\nbatch size 24\\nλ1,λ2,λ3 1.0, 1.0, 1.0\\nα 0.05\\nCircle-C\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 1.0, 2.0, 1.0\\nα 0.05\\nSine\\nlearning rate for Ec,D,C 5e-5\\nlearning rate for Ew,Fw,Ev,Fv 5e-6\\nbatch size 24\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nSine-C\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nRMNIST\\nlearning rate for Ec,D,C 1e-3\\nlearning rate for Ew,Fw,Ev,Fv 1e-4\\nbatch size 48\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nPortraits\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 0.5, 1.0, 1.0\\nα 0.05\\nCalTran\\nlearning rate for Ec,D,C 5e-5\\nlearning rate for Ew,Fw,Ev,Fv 5e-6\\nbatch size 24\\nλ1,λ2,λ3 1.0, 1.0, 1.0\\nα 0.1\\nPowerSupply\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 48\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nD. Additional Experimental Results\\nIn this section, we provide more experimental results for our proposed evolving domain generalization task. As we can see,\\nin most of the cases, we can achieve the state-of-the-art performance compared with other domain generalization baselines.\\nHowever, in some cases, our proposed method cannot achieve desired results. For example, in Sine-C, ERM and some\\ndomain generalization baselines can achieve a desired classiﬁcation performance especially for domain index 17,18 and 19,\\nwhich we conjecture the reason that their decision boundaries overﬁt to the abrupt concept shift, which further leads to their\\npoor performance on the following domains after domain 19. However, our proposed method aims to learn the evolving\\npattern starting from t= 0, which may not be optimal to suit to this abrupt concept shift case. One possible future direction\\nis only to learn the evolving pattern in a certain time duration (i.e., time duration in [t−T0,t], where tis the current time\\nstamp). Nevertheless, our proposed method shows its effectiveness in dynamic modeling for evolving domain generalization\\ntask. We will leave the discussion how to ﬁnd a suitable time duration (i.e., a suitable T0) in our future work.\\nTable 12.Circle. We show the results on each target domain by domain index.\\nAlgorithm 21 22 23 24 25 26 27 28 29 30 Avg\\nERM 53.9 ±3.5 55.8 ±4.8 53.9 ±5.2 44.7 ±6.3 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 49.9\\nMixup 48.6 ±3.8 51.7 ±4.0 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 48.4\\nMMD 50.0 ±3.9 53.6 ±4.4 55.0 ±4.3 51.9 ±6.0 60.8 ±3.9 49.7 ±7.2 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 50.7\\nMLDG 57.8 ±3.6 57.7 ±5.0 55.3 ±4.9 46.4 ±6.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 50.8\\nIRM 57.8 ±3.9 59.4 ±5.4 56.9 ±4.9 48.1 ±7.4 57.5 ±4.3 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 51.3\\nRSC 45.3 ±3.6 51.4 ±3.8 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 48.0\\nMTL 61.4 ±2.2 57.2 ±6.4 53.3 ±5.1 48.3 ±6.2 56.9 ±4.8 49.2 ±3.7 43.3 ±5.0 45.8 ±2.8 54.2 ±5.7 42.2 ±4.9 51.2\\nFish 51.7 ±3.7 53.1 ±3.7 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 48.8\\nCORAL 65.3 ±3.2 63.9 ±4.4 60.0 ±4.8 56.4 ±6.0 60.2 ±4.3 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 53.9\\nAndMask 42.8 ±3.4 50.6 ±4.1 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 49.7 ±5.4 47.9\\nDIV A 81.3 ±3.5 76.3 ±4.2 74.7 ±4.6 56.7 ±5.1 67.0 ±6.1 62.3 ±5.1 62.0 ±5.6 66.3 ±4.1 70.3 ±5.6 62.0 ±4.2 67.9\\nLSSAE (Ours) 95.8 ±1.9 95.6 ±2.1 93.5 ±2.9 96.3 ±1.8 83.8 ±5.2 74.3 ±3.6 51.9 ±5.6 52.3 ±8.1 46.5 ±9.2 48.4 ±5.3 73.8\\nTable 13.Sine. We show the results on each target domain denoted by domain index.\\nAlgorithm 17 18 19 20 21 22 23 24 Avg\\nERM 71.4 ±6.1 91.0 ±1.5 81.6 ±2.4 53.4 ±2.9 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 63.0\\nMixup 63.1 ±5.9 93.5 ±1.7 80.6 ±3.8 52.8 ±2.9 60.3 ±7.2 54.2 ±2.7 49.5 ±4.4 49.3 ±8.0 62.9\\nMMD 57.0 ±4.2 57.1 ±4.1 47.6 ±5.4 50.0 ±1.8 55.1 ±6.7 54.4 ±4.7 49.5 ±4.8 51.7 ±5.0 55.8\\nMLDG 69.2 ±4.2 67.7 ±4.1 52.1 ±5.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 63.2\\nIRM 66.9 ±6.2 81.1 ±3.2 88.5 ±3.0 56.6 ±6.0 57.2 ±5.8 53.7 ±5.1 49.5 ±2.2 51.7 ±5.4 63.2\\nRSC 61.3 ±6.6 83.5 ±1.9 84.5 ±2.6 52.8 ±2.8 55.1 ±6.7 54.4 ±4.7 49.5 ±4.8 51.7 ±5.0 61.5\\nMTL 70.6 ±6.6 91.6 ±1.2 79.9 ±3.4 51.0 ±4.7 60.3 ±7.6 53.6 ±5.2 49.5 ±5.3 46.9 ±5.9 62.9\\nFish 66.1 ±6.9 82.0 ±2.7 87.5 ±2.4 55.2 ±3.0 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 62.3\\nCORAL 60.0 ±5.3 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 49.7 ±6.2 48.6 ±4.6 46.3 ±5.0 51.7 ±5.0 51.6\\nAndMask 44.2 ±5.1 42.9 ±4.2 54.2 ±7.0 71.9 ±1.9 86.4 ±3.2 90.4 ±2.9 88.1 ±3.4 76.4 ±3.7 69.3\\nDIV A 79.0 ±6.6 60.8 ±1.9 47.6 ±2.6 50.0 ±2.8 55.1 ±6.7 51.9 ±4.7 38.6 ±4.8 40.4 ±5.0 52.9\\nLSSAE (Ours) 93.0 ±1.7 86.9 ±0.7 69.2 ±1.5 63.8 ±3.8 68.8 ±2.5 76.8 ±4.8 63.9 ±1.3 49.0 ±3.1 71.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 14.RMNIST. We show the results on each target domain denoted by rotation angle.\\nAlgorithm 130◦ 140◦ 150◦ 160◦ 170◦ 180◦ Avg\\nERM 56.8 ±0.9 44.2 ±0.8 37.8 ±0.6 38.3 ±0.8 40.9 ±0.8 43.6 ±0.8 43.6\\nMixup 61.3 ±0.7 47.4 ±0.8 39.1 ±0.7 38.3 ±0.7 40.5 ±0.8 42.8 ±0.9 44.9\\nMMD 59.2 ±0.9 46.0 ±0.8 39.0 ±0.7 39.3 ±0.8 41.6 ±0.7 43.7 ±0.8 44.8\\nMLDG 57.4 ±0.7 44.5 ±0.9 37.5 ±0.8 37.5 ±0.8 39.9 ±0.8 42.0 ±0.9 43.1\\nIRM 47.7 ±0.9 38.5 ±0.7 34.1 ±0.7 35.7 ±0.8 37.8 ±0.8 40.3 ±0.8 39.0\\nRSC 54.1 ±0.9 41.9 ±0.8 35.8 ±0.7 37.0 ±0.8 39.8 ±0.8 41.6 ±0.8 41.7\\nMTL 54.8 ±0.9 43.1 ±0.8 36.4 ±0.8 36.1 ±0.8 39.1 ±0.9 40.9 ±0.8 41.7\\nFish 60.8 ±0.8 47.8 ±0.8 39.2 ±0.8 37.6 ±0.7 39.0 ±0.8 40.7 ±0.7 44.2\\nCORAL 58.8 ±0.9 46.2 ±0.8 38.9 ±0.7 38.5 ±0.8 41.3 ±0.8 43.5 ±0.8 44.5\\nAndMask 53.5 ±0.9 42.9 ±0.8 37.8 ±0.7 38.6 ±0.8 40.8 ±0.8 43.2 ±0.8 42.8\\nDIV A 58.3 ±0.8 45.0 ±0.8 37.6 ±0.8 36.9 ±0.7 38.1 ±0.8 40.1 ±0.8 42.7\\nLSSAE (Ours) 64.1 ±0.8 51.6 ±0.8 43.4 ±0.8 38.6 ±0.7 40.3 ±0.8 40.4 ±0.8 46.4\\nTable 15.Portraits. We show the results on each target domain denoted by domain index.\\nAlgorithm 25 26 27 28 29 30 31 32 33 34 Avg\\nERM 75.5 ±0.9 83.8 ±0.9 88.5 ±0.8 93.3 ±0.7 93.4 ±0.6 92.1 ±0.7 90.6 ±0.8 84.3 ±0.9 88.5 ±0.9 87.9 ±1.4 87.8\\nMixup 75.5 ±0.9 83.8 ±0.9 88.5 ±0.8 93.3 ±0.7 93.4 ±0.6 92.1 ±0.7 90.6 ±0.8 84.3 ±0.9 88.5 ±0.9 87.9 ±1.4 87.8\\nMMD 74.0 ±1.0 83.8 ±0.8 87.2 ±0.8 93.0 ±0.7 93.0 ±0.6 91.9 ±0.7 90.9 ±0.7 84.7 ±1.4 88.3 ±0.9 85.8 ±1.8 87.3\\nMLDG 76.4 ±0.8 85.5 ±0.9 90.1 ±0.7 94.3 ±0.6 93.5 ±0.6 92.0 ±0.7 90.8 ±0.8 85.6 ±1.1 89.3 ±0.8 87.6 ±1.6 88.5\\nIRM 74.2 ±0.9 83.5 ±0.9 88.5 ±0.8 91.0 ±0.8 90.4 ±0.7 87.3 ±0.8 87.0 ±0.9 80.4 ±1.5 86.7 ±0.9 85.1 ±1.8 85.4\\nRSC 75.2 ±0.9 84.7 ±0.8 87.9 ±0.7 93.3 ±0.7 92.5 ±0.7 91.0 ±0.7 90.0 ±0.7 84.6 ±1.2 88.2 ±0.8 85.8 ±1.9 87.3\\nMTL 78.2 ±0.9 86.5 ±0.8 90.9 ±0.8 94.2 ±0.7 93.8 ±0.6 92.0 ±0.7 91.2 ±0.7 86.0 ±1.2 89.3 ±0.8 87.4 ±1.4 89.0\\nFish 78.6 ±0.9 86.9 ±0.8 89.5 ±0.8 93.5 ±0.7 93.3 ±0.6 92.1 ±0.6 91.1 ±0.7 86.2 ±1.3 88.7 ±0.9 87.7 ±1.6 88.8\\nCORAL 74.6 ±0.9 84.6 ±0.8 87.9 ±0.8 93.3 ±0.6 92.7 ±0.7 91.5 ±0.7 90.7 ±0.7 84.6 ±1.5 88.1 ±0.9 85.9 ±1.9 87.4\\nAndMask 62.0 ±1.1 70.8 ±1.1 67.0 ±1.2 70.2 ±1.1 75.2 ±1.1 74.1 ±1.0 72.7 ±1.1 64.7 ±1.6 77.3 ±1.1 74.9 ±2.1 70.9\\nDIV A 76.2 ±1.0 86.6 ±0.8 88.8 ±0.8 93.5 ±0.7 93.1 ±0.6 91.6 ±0.6 91.1 ±0.7 84.7 ±1.3 89.1 ±0.8 87.0 ±1.5 88.2\\nLSSAE (Ours) 77.7 ±0.9 87.1 ±0.8 90.8 ±0.7 94.3 ±0.6 94.3 ±0.6 92.2 ±0.6 91.2 ±0.7 86.7 ±1.1 89.6 ±0.8 86.9 ±1.4 89.1\\nTable 16.CalTran. We show the results on each target domain denoted by domain index.\\nAlgorithm 25 26 27 28 29 30 31 32 33 34 Avg\\nERM 29.9 ±3.5 88.4 ±2.1 61.1 ±3.5 56.3 ±3.2 90.0 ±1.6 60.1 ±2.5 55.5 ±3.5 88.8 ±2.4 57.1 ±3.5 50.5 ±5.2 66.3\\nMixup 53.6 ±3.9 89.0 ±2.0 61.8 ±2.4 55.7 ±2.9 88.2 ±2.1 58.6 ±3.0 52.3 ±3.7 88.6 ±2.7 57.1 ±3.0 55.1 ±4.3 66.0\\nMMD 30.2 ±2.1 92.7 ±1.7 56.4 ±3.7 39.1 ±3.2 93.6 ±1.7 52.1 ±3.2 42.8 ±3.0 92.1 ±2.2 42.1 ±3.8 29.4 ±3.8 57.1\\nMLDG 54.8 ±4.1 88.6 ±2.6 62.2 ±3.6 55.1 ±4.1 88.3 ±1.7 60.9 ±4.3 51.7 ±2.6 89.0 ±1.9 56.5 ±3.4 55.3 ±4.8 66.2\\nIRM 46.4 ±3.7 90.8 ±1.7 60.8 ±3.4 52.9 ±3.1 91.8 ±1.7 56.6 ±3.1 52.1 ±2.9 90.9 ±2.6 55.6 ±3.9 43.1 ±5.5 64.1\\nRSC 57.2 ±3.0 88.4 ±2.6 62.6 ±3.0 56.5 ±3.7 88.0 ±2.4 59.4 ±3.0 51.9 ±2.9 90.0 ±2.0 59.4 ±2.9 56.0 ±3.1 67.0\\nMTL 64.2 ±3.0 87.2 ±2.5 64.9 ±3.9 60.0 ±4.8 84.5 ±2.2 60.6 ±3.5 52.6 ±3.7 83.9 ±2.9 58.2 ±4.1 65.7 ±5.6 68.2\\nFish 61.1 ±3.5 88.2 ±1.5 64.7 ±4.0 57.9 ±3.1 88.3 ±2.2 59.9 ±3.0 57.5 ±2.7 87.4 ±2.8 57.7 ±3.7 63.0 ±6.1 68.6\\nCORAL 50.4 ±3.0 90.8 ±2.0 61.2 ±3.8 55.0 ±2.5 92.0 ±1.7 56.8 ±3.8 52.0 ±3.8 90.9 ±1.6 56.8 ±2.4 50.9 ±5.6 65.7\\nAndMask 30.0 ±2.2 92.7 ±1.7 56.2 ±3.8 39.1 ±3.2 93.6 ±1.7 51.6 ±3.2 42.6 ±2.9 92.1 ±2.2 41.2 ±3.7 29.9 ±3.6 56.9\\nDIV A 60.6 ±2.9 90.1 ±1.7 67.5 ±3.1 58.9 ±3.5 88.4 ±2.8 58.7 ±3.3 53.8 ±3.6 89.8 ±1.7 61.8 ±4.8 62.0 ±3.4 69.2\\nLSSV AE 63.4 ±3.4 92.1 ±2.0 62.6 ±4.7 58.8 ±4.4 92.9 ±1.6 62.0 ±3.9 54.3 ±3.0 92.1 ±2.2 60.5 ±3.8 67.4 ±3.6 70.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 17.Circle-C. We show the results on each target domain denoted by domain index.\\nAlgorithm 21 22 23 24 25 26 27 28 29 30 Avg\\nERM 40.6 ±3.8 43.1 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.9\\nMixup 40.6 ±3.8 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.8\\nMMD 38.6 ±3.7 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.6\\nMLDG 44.4 ±4.4 43.9 ±5.0 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 35.3\\nIRM 63.6 ±4.2 56.7 ±7.0 48.1 ±4.3 37.2 ±4.8 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 39.7\\nRSC 38.6 ±3.7 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.6\\nMTL 41.7 ±5.0 45.6 ±7.1 36.9 ±6.3 36.4 ±6.2 44.7 ±6.0 31.4 ±3.2 27.8 ±4.2 28.3 ±2.8 31.9 ±4.6 26.1 ±4.1 35.1\\nFish 42.5 ±3.8 43.3 ±4.8 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 35.1\\nCORAL 40.8 ±3.7 43.3 ±4.8 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.9\\nAndMask 53.6 ±4.2 54.7 ±5.4 51.7 ±4.7 33.6 ±5.1 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 35.0 ±6.7 38.9\\nDIV A 40.6 ±3.8 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.8\\nLSSAE (Ours) 74.5 ±1.8 65.5 ±3.9 55.5 ±3.2 36.0 ±3.5 45.0 ±0.7 40.5 ±4.6 35.0 ±1.4 33.0 ±1.4 34.0 ±6.4 29.0 ±2.8 44.8\\nTable 18.Sine-C. We show the results on each target domain denoted by domain index.\\nAlgorithm 17 18 19 20 21 22 23 24 Avg\\nERM 64.2 ±6.8 84.9 ±3.2 83.7 ±3.1 54.9 ±2.1 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.8\\nMixup 60.3 ±7.8 77.4 ±3.3 87.8 ±1.8 57.3 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.2\\nMMD 55.8 ±5.1 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 52.4\\nMLDG 65.6 ±6.4 88.7 ±2.3 84.4 ±2.8 52.4 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 62.2\\nIRM 61.4 ±7.0 82.8 ±3.2 86.5 ±3.0 54.9 ±1.8 55.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.5\\nRSC 65.0 ±6.7 83.5 ±3.1 85.4 ±3.1 53.8 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.8\\nMTL 61.9 ±7.3 82.0 ±2.3 83.7 ±4.2 54.2 ±5.4 60.3 ±7.6 53.6 ±5.2 49.5 ±5.3 46.9 ±5.9 61.5\\nFish 69.4 ±6.2 94.5 ±1.5 79.5 ±3.1 52.4 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 62.8\\nCORAL 70.3 ±5.0 77.2 ±3.6 67.0 ±4.2 52.1 ±2.5 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 59.2\\nAndMask 55.8 ±5.1 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 52.3\\nDIV A 76.9 ±6.1 61.1 ±5.9 47.6 ±3.4 48.6 ±4.2 51.1 ±7.0 52.9 ±6.1 38.5 ±5.0 36.5 ±6.8 51.7\\nLSSAE (Ours) 62.3 ±3.1 63.2 ±6.7 57.6 ±2.6 66.4 ±2.6 63.5 ±3.9 59.5 ±4.0 52.6 ±2.2 61.3 ±1.9 60.8\\nTable 19.PowerSupply. We show the results on each target domain denoted by domain index.\\nAlgorithm 21 22 23 24 25 26 27 28 29 30 Avg\\nERM 69.8 ±1.4 70.0 ±1.4 69.2 ±1.3 64.4 ±1.5 85.8 ±1.0 76.0 ±1.3 70.1 ±1.5 69.8 ±1.5 69.0 ±1.3 65.5 ±1.5 71.0\\nMixup 69.6 ±1.4 69.5 ±1.5 68.3 ±1.5 64.3 ±1.5 87.1 ±1.0 76.6 ±1.3 70.1 ±1.4 69.2 ±1.3 68.1 ±1.5 65.0 ±1.6 70.8\\nMMD 70.0 ±1.3 69.7 ±1.4 68.7 ±1.4 64.8 ±1.5 85.6 ±1.0 76.1 ±1.3 70.0 ±1.5 69.5 ±1.4 68.7 ±1.3 65.6 ±1.5 70.9\\nMLDG 69.7 ±1.4 69.7 ±1.5 68.6 ±1.5 64.6 ±1.5 86.4 ±1.1 76.3 ±1.4 70.1 ±1.4 69.4 ±1.3 68.4 ±1.5 65.6 ±1.5 70.8\\nIRM 69.8 ±1.4 69.5 ±1.4 68.3 ±1.4 64.1 ±1.4 87.2 ±0.9 76.5 ±1.3 70.0 ±1.5 69.1 ±1.5 68.2 ±1.3 65.0 ±1.4 70.8\\nRSC 69.9 ±1.4 69.6 ±1.4 68.6 ±1.4 64.4 ±1.5 86.6 ±1.0 76.3 ±1.3 70.0 ±1.5 69.4 ±1.4 68.4 ±1.3 65.4 ±1.5 70.9\\nMTL 69.6 ±1.4 69.4 ±1.5 68.2 ±1.6 64.2 ±1.5 87.4 ±1.2 76.6 ±1.3 69.9 ±1.5 69.1 ±1.5 68.2 ±1.5 64.6 ±1.4 70.7\\nFish 69.7 ±1.4 69.4 ±1.4 68.2 ±1.4 64.2 ±1.4 87.3 ±1.0 76.6 ±1.3 69.9 ±1.5 69.2 ±1.5 68.2 ±1.3 65.2 ±1.5 70.8\\nCORAL 69.9 ±1.4 69.7 ±1.4 68.9 ±1.4 64.6 ±1.4 86.1 ±1.0 76.3 ±1.3 70.0 ±1.5 69.5 ±1.5 68.8 ±1.3 65.7 ±1.5 71.0\\nAndMask 69.9 ±1.4 69.4 ±1.4 68.2 ±1.3 64.0 ±1.4 87.4 ±0.9 76.7 ±1.3 70.0 ±1.5 69.1 ±1.5 68.0 ±1.3 64.7 ±1.5 70.7\\nDIV A 69.7 ±1.4 69.5 ±1.3 68.2 ±1.4 63.9 ±1.5 87.5 ±1.0 76.5 ±1.3 69.9 ±1.5 69.1 ±1.5 68.1 ±1.3 64.7 ±1.5 70.7\\nLSSAE (Ours) 70.0 ±1.4 69.8 ±1.4 69.0 ±1.5 65.4 ±1.4 85.1 ±1.1 76.0 ±1.4 70.1 ±1.7 69.9 ±1.3 69.0 ±1.6 66.3 ±1.4 71.1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5154dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\",\"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents=documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d88d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 85 documents into 383 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Unsupervised Domain Adaptation by Backpropagation\n",
      "Yaroslav Ganin GANIN @SKOLTECH .RU\n",
      "Victor Lempitsky LEMPITSKY @SKOLTECH .RU\n",
      "Skolkovo Institute of Science and Technology (Skoltech)\n",
      "Abstract\n",
      "Top-perfo...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nYaroslav Ganin GANIN @SKOLTECH .RU\\nVictor Lempitsky LEMPITSKY @SKOLTECH .RU\\nSkolkovo Institute of Science and Technology (Skoltech)\\nAbstract\\nTop-performing deep architectures are trained on\\nmassive amounts of labeled data. In the absence\\nof labeled data for a certain task, domain adap-\\ntation often provides an attractive option given\\nthat labeled data of similar nature but from a dif-\\nferent domain (e.g. synthetic images) are avail-\\nable. Here, we propose a new approach to do-\\nmain adaptation in deep architectures that can\\nbe trained on large amount of labeled data from\\nthe source domain and large amount of unlabeled\\ndata from the target domain (no labeled target-\\ndomain data is necessary).\\nAs the training progresses, the approach pro-\\nmotes the emergence of “deep” features that are\\n(i) discriminative for the main learning task on\\nthe source domain and (ii) invariant with respect\\nto the shift between the domains. We show that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='motes the emergence of “deep” features that are\\n(i) discriminative for the main learning task on\\nthe source domain and (ii) invariant with respect\\nto the shift between the domains. We show that\\nthis adaptation behaviour can be achieved in al-\\nmost any feed-forward model by augmenting it\\nwith few standard layers and a simple new gra-\\ndient reversal layer. The resulting augmented\\narchitecture can be trained using standard back-\\npropagation.\\nOverall, the approach can be implemented with\\nlittle effort using any of the deep-learning pack-\\nages. The method performs very well in a se-\\nries of image classiﬁcation experiments, achiev-\\ning adaptation effect in the presence of big do-\\nmain shifts and outperforming previous state-of-\\nthe-art on Ofﬁce datasets.\\n1. Introduction\\nDeep feed-forward architectures have brought impressive\\nadvances to the state-of-the-art across a wide variety of\\nmachine-learning tasks and applications. At the moment,\\nhowever, these leaps in performance come only when a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='advances to the state-of-the-art across a wide variety of\\nmachine-learning tasks and applications. At the moment,\\nhowever, these leaps in performance come only when a\\nlarge amount of labeled training data is available. At the\\nsame time, for problems lacking labeled data, it may be\\nstill possible to obtain training sets that are big enough for\\ntraining large-scale deep models, but that suffer from the\\nshift in data distribution from the actual data encountered\\nat “test time”. One particularly important example is syn-\\nthetic or semi-synthetic training data, which may come in\\nabundance and be fully labeled, but which inevitably have\\na distribution that is different from real data (Liebelt &\\nSchmid, 2010; Stark et al., 2010; V´azquez et al., 2014; Sun\\n& Saenko, 2014).\\nLearning a discriminative classiﬁer or other predictor in\\nthe presence of a shift between training and test distribu-\\ntions is known as domain adaptation (DA). A number of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='& Saenko, 2014).\\nLearning a discriminative classiﬁer or other predictor in\\nthe presence of a shift between training and test distribu-\\ntions is known as domain adaptation (DA). A number of\\napproaches to domain adaptation has been suggested in the\\ncontext of shallow learning, e.g. in the situation when data\\nrepresentation/features are given and ﬁxed. The proposed\\napproaches then build the mappings between the source\\n(training-time) and the target (test-time) domains, so that\\nthe classiﬁer learned for the source domain can also be ap-\\nplied to the target domain, when composed with the learned\\nmapping between domains. The appeal of the domain\\nadaptation approaches is the ability to learn a mapping be-\\ntween domains in the situation when the target domain data\\nare either fully unlabeled ( unsupervised domain annota-\\ntion) or have few labeled samples (semi-supervised domain\\nadaptation). Below, we focus on the harder unsupervised\\ncase, although the proposed approach can be generalized to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='tion) or have few labeled samples (semi-supervised domain\\nadaptation). Below, we focus on the harder unsupervised\\ncase, although the proposed approach can be generalized to\\nthe semi-supervised case rather straightforwardly.\\nUnlike most previous papers on domain adaptation that\\nworked with ﬁxed feature representations, we focus on\\ncombining domain adaptation and deep feature learning\\nwithin one training process (deep domain adaptation). Our\\ngoal is to embed domain adaptation into the process of\\nlearning representation, so that the ﬁnal classiﬁcation de-\\ncisions are made based on features that are both discrim-\\ninative and invariant to the change of domains, i.e. have\\nthe same or very similar distributions in the source and the\\ntarget domains. In this way, the obtained feed-forward net-\\nwork can be applicable to the target domain without being\\nhindered by the shift between the two domains.\\nWe thus focus on learning features that combine (i)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='work can be applicable to the target domain without being\\nhindered by the shift between the two domains.\\nWe thus focus on learning features that combine (i)\\ndiscriminativeness and (ii) domain-invariance. This is\\nachieved by jointly optimizing the underlying features as\\nwell as two discriminative classiﬁers operating on these\\nfeatures: (i) the label predictor that predicts class labels\\nand is used both during training and at test time and (ii) the\\narXiv:1409.7495v2  [stat.ML]  27 Feb 2015'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\ndomain classiﬁer that discriminates between the source and\\nthe target domains during training. While the parameters of\\nthe classiﬁers are optimized in order to minimize their error\\non the training set, the parameters of the underlying deep\\nfeature mapping are optimized in order tominimize the loss\\nof the label classiﬁer and tomaximize the loss of the domain\\nclassiﬁer. The latter encourages domain-invariant features\\nto emerge in the course of the optimization.\\nCrucially, we show that all three training processes can\\nbe embedded into an appropriately composed deep feed-\\nforward network (Figure 1) that uses standard layers and\\nloss functions, and can be trained using standard backprop-\\nagation algorithms based on stochastic gradient descent or\\nits modiﬁcations (e.g. SGD with momentum). Our ap-\\nproach is generic as it can be used to add domain adaptation\\nto any existing feed-forward architecture that is trainable by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='its modiﬁcations (e.g. SGD with momentum). Our ap-\\nproach is generic as it can be used to add domain adaptation\\nto any existing feed-forward architecture that is trainable by\\nbackpropagation. In practice, the only non-standard com-\\nponent of the proposed architecture is a rather trivial gra-\\ndient reversal layer that leaves the input unchanged during\\nforward propagation and reverses the gradient by multiply-\\ning it by a negative scalar during the backpropagation.\\nBelow, we detail the proposed approach to domain adap-\\ntation in deep architectures, and present results on tradi-\\ntional deep learning image datasets (such as MNIST (Le-\\nCun et al., 1998) and SVHN (Netzer et al., 2011)) as well\\nas on O FFICE benchmarks (Saenko et al., 2010), where\\nthe proposed method considerably improves over previous\\nstate-of-the-art accuracy.\\n2. Related work\\nA large number of domain adaptation methods have been\\nproposed over the recent years, and here we focus on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='state-of-the-art accuracy.\\n2. Related work\\nA large number of domain adaptation methods have been\\nproposed over the recent years, and here we focus on the\\nmost related ones. Multiple methods perform unsuper-\\nvised domain adaptation by matching the feature distri-\\nbutions in the source and the target domains. Some ap-\\nproaches perform this by reweighing or selecting samples\\nfrom the source domain (Borgwardt et al., 2006; Huang\\net al., 2006; Gong et al., 2013), while others seek an ex-\\nplicit feature space transformation that would map source\\ndistribution into the target ones (Pan et al., 2011; Gopalan\\net al., 2011; Baktashmotlagh et al., 2013). An important\\naspect of the distribution matching approach is the way the\\n(dis)similarity between distributions is measured. Here,\\none popular choice is matching the distribution means in\\nthe kernel-reproducing Hilbert space (Borgwardt et al.,\\n2006; Huang et al., 2006), whereas (Gong et al., 2012; Fer-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='one popular choice is matching the distribution means in\\nthe kernel-reproducing Hilbert space (Borgwardt et al.,\\n2006; Huang et al., 2006), whereas (Gong et al., 2012; Fer-\\nnando et al., 2013) map the principal axes associated with\\neach of the distributions. Our approach also attempts to\\nmatch feature space distributions, however this is accom-\\nplished by modifying the feature representation itself rather\\nthan by reweighing or geometric transformation. Also, our\\nmethod uses (implicitly) a rather different way to measure\\nthe disparity between distributions based on their separa-\\nbility by a deep discriminatively-trained classiﬁer.\\nSeveral approaches perform gradual transition from the\\nsource to the target domain (Gopalan et al., 2011; Gong\\net al., 2012) by a gradual change of the training distribu-\\ntion. Among these methods, (S. Chopra & Gopalan, 2013)\\ndoes this in a “deep” way by the layerwise training of a\\nsequence of deep autoencoders, while gradually replacing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='tion. Among these methods, (S. Chopra & Gopalan, 2013)\\ndoes this in a “deep” way by the layerwise training of a\\nsequence of deep autoencoders, while gradually replacing\\nsource-domain samples with target-domain samples. This\\nimproves over a similar approach of (Glorot et al., 2011)\\nthat simply trains a single deep autoencoder for both do-\\nmains. In both approaches, the actual classiﬁer/predictor\\nis learned in a separate step using the feature representa-\\ntion learned by autoencoder(s). In contrast to (Glorot et al.,\\n2011; S. Chopra & Gopalan, 2013), our approach performs\\nfeature learning, domain adaptation and classiﬁer learning\\njointly, in a uniﬁed architecture, and using a single learning\\nalgorithm (backpropagation). We therefore argue that our\\napproach is simpler (both conceptually and in terms of its\\nimplementation). Our method also achieves considerably\\nbetter results on the popular OFFICE benchmark.\\nWhile the above approaches perform unsupervised domain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='implementation). Our method also achieves considerably\\nbetter results on the popular OFFICE benchmark.\\nWhile the above approaches perform unsupervised domain\\nadaptation, there are approaches that perform supervised\\ndomain adaptation by exploiting labeled data from the tar-\\nget domain. In the context of deep feed-forward archi-\\ntectures, such data can be used to “ﬁne-tune” the net-\\nwork trained on the source domain (Zeiler & Fergus, 2013;\\nOquab et al., 2014; Babenko et al., 2014). Our approach\\ndoes not require labeled target-domain data. At the same\\ntime, it can easily incorporate such data when it is avail-\\nable.\\nAn idea related to ours is described in (Goodfellow et al.,\\n2014). While their goal is quite different (building gener-\\native deep networks that can synthesize samples), the way\\nthey measure and minimize the discrepancy between the\\ndistribution of the training data and the distribution of the\\nsynthesized data is very similar to the way our architecture'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='they measure and minimize the discrepancy between the\\ndistribution of the training data and the distribution of the\\nsynthesized data is very similar to the way our architecture\\nmeasures and minimizes the discrepancy between feature\\ndistributions for the two domains.\\nFinally, a recent and concurrent report by (Tzeng et al.,\\n2014) also focuses on domain adaptation in feed-forward\\nnetworks. Their set of techniques measures and minimizes\\nthe distance of the data means across domains. This ap-\\nproach may be regarded as a “ﬁrst-order” approximation\\nto our approach, which seeks a tighter alignment between\\ndistributions.\\n3. Deep Domain Adaptation\\n3.1. The model\\nWe now detail the proposed model for the domain adap-\\ntation. We assume that the model works with input sam-\\nples x ∈ X, where X is some input space and cer-\\ntain labels (output) y from the label space Y. Below,\\nwe assume classiﬁcation problems where Y is a ﬁnite set\\n(Y = {1,2,...L }), however our approach is generic and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='tain labels (output) y from the label space Y. Below,\\nwe assume classiﬁcation problems where Y is a ﬁnite set\\n(Y = {1,2,...L }), however our approach is generic and\\ncan handle any output label space that other deep feed-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nFigure 1.The proposed architecture includes a deep feature extractor (green) and a deep label predictor (blue), which together form\\na standard feed-forward architecture. Unsupervised domain adaptation is achieved by adding a domain classiﬁer (red) connected to the\\nfeature extractor via a gradient reversal layer that multiplies the gradient by a certain negative constant during the backpropagation-\\nbased training. Otherwise, the training proceeds in a standard way and minimizes the label prediction loss (for source examples) and\\nthe domain classiﬁcation loss (for all samples). Gradient reversal ensures that the feature distributions over the two domains are made\\nsimilar (as indistinguishable as possible for the domain classiﬁer), thus resulting in the domain-invariant features.\\nforward models can handle. We further assume that there\\nexist two distributions S(x,y) and T(x,y) on X ⊗Y,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='forward models can handle. We further assume that there\\nexist two distributions S(x,y) and T(x,y) on X ⊗Y,\\nwhich will be referred to as the source distribution and\\nthe target distribution (or the source domain and the tar-\\nget domain). Both distributions are assumed complex and\\nunknown, and furthermore similar but different (in other\\nwords, Sis “shifted” from T by some domain shift).\\nOur ultimate goal is to be able to predict labels y given\\nthe input x for the target distribution. At training time,\\nwe have an access to a large set of training samples\\n{x1,x2,..., xN}from both the source and the target do-\\nmains distributed according to the marginal distributions\\nS(x) and T(x). We denote with di the binary variable (do-\\nmain label) for the i-th example, which indicates whether\\nxi come from the source distribution (xi∼S(x) if di=0) or\\nfrom the target distribution (xi∼T(x) if di=1). For the ex-\\namples from the source distribution (di=0) the correspond-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='xi come from the source distribution (xi∼S(x) if di=0) or\\nfrom the target distribution (xi∼T(x) if di=1). For the ex-\\namples from the source distribution (di=0) the correspond-\\ning labels yi ∈Y are known at training time. For the ex-\\namples from the target domains, we do not know the labels\\nat training time, and we want to predict such labels at test\\ntime.\\nWe now deﬁne a deep feed-forward architecture that for\\neach input x predicts its label y ∈Y and its domain label\\nd ∈{0,1}. We decompose such mapping into three parts.\\nWe assume that the input x is ﬁrst mapped by a mapping\\nGf (a feature extractor) to a D-dimensional feature vector\\nf ∈RD. The feature mapping may also include several\\nfeed-forward layers and we denote the vector of parame-\\nters of all layers in this mapping as θf, i.e. f = Gf(x; θf).\\nThen, the feature vector f is mapped by a mapping Gy (la-\\nbel predictor) to the label y, and we denote the parameters\\nof this mapping with θy. Finally, the same feature vector f'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Then, the feature vector f is mapped by a mapping Gy (la-\\nbel predictor) to the label y, and we denote the parameters\\nof this mapping with θy. Finally, the same feature vector f\\nis mapped to the domain label dby a mapping Gd (domain\\nclassiﬁer) with the parameters θd (Figure 1).\\nDuring the learning stage, we aim to minimize the label\\nprediction loss on the annotated part (i.e. the source part)\\nof the training set, and the parameters of both the feature\\nextractor and the label predictor are thus optimized in or-\\nder to minimize the empirical loss for the source domain\\nsamples. This ensures the discriminativeness of the fea-\\ntures f and the overall good prediction performance of the\\ncombination of the feature extractor and the label predictor\\non the source domain.\\nAt the same time, we want to make the features f\\ndomain-invariant. That is, we want to make the dis-\\ntributions S(f) = {Gf(x; θf) |x∼S(x)}and T(f) =\\n{Gf(x; θf) |x∼T(x)}to be similar. Under the covariate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='domain-invariant. That is, we want to make the dis-\\ntributions S(f) = {Gf(x; θf) |x∼S(x)}and T(f) =\\n{Gf(x; θf) |x∼T(x)}to be similar. Under the covariate\\nshift assumption, this would make the label prediction ac-\\ncuracy on the target domain to be the same as on the source\\ndomain (Shimodaira, 2000). Measuring the dissimilarity\\nof the distributions S(f) and T(f) is however non-trivial,\\ngiven that f is high-dimensional, and that the distributions\\nthemselves are constantly changing as learning progresses.\\nOne way to estimate the dissimilarity is to look at the loss\\nof the domain classiﬁer Gd, provided that the parameters\\nθd of the domain classiﬁer have been trained to discrim-\\ninate between the two feature distributions in an optimal\\nway.\\nThis observation leads to our idea. At training time, in or-\\nder to obtain domain-invariant features, we seek the param-\\neters θf of the feature mapping that maximize the loss of\\nthe domain classiﬁer (by making the two feature distribu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='der to obtain domain-invariant features, we seek the param-\\neters θf of the feature mapping that maximize the loss of\\nthe domain classiﬁer (by making the two feature distribu-\\ntions as similar as possible), while simultaneously seeking\\nthe parameters θd of the domain classiﬁer that minimize the\\nloss of the domain classiﬁer. In addition, we seek to mini-\\nmize the loss of the label predictor.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nMore formally, we consider the functional:\\nE(θf,θy,θd) =\\n∑\\ni=1..N\\ndi=0\\nLy\\n(\\nGy(Gf(xi; θf); θy),yi\\n)\\n−\\nλ\\n∑\\ni=1..N\\nLd\\n(\\nGd(Gf(xi; θf); θd),yi\\n)\\n=\\n=\\n∑\\ni=1..N\\ndi=0\\nLi\\ny(θf,θy) −λ\\n∑\\ni=1..N\\nLi\\nd(θf,θd) (1)\\nHere, Ly(·,·) is the loss for label prediction (e.g. multino-\\nmial), Ld(·,·) is the loss for the domain classiﬁcation (e.g.\\nlogistic), while Li\\ny and Li\\nd denote the corresponding loss\\nfunctions evaluated at the i-th training example.\\nBased on our idea, we are seeking the parametersˆθf,ˆθy,ˆθd\\nthat deliver a saddle point of the functional (1):\\n(ˆθf,ˆθy) = arg min\\nθf ,θy\\nE(θf,θy,ˆθd) (2)\\nˆθd = arg max\\nθd\\nE(ˆθf,ˆθy,θd) . (3)\\nAt the saddle point, the parametersθd of the domain classi-\\nﬁer θd minimize the domain classiﬁcation loss (since it en-\\nters into (1) with the minus sign) while the parametersθy of\\nthe label predictor minimize the label prediction loss. The\\nfeature mapping parameters θf minimize the label predic-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='ters into (1) with the minus sign) while the parametersθy of\\nthe label predictor minimize the label prediction loss. The\\nfeature mapping parameters θf minimize the label predic-\\ntion loss (i.e. the features are discriminative), while maxi-\\nmizing the domain classiﬁcation loss (i.e. the features are\\ndomain-invariant). The parameter λcontrols the trade-off\\nbetween the two objectives that shape the features during\\nlearning.\\nBelow, we demonstrate that standard stochastic gradient\\nsolvers (SGD) can be adapted for the search of the saddle\\npoint (2)-(3).\\n3.2. Optimization with backpropagation\\nA saddle point (2)-(3) can be found as a stationary point of\\nthe following stochastic updates:\\nθf ←− θf −µ\\n(\\n∂Li\\ny\\n∂θf\\n−λ∂Li\\nd\\n∂θf\\n)\\n(4)\\nθy ←− θy −µ∂Li\\ny\\n∂θy\\n(5)\\nθd ←− θd −µ∂Li\\nd\\n∂θd\\n(6)\\nwhere µis the learning rate (which can vary over time).\\nThe updates (4)-(6) are very similar to stochastic gradient\\ndescent (SGD) updates for a feed-forward deep model that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='θd ←− θd −µ∂Li\\nd\\n∂θd\\n(6)\\nwhere µis the learning rate (which can vary over time).\\nThe updates (4)-(6) are very similar to stochastic gradient\\ndescent (SGD) updates for a feed-forward deep model that\\ncomprises feature extractor fed into the label predictor and\\ninto the domain classiﬁer. The difference is the −λfactor\\nin (4) (the difference is important, as without such factor,\\nstochastic gradient descent would try to make features dis-\\nsimilar across domains in order to minimize the domain\\nclassiﬁcation loss). Although direct implementation of (4)-\\n(6) as SGD is not possible, it is highly desirable to reduce\\nthe updates (4)-(6) to some form of SGD, since SGD (and\\nits variants) is the main learning algorithm implemented in\\nmost packages for deep learning.\\nFortunately, such reduction can be accomplished by intro-\\nducing a special gradient reversal layer (GRL) deﬁned as\\nfollows. The gradient reversal layer has no parameters as-\\nsociated with it (apart from the meta-parameter λ, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='ducing a special gradient reversal layer (GRL) deﬁned as\\nfollows. The gradient reversal layer has no parameters as-\\nsociated with it (apart from the meta-parameter λ, which\\nis not updated by backpropagation). During the forward\\npropagation, GRL acts as an identity transform. During\\nthe backpropagation though, GRL takes the gradient from\\nthe subsequent level, multiplies it by −λ and passes it to\\nthe preceding layer. Implementing such layer using exist-\\ning object-oriented packages for deep learning is simple, as\\ndeﬁning procedures for forwardprop (identity transform),\\nbackprop (multiplying by a constant), and parameter up-\\ndate (nothing) is trivial.\\nThe GRL as deﬁned above is inserted between the feature\\nextractor and the domain classiﬁer, resulting in the archi-\\ntecture depicted in Figure 1. As the backpropagation pro-\\ncess passes through the GRL, the partial derivatives of the\\nloss that is downstream the GRL (i.e. Ld) w.r.t. the layer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='tecture depicted in Figure 1. As the backpropagation pro-\\ncess passes through the GRL, the partial derivatives of the\\nloss that is downstream the GRL (i.e. Ld) w.r.t. the layer\\nparameters that are upstream the GRL (i.e. θf) get multi-\\nplied by −λ, i.e. ∂Ld\\n∂θf\\nis effectively replaced with −λ∂Ld\\n∂θf\\n.\\nTherefore, running SGD in the resulting model implements\\nthe updates (4)-(6) and converges to a saddle point of (1).\\nMathematically, we can formally treat the gradient reversal\\nlayer as a “pseudo-function”Rλ(x) deﬁned by two (incom-\\npatible) equations describing its forward- and backpropa-\\ngation behaviour:\\nRλ(x) = x (7)\\ndRλ\\ndx = −λI (8)\\nwhere I is an identity matrix. We can then deﬁne the\\nobjective “pseudo-function” of (θf,θy,θd) that is being\\noptimized by the stochastic gradient descent within our\\nmethod:\\n˜E(θf,θy,θd) =\\n∑\\ni=1..N\\ndi=0\\nLy\\n(\\nGy(Gf(xi; θf); θy),yi\\n)\\n+\\n∑\\ni=1..N\\nLd\\n(\\nGd(Rλ(Gf(xi; θf)); θd),yi\\n)\\n(9)\\nRunning updates (4)-(6) can then be implemented as do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='method:\\n˜E(θf,θy,θd) =\\n∑\\ni=1..N\\ndi=0\\nLy\\n(\\nGy(Gf(xi; θf); θy),yi\\n)\\n+\\n∑\\ni=1..N\\nLd\\n(\\nGd(Rλ(Gf(xi; θf)); θd),yi\\n)\\n(9)\\nRunning updates (4)-(6) can then be implemented as do-\\ning SGD for (9) and leads to the emergence of features\\nthat are domain-invariant and discriminative at the same\\ntime. After the learning, the label predictor y(x) =\\nGy(Gf(x; θf); θy) can be used to predict labels for sam-\\nples from the target domain (as well as from the source\\ndomain).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nThe simple learning procedure outlined above can be re-\\nderived/generalized along the lines suggested in (Goodfel-\\nlow et al., 2014) (see Appendix A).\\n3.3. Relation to H∆H-distance\\nIn this section we give a brief analysis of our method in\\nterms of H∆H-distance (Ben-David et al., 2010; Cortes &\\nMohri, 2011) which is widely used in the theory of non-\\nconservative domain adaptation. Formally,\\ndH∆H(S,T) = 2 sup\\nh1,h2∈H\\n|Pf ∼S[h1(f) ̸= h2(f)]−\\n−Pf ∼T[h1(f) ̸= h2(f)]| (10)\\ndeﬁnes a discrepancy distance between two distributions S\\nand T w.r.t. a hypothesis set H. Using this notion one can\\nobtain a probabilistic bound (Ben-David et al., 2010) on the\\nperformance εT(h) of some classiﬁer hfrom T evaluated\\non the target domain given its performance εS(h) on the\\nsource domain:\\nεT(h) ≤εS(h) + 1\\n2dH∆H(S,T) + C, (11)\\nwhere Sand T are source and target distributions respec-\\ntively, and Cdoes not depend on particular h.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='source domain:\\nεT(h) ≤εS(h) + 1\\n2dH∆H(S,T) + C, (11)\\nwhere Sand T are source and target distributions respec-\\ntively, and Cdoes not depend on particular h.\\nConsider ﬁxed Sand T over the representation space pro-\\nduced by the feature extractor Gf and a family of label\\npredictors Hp. We assume that the family of domain classi-\\nﬁers Hd is rich enough to contain the symmetric difference\\nhypothesis set of Hp:\\nHp∆Hp = {h|h= h1 ⊕h2 , h1,h2 ∈Hp}. (12)\\nIt is not an unrealistic assumption as we have a freedom to\\npick Hd whichever we want. For example, we can set the\\narchitecture of the domain discriminator to be the layer-\\nby-layer concatenation of two replicas of the label predic-\\ntor followed by a two layer non-linear perceptron aimed to\\nlearn the XOR-function. Given the assumption holds, one\\ncan easily show that training the Gd is closely related to\\nthe estimation of dHp∆Hp (S,T). Indeed,\\ndHp∆Hp (S,T) =\\n= 2 sup\\nh∈Hp∆Hp\\n|Pf ∼S[h(f) = 1] −Pf ∼T[h(f) = 1]|≤\\n≤2 sup\\nh∈Hd'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='can easily show that training the Gd is closely related to\\nthe estimation of dHp∆Hp (S,T). Indeed,\\ndHp∆Hp (S,T) =\\n= 2 sup\\nh∈Hp∆Hp\\n|Pf ∼S[h(f) = 1] −Pf ∼T[h(f) = 1]|≤\\n≤2 sup\\nh∈Hd\\n|Pf ∼S[h(f) = 1] −Pf ∼T[h(f) = 1]|=\\n= 2 sup\\nh∈Hd\\n|1 −α(h)|= 2 sup\\nh∈Hd\\n[α(h) −1]\\n(13)\\nwhere α(h) = Pf ∼S[h(f) = 0] + Pf ∼T[h(f) = 1] is max-\\nimized by the optimal Gd.\\nThus, optimal discriminator gives the upper bound for\\ndHp∆Hp (S,T). At the same time, backpropagation of\\nthe reversed gradient changes the representation space\\nso that α(Gd) becomes smaller effectively reducing\\ndHp∆Hp (S,T) and leading to the better approximation of\\nεT(Gy) by εS(Gy).\\n4. Experiments\\nWe perform extensive evaluation of the proposed approach\\non a number of popular image datasets and their modiﬁ-\\ncations. These include large-scale datasets of small im-\\nages popular with deep learning methods, and the O FFICE\\ndatasets (Saenko et al., 2010), which are ade facto standard\\nfor domain adaptation in computer vision, but have much'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='ages popular with deep learning methods, and the O FFICE\\ndatasets (Saenko et al., 2010), which are ade facto standard\\nfor domain adaptation in computer vision, but have much\\nfewer images.\\nBaselines. For the bulk of experiments the following base-\\nlines are evaluated. The source-only model is trained with-\\nout consideration for target-domain data (no domain clas-\\nsiﬁer branch included into the network). The train-on-\\ntarget model is trained on the target domain with class\\nlabels revealed. This model serves as an upper bound on\\nDA methods, assuming that target data are abundant and\\nthe shift between the domains is considerable.\\nIn addition, we compare our approach against the recently\\nproposed unsupervised DA method based on subspace\\nalignment (SA) (Fernando et al., 2013), which is simple\\nto setup and test on new datasets, but has also been shown\\nto perform very well in experimental comparisons with\\nother “shallow” DA methods. To boost the performance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='to setup and test on new datasets, but has also been shown\\nto perform very well in experimental comparisons with\\nother “shallow” DA methods. To boost the performance\\nof this baseline, we pick its most important free parame-\\nter (the number of principal components) from the range\\n{2,..., 60}, so that the test performance on the target do-\\nmain is maximized. To apply SA in our setting, we train\\na source-only model and then consider the activations of\\nthe last hidden layer in the label predictor (before the ﬁnal\\nlinear classiﬁer) as descriptors/features, and learn the map-\\nping between the source and the target domains (Fernando\\net al., 2013).\\nSince the SA baseline requires to train a new classiﬁer after\\nadapting the features, and in order to put all the compared\\nsettings on an equal footing, we retrain the last layer of\\nthe label predictor using a standard linear SVM (Fan et al.,\\n2008) for all four considered methods (including ours; the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='settings on an equal footing, we retrain the last layer of\\nthe label predictor using a standard linear SVM (Fan et al.,\\n2008) for all four considered methods (including ours; the\\nperformance on the target domain remains approximately\\nthe same after the retraining).\\nFor the O FFICE dataset (Saenko et al., 2010), we directly\\ncompare the performance of our full network (feature ex-\\ntractor and label predictor) against recent DA approaches\\nusing previously published results.\\nCNN architectures. In general, we compose feature ex-\\ntractor from two or three convolutional layers, picking their\\nexact conﬁgurations from previous works. We give the ex-\\nact architectures in Appendix B.\\nFor the domain adaptator we stick to the three fully con-\\nnected layers ( x → 1024 → 1024 → 2), except for\\nMNIST where we used a simpler ( x → 100 → 2) ar-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nMNIST S YN NUMBERS SVHN S YN SIGNS\\nSOURCE\\nTARGET\\nMNIST-M SVHN MNIST GTSRB\\nFigure 2.Examples of domain pairs used in the experiments. See Section 4.1 for details.\\nMETHOD\\nSOURCE MNIST S YN NUMBERS SVHN S YN SIGNS\\nTARGET MNIST-M SVHN MNIST GTSRB\\nSOURCE ONLY .5749 .8665 .5919 .7400\\nSA (F ERNANDO ET AL ., 2013) .6078 (7.9%) .8672 (1.3%) .6157 (5.9%) .7635 (9.1%)\\nPROPOSED APPROACH .8149 (57.9%) .9048 (66.1%) .7107 (29.3%) .8866 (56.7%)\\nTRAIN ON TARGET .9891 .9244 .9951 .9987\\nTable 1.Classiﬁcation accuracies for digit image classiﬁcations for different source and target domains. MNIST-M corresponds to\\ndifference-blended digits over non-uniform background. The ﬁrst row corresponds to the lower performance bound (i.e. if no adaptation\\nis performed). The last row corresponds to training on the target domain data with known class labels (upper bound on the DA perfor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='is performed). The last row corresponds to training on the target domain data with known class labels (upper bound on the DA perfor-\\nmance). For each of the two DA methods (ours and (Fernando et al., 2013)) we show how much of the gap between the lower and the\\nupper bounds was covered (in brackets). For all ﬁve cases, our approach outperforms (Fernando et al., 2013) considerably, and covers a\\nbig portion of the gap.\\nchitecture to speed up the experiments.\\nFor loss functions, we set Ly and Ld to be the logistic re-\\ngression loss and the binomial cross-entropy respectively.\\nCNN training procedure. The model is trained on 128-\\nsized batches. Images are preprocessed by the mean sub-\\ntraction. A half of each batch is populated by the sam-\\nples from the source domain (with known labels), the rest\\nis comprised of the target domain (with unknown labels).\\nIn order to suppress noisy signal from the domain classiﬁer\\nat the early stages of the training procedure instead of ﬁxing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='is comprised of the target domain (with unknown labels).\\nIn order to suppress noisy signal from the domain classiﬁer\\nat the early stages of the training procedure instead of ﬁxing\\nthe adaptation factor λ, we gradually change it from 0 to 1\\nusing the following schedule:\\nλp = 2\\n1 + exp(−γ·p) −1, (14)\\nwhere γwas set to 10 in all experiments (the schedule was\\nnot optimized/tweaked). Further details on the CNN train-\\ning can be found in Appendix C.\\nVisualizations. We use t-SNE (van der Maaten, 2013) pro-\\njection to visualize feature distributions at different points\\nof the network, while color-coding the domains (Figure 3).\\nWe observe strong correspondence between the success of\\nthe adaptation in terms of the classiﬁcation accuracy for the\\ntarget domain, and the overlap between the domain distri-\\nbutions in such visualizations.\\nChoosing meta-parameters. In general, good unsu-\\npervised DA methods should provide ways to set meta-\\nparameters (such as λ, the learning rate, the momentum'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='butions in such visualizations.\\nChoosing meta-parameters. In general, good unsu-\\npervised DA methods should provide ways to set meta-\\nparameters (such as λ, the learning rate, the momentum\\nrate, the network architecture for our method) in an unsu-\\npervised way, i.e. without referring to labeled data in the\\ntarget domain. In our method, one can assess the per-\\nformance of the whole system (and the effect of chang-\\ning hyper-parameters) by observing the test error on the\\nsource domain and the domain classiﬁer error. In general,\\nwe observed a good correspondence between the success of\\nadaptation and these errors (adaptation is more successful\\nwhen the source domain test error is low, while the domain\\nclassiﬁer error is high). In addition, the layer, where the\\nthe domain adaptator is attached can be picked by comput-\\ning difference between means as suggested in (Tzeng et al.,\\n2014).\\n4.1. Results\\nWe now discuss the experimental settings and the results.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='the domain adaptator is attached can be picked by comput-\\ning difference between means as suggested in (Tzeng et al.,\\n2014).\\n4.1. Results\\nWe now discuss the experimental settings and the results.\\nIn each case, we train on the source dataset and test on a\\ndifferent target domain dataset, with considerable shifts be-\\ntween domains (see Figure 2). The results are summarized\\nin Table 1 and Table 2.\\nMNIST →MNIST-M. Our ﬁrst experiment deals with\\nthe MNIST dataset (LeCun et al., 1998) (source). In or-\\nder to obtain the target domain (MNIST-M) we blend dig-\\nits from the original set over patches randomly extracted\\nfrom color photos from BSDS500 (Arbelaez et al., 2011).\\nThis operation is formally deﬁned for two images I1,I2 as\\nIout\\nijk = |I1\\nijk −I2\\nijk|, where i,j are the coordinates of a\\npixel and k is a channel index. In other words, an output\\nsample is produced by taking a patch from a photo and in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nMETHOD\\nSOURCE AMAZON DSLR W EBCAM\\nTARGET WEBCAM WEBCAM DSLR\\nGFK(PLS, PCA) (G ONG ET AL ., 2012) .464 ±.005 .613 ±.004 .663 ±.004\\nSA (F ERNANDO ET AL ., 2013) .450 .648 .699\\nDA-NBNN (T OMMASI & CAPUTO , 2013) .528 ±.037 .766 ±.017 .762 ±.025\\nDLID (S. C HOPRA & GOPALAN , 2013) .519 .782 .899\\nDECAF6 SOURCE ONLY (DONAHUE ET AL ., 2014) .522 ±.017 .915 ±.015 –\\nDANN (G HIFARY ET AL ., 2014) .536 ±.002 .712 ±.000 .835 ±.000\\nDDC (T ZENG ET AL ., 2014) .594 ±.008 .925 ±.003 .917 ±.008\\nPROPOSED APPROACH .673 ±.017 .940 ±.008 .937 ±.010\\nTable 2.Accuracy evaluation of different DA approaches on the standard O FFICE (Saenko et al., 2010) dataset. Our method (last row)\\noutperforms competitors setting the new state-of-the-art.\\nMNIST →MNIST-M: top feature extractor layer\\n(a) Non-adapted\\n (b) Adapted\\nSYN NUMBERS →SVHN: last hidden layer of the label predictor\\n(a) Non-adapted\\n (b) Adapted'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='MNIST →MNIST-M: top feature extractor layer\\n(a) Non-adapted\\n (b) Adapted\\nSYN NUMBERS →SVHN: last hidden layer of the label predictor\\n(a) Non-adapted\\n (b) Adapted\\nFigure 3.The effect of adaptation on the distribution of the extracted features (best viewed in color). The ﬁgure shows t-SNE (van der\\nMaaten, 2013) visualizations of the CNN’s activations(a) in case when no adaptation was performed and(b) in case when our adaptation\\nprocedure was incorporated into training. Blue points correspond to the source domain examples, whilered ones correspond to the target\\ndomain. In all cases, the adaptation in our method makes the two distributions of features much closer.\\nverting its pixels at positions corresponding to the pixels of\\na digit. For a human the classiﬁcation task becomes only\\nslightly harder compared to the original dataset (the digits\\nare still clearly distinguishable) whereas for a CNN trained\\non MNIST this domain is quite distinct, as the background'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='slightly harder compared to the original dataset (the digits\\nare still clearly distinguishable) whereas for a CNN trained\\non MNIST this domain is quite distinct, as the background\\nand the strokes are no longer constant. Consequently, the\\nsource-only model performs poorly. Our approach suc-\\nceeded at aligning feature distributions (Figure 3), which\\nled to successful adaptation results (considering that the\\nadaptation is unsupervised). At the same time, the im-\\nprovement over source-only model achieved by subspace\\nalignment (SA) (Fernando et al., 2013) is quite modest,\\nthus highlighting the difﬁculty of the adaptation task.\\nSynthetic numbers →SVHN. To address a common sce-\\nnario of training on synthetic data and testing on real data,\\nwe use Street-View House Number dataset SVHN (Netzer\\net al., 2011) as the target domain and synthetic digits as the\\nsource. The latter (S YN NUMBERS ) consists of 500,000\\nimages generated by ourselves from Windows fonts by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='et al., 2011) as the target domain and synthetic digits as the\\nsource. The latter (S YN NUMBERS ) consists of 500,000\\nimages generated by ourselves from Windows fonts by\\nvarying the text (that includes different one-, two-, and\\nthree-digit numbers), positioning, orientation, background\\nand stroke colors, and the amount of blur. The degrees of\\nvariation were chosen manually to simulate SVHN, how-\\never the two datasets are still rather distinct, the biggest\\ndifference being the structured clutter in the background of\\nSVHN images.\\nThe proposed backpropagation-based technique works well\\ncovering two thirds of the gap between training with source\\ndata only and training on target domain data with known\\ntarget labels. In contrast, SA (Fernando et al., 2013) does\\nnot result in any signiﬁcant improvement in the classiﬁca-\\ntion accuracy, thus highlighting that the adaptation task is\\neven more challenging than in the case of the MNIST ex-\\nperiment.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='not result in any signiﬁcant improvement in the classiﬁca-\\ntion accuracy, thus highlighting that the adaptation task is\\neven more challenging than in the case of the MNIST ex-\\nperiment.\\nMNIST ↔SVHN. In this experiment, we further increase\\nthe gap between distributions, and test on MNIST and\\nSVHN, which are signiﬁcantly different in appearance.\\nTraining on SVHN even without adaptation is challeng-\\ning — classiﬁcation error stays high during the ﬁrst 150\\nepochs. In order to avoid ending up in a poor local min-\\nimum we, therefore, do not use learning rate annealing\\nhere. Obviously, the two directions (MNIST →SVHN\\nand SVHN → MNIST) are not equally difﬁcult. As\\nSVHN is more diverse, a model trained on SVHN is ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\n1 2 3 4 5\\n·104\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nBatches seen\\nValidation error\\nReal data only\\nSynthetic data only\\nBoth\\nFigure 4.Semi-supervised domain adaptation for the trafﬁc signs.\\nAs labeled target domain data are shown to the method, it achieves\\nsigniﬁcantly lower error than the model trained on target domain\\ndata only or on source domain data only.\\npected to be more generic and to perform reasonably on\\nthe MNIST dataset. This, indeed, turns out to be the case\\nand is supported by the appearance of the feature distribu-\\ntions. We observe a quite strong separation between the\\ndomains when we feed them into the CNN trained solely\\non MNIST, whereas for the SVHN-trained network the\\nfeatures are much more intermixed. This difference prob-\\nably explains why our method succeeded in improving the\\nperformance by adaptation in the SVHN →MNIST sce-\\nnario (see Table 1) but not in the opposite direction (SA is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='ably explains why our method succeeded in improving the\\nperformance by adaptation in the SVHN →MNIST sce-\\nnario (see Table 1) but not in the opposite direction (SA is\\nnot able to perform adaptation in this case either). Unsu-\\npervised adaptation from MNIST to SVHN gives a failure\\nexample for our approach (we are unaware of any unsuper-\\nvised DA methods capable of performing such adaptation).\\nSynthetic Signs →GTSRB. Overall, this setting is sim-\\nilar to the S YN NUMBERS →SVHN experiment, except\\nthe distribution of the features is more complex due to the\\nsigniﬁcantly larger number of classes (43 instead of 10).\\nFor the source domain we obtained 100,000 synthetic im-\\nages (which we call S YN SIGNS ) simulating various pho-\\ntoshooting conditions. Once again, our method achieves\\na sensible increase in performance once again proving its\\nsuitability for the synthetic-to-real data adaptation.\\nAs an additional experiment, we also evaluate the pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='a sensible increase in performance once again proving its\\nsuitability for the synthetic-to-real data adaptation.\\nAs an additional experiment, we also evaluate the pro-\\nposed algorithm for semi-supervised domain adaptation,\\ni.e. when one is additionally provided with a small amount\\nof labeled target data. For that purpose we split GTSRB\\ninto the train set (1280 random samples with labels) and\\nthe validation set (the rest of the dataset). The validation\\npart is used solely for the evaluation and does not partic-\\nipate in the adaptation. The training procedure changes\\nslightly as the label predictor is now exposed to the tar-\\nget data. Figure 4 shows the change of the validation error\\nthroughout the training. While the graph clearly suggests\\nthat our method can be used in the semi-supervised setting,\\nthorough veriﬁcation of semi-supervised setting is left for\\nfuture work.\\nOfﬁce dataset. We ﬁnally evaluate our method on O F-\\nFICE dataset, which is a collection of three distinct do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='thorough veriﬁcation of semi-supervised setting is left for\\nfuture work.\\nOfﬁce dataset. We ﬁnally evaluate our method on O F-\\nFICE dataset, which is a collection of three distinct do-\\nmains: A MAZON , DSLR, and W EBCAM . Unlike previ-\\nously discussed datasets, OFFICE is rather small-scale with\\nonly 2817 labeled images spread across 31 different cat-\\negories in the largest domain. The amount of available\\ndata is crucial for a successful training of a deep model,\\nhence we opted for the ﬁne-tuning of the CNN pre-trained\\non the ImageNet (Jia et al., 2014) as it is done in some re-\\ncent DA works (Donahue et al., 2014; Tzeng et al., 2014;\\nHoffman et al., 2013). We make our approach more com-\\nparable with (Tzeng et al., 2014) by using exactly the same\\nnetwork architecture replacing domain mean-based regu-\\nlarization with the domain classiﬁer.\\nFollowing most previous works, we evaluate our method\\nusing 5 random splits for each of the 3 transfer tasks com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='larization with the domain classiﬁer.\\nFollowing most previous works, we evaluate our method\\nusing 5 random splits for each of the 3 transfer tasks com-\\nmonly used for evaluation. Our training protocol is close to\\n(Tzeng et al., 2014; Saenko et al., 2010; Gong et al., 2012)\\nas we use the same number of labeled source-domain im-\\nages per category. Unlike those works and similarly to e.g.\\nDLID (S. Chopra & Gopalan, 2013) we use the whole un-\\nlabeled target domain (as the premise of our method is the\\nabundance of unlabeled data in the target domain). Un-\\nder this transductive setting, our method is able to improve\\npreviously-reported state-of-the-art accuracy for unsuper-\\nvised adaptation very considerably (Table 2), especially in\\nthe most challenging AMAZON →WEBCAM scenario (the\\ntwo domains with the largest domain shift).\\n5. Discussion\\nWe have proposed a new approach to unsupervised do-\\nmain adaptation of deep feed-forward architectures, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='two domains with the largest domain shift).\\n5. Discussion\\nWe have proposed a new approach to unsupervised do-\\nmain adaptation of deep feed-forward architectures, which\\nallows large-scale training based on large amount of an-\\nnotated data in the source domain and large amount of\\nunannotated data in the target domain. Similarly to many\\nprevious shallow and deep DA techniques, the adaptation\\nis achieved through aligning the distributions of features\\nacross the two domains. However, unlike previous ap-\\nproaches, the alignment is accomplished through standard\\nbackpropagation training. The approach is therefore rather\\nscalable, and can be implemented using any deep learning\\npackage. To this end we plan to release the source code for\\nthe Gradient Reversal layer along with the usage examples\\nas an extension to Caffe (Jia et al., 2014).\\nFurther evaluation on larger-scale tasks and in semi-\\nsupervised settings constitutes future work. It is also in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='as an extension to Caffe (Jia et al., 2014).\\nFurther evaluation on larger-scale tasks and in semi-\\nsupervised settings constitutes future work. It is also in-\\nteresting whether the approach can beneﬁt from a good ini-\\ntialization of the feature extractor. For this, a natural choice\\nwould be to use deep autoencoder/deconvolution network\\ntrained on both domains (or on the target domain) in the\\nsame vein as (Glorot et al., 2011; S. Chopra & Gopalan,\\n2013), effectively using (Glorot et al., 2011; S. Chopra &\\nGopalan, 2013) as an initialization to our method.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nAppendix A. An alternative optimization\\napproach\\nThere exists an alternative construction (inspired by (Good-\\nfellow et al., 2014)) that leads to the same updates (4)-(6).\\nRather than using the gradient reversal layer, the construc-\\ntion introduces two different loss functions for the domain\\nclassiﬁer. Minimization of the ﬁrst domain loss ( Ld+)\\nshould lead to a better domain discrimination, while the\\nsecond domain loss (Ld−) is minimized when the domains\\nare distinct. Stochastic updates for θf and θd are then de-\\nﬁned as:\\nθf ←− θf −µ\\n(\\n∂Li\\ny\\n∂θf\\n+ ∂Li\\nd−\\n∂θf\\n)\\nθd ←− θd −µ∂Li\\nd+\\n∂θd\\n,\\nThus, different parameters participate in the optimization\\nof different losses\\nIn this framework, the gradient reversal layer constitutes\\na special case, corresponding to the pair of domain losses\\n(Ld,−λLd). However, other pairs of loss functions can be\\nused. One example would be the binomial cross-entropy\\n(Goodfellow et al., 2014):\\nLd+(q,d) =\\n∑\\ni=1..N'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='(Ld,−λLd). However, other pairs of loss functions can be\\nused. One example would be the binomial cross-entropy\\n(Goodfellow et al., 2014):\\nLd+(q,d) =\\n∑\\ni=1..N\\ndilog(qi) + (1−di) log(1−qi) ,\\nwhere dindicates domain indices and qis an output of the\\npredictor. In that case “adversarial” loss is easily obtained\\nby swapping domain labels, i.e.Ld−(q,d) = Ld+(q,1−d).\\nThis particular pair has a potential advantage of produc-\\ning stronger gradients at early learning stages if the do-\\nmains are quite dissimilar. In our experiments, however,\\nwe did not observe any signiﬁcant improvement resulting\\nfrom this choice of losses.\\nAppendix B. CNN architectures\\nFour different architectures were used in our experiments\\n(ﬁrst three are shown in Figure 5):\\n• A smaller one (a) if the source domain is MNIST. This\\narchitecture was inspired by the classical LeNet-5 (Le-\\nCun et al., 1998).\\n• (b) for the experiments involving SVHN dataset. This\\none is adopted from (Srivastava et al., 2014).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='architecture was inspired by the classical LeNet-5 (Le-\\nCun et al., 1998).\\n• (b) for the experiments involving SVHN dataset. This\\none is adopted from (Srivastava et al., 2014).\\n• (c) in the S YN SINGS →GTSRB setting. We used\\nthe single-CNN baseline from (Cires ¸an et al., 2012)\\nas our starting point.\\n• Finally, we use pre-trained AlexNet from the\\nCaffe-package (Jia et al., 2014) for the O FFICE do-\\nmains. Adaptation architecture is identical to (Tzeng\\net al., 2014): 2-layer domain classiﬁer ( x→1024 →\\n1024 →2) is attached to the 256-dimensional bottle-\\nneck of fc7.\\nThe domain classiﬁer branch in all cases is somewhat ar-\\nbitrary (better adaptation performance might be attained if\\nthis part of the architecture is tuned).\\nAppendix C. Training procedure\\nWe use stochastic gradient descent with 0.9 momentum and\\nthe learning rate annealing described by the following for-\\nmula:\\nµp = µ0\\n(1 + α·p)β ,\\nwhere p is the training progress linearly changing from 0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='the learning rate annealing described by the following for-\\nmula:\\nµp = µ0\\n(1 + α·p)β ,\\nwhere p is the training progress linearly changing from 0\\nto 1, µ0 = 0 .01, α = 10 and β = 0 .75 (the schedule\\nwas optimized to promote convergence and low error on\\nthe source domain).\\nFollowing (Srivastava et al., 2014) we also use dropout and\\nℓ2-norm restriction when we train the SVHN architecture.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nconv 5x532 mapsReLU\\nmax-pool 2x22x2 stride\\nconv 5x548 mapsReLU\\nmax-pool 2x22x2 stride\\nfully-conn100 unitsReLU\\nfully-conn100 unitsReLU\\nfully-conn10 unitsSoft-max\\nGRL\\n fully-conn100 unitsReLU\\nfully-conn1 unitLogistic\\n(a) MNIST architecture\\nconv 5x564 mapsReLU\\nmax-pool 3x32x2 stride\\nconv 5x564 mapsReLU\\nmax-pool 3x32x2 stride\\nconv 5x5128 mapsReLU\\nfully-conn3072 unitsReLU\\nfully-conn2048 unitsReLU\\nfully-conn10 unitsSoft-max\\nGRL\\n fully-conn1024 unitsReLU\\nfully-conn1024 unitsReLU\\nfully-conn1 unitLogistic\\n(b) SVHN architecture\\nconv 5x596 mapsReLU\\nmax-pool 2x22x2 stride\\nconv 3x3144 mapsReLU\\nmax-pool 2x22x2 stride\\nconv 5x5256 mapsReLU\\nmax-pool 2x22x2 stride\\nfully-conn512 unitsReLU\\nfully-conn10 unitsSoft-max\\nGRL\\n fully-conn1024 unitsReLU\\nfully-conn1024 unitsReLU\\nfully-conn1 unitLogistic\\n(c) GTSRB architecture\\nFigure 5.CNN architectures used in the experiments. Boxes correspond to transformations applied to the data. Color-coding is the same'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='fully-conn1 unitLogistic\\n(c) GTSRB architecture\\nFigure 5.CNN architectures used in the experiments. Boxes correspond to transformations applied to the data. Color-coding is the same\\nas in Figure 1.\\nReferences\\nArbelaez, Pablo, Maire, Michael, Fowlkes, Charless, and\\nMalik, Jitendra. Contour detection and hierarchical im-\\nage segmentation. PAMI, 33, 2011.\\nBabenko, Artem, Slesarev, Anton, Chigorin, Alexander,\\nand Lempitsky, Victor S. Neural codes for image re-\\ntrieval. In ECCV, pp. 584–599, 2014.\\nBaktashmotlagh, Mahsa, Harandi, Mehrtash Tafazzoli,\\nLovell, Brian C., and Salzmann, Mathieu. Unsupervised\\ndomain adaptation by domain invariant projection. In\\nICCV, pp. 769–776, 2013.\\nBen-David, Shai, Blitzer, John, Crammer, Koby, Kulesza,\\nAlex, Pereira, Fernando, and Vaughan, Jennifer Wort-\\nman. A theory of learning from different domains.\\nJMLR, 79, 2010.\\nBorgwardt, Karsten M., Gretton, Arthur, Rasch, Malte J.,\\nKriegel, Hans-Peter, Sch ¨olkopf, Bernhard, and Smola,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='man. A theory of learning from different domains.\\nJMLR, 79, 2010.\\nBorgwardt, Karsten M., Gretton, Arthur, Rasch, Malte J.,\\nKriegel, Hans-Peter, Sch ¨olkopf, Bernhard, and Smola,\\nAlexander J. Integrating structured biological data by\\nkernel maximum mean discrepancy. In ISMB, pp. 49–\\n57, 2006.\\nCires ¸an, Dan, Meier, Ueli, Masci, Jonathan, and Schmid-\\nhuber, J ¨urgen. Multi-column deep neural network for\\ntrafﬁc sign classiﬁcation. Neural Networks, (32):333–\\n338, 2012.\\nCortes, Corinna and Mohri, Mehryar. Domain adaptation\\nin regression. In Algorithmic Learning Theory, 2011.\\nDonahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman,\\nJudy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. De-\\ncaf: A deep convolutional activation feature for generic\\nvisual recognition, 2014.\\nFan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,\\nXiang-Rui, and Lin, Chih-Jen. LIBLINEAR: A library\\nfor large linear classiﬁcation. Journal of Machine Learn-\\ning Research, 9:1871–1874, 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,\\nXiang-Rui, and Lin, Chih-Jen. LIBLINEAR: A library\\nfor large linear classiﬁcation. Journal of Machine Learn-\\ning Research, 9:1871–1874, 2008.\\nFernando, Basura, Habrard, Amaury, Sebban, Marc, and\\nTuytelaars, Tinne. Unsupervised visual domain adapta-\\ntion using subspace alignment. In ICCV, 2013.\\nGhifary, Muhammad, Kleijn, W Bastiaan, and Zhang,\\nMengjie. Domain adaptive neural networks for object\\nrecognition. In PRICAI 2014: Trends in Artiﬁcial Intel-\\nligence. 2014.\\nGlorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Do-\\nmain adaptation for large-scale sentiment classiﬁcation:\\nA deep learning approach. In ICML, pp. 513–520, 2011.\\nGong, Boqing, Shi, Yuan, Sha, Fei, and Grauman, Kristen.\\nGeodesic ﬂow kernel for unsupervised domain adapta-\\ntion. In CVPR, pp. 2066–2073, 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nGong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\\nnecting the dots with landmarks: Discriminatively learn-\\ning domain-invariant features for unsupervised domain\\nadaptation. In ICML, pp. 222–230, 2013.\\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\\nBing, Warde-Farley, David, Ozair, Sherjil, Courville,\\nAaron, and Bengio, Yoshua. Generative adversarial nets.\\nIn NIPS, 2014.\\nGopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\\nDomain adaptation for object recognition: An unsuper-\\nvised approach. In ICCV, pp. 999–1006, 2011.\\nHoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\\nSaenko, Kate, and Darrell, Trevor. One-shot adapta-\\ntion of supervised deep convolutional models. CoRR,\\nabs/1312.6204, 2013.\\nHuang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\\nBorgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\\nrecting sample selection bias by unlabeled data. InNIPS,\\npp. 601–608, 2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Huang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\\nBorgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\\nrecting sample selection bias by unlabeled data. InNIPS,\\npp. 601–608, 2006.\\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev,\\nSergey, Long, Jonathan, Girshick, Ross, Guadar-\\nrama, Sergio, and Darrell, Trevor. Caffe: Convolu-\\ntional architecture for fast feature embedding. CoRR,\\nabs/1408.5093, 2014.\\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-\\nbased learning applied to document recognition. Pro-\\nceedings of the IEEE , 86(11):2278–2324, November\\n1998.\\nLiebelt, Joerg and Schmid, Cordelia. Multi-view object\\nclass detection with a 3d geometric model. In CVPR,\\n2010.\\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,\\nAlessandro, Wu, Bo, and Ng, Andrew Y . Reading dig-\\nits in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning 2011, 2011.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='its in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning 2011, 2011.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\\nand transferring mid-level image representations using\\nconvolutional neural networks. In CVPR, 2014.\\nPan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\\nYang, Qiang. Domain adaptation via transfer component\\nanalysis. IEEE Transactions on Neural Networks, 22(2):\\n199–210, 2011.\\nS. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\\nlearning for domain adaptation by interpolating between\\ndomains. In ICML Workshop on Challenges in Repre-\\nsentation Learning, 2013.\\nSaenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\\nTrevor. Adapting visual category models to new do-\\nmains. In ECCV, pp. 213–226. 2010.\\nShimodaira, Hidetoshi. Improving predictive inference un-\\nder covariate shift by weighting the log-likelihood func-\\ntion. Journal of Statistical Planning and Inference , 90'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Shimodaira, Hidetoshi. Improving predictive inference un-\\nder covariate shift by weighting the log-likelihood func-\\ntion. Journal of Statistical Planning and Inference , 90\\n(2):227–244, October 2000.\\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\\nA simple way to prevent neural networks from overﬁt-\\nting. The Journal of Machine Learning Research, 15(1):\\n1929–1958, 2014.\\nStark, Michael, Goesele, Michael, and Schiele, Bernt. Back\\nto the future: Learning shape models from 3d CAD data.\\nIn BMVC, pp. 1–11, 2010.\\nSun, Baochen and Saenko, Kate. From virtual to reality:\\nFast adaptation of virtual object detectors to real do-\\nmains. In BMVC, 2014.\\nTommasi, Tatiana and Caputo, Barbara. Frustratingly easy\\nnbnn domain adaptation. In ICCV, 2013.\\nTzeng, Eric, Hoffman, Judy, Zhang, Ning, Saenko, Kate,\\nand Darrell, Trevor. Deep domain confusion: Maximiz-\\ning for domain invariance. CoRR, abs/1412.3474, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Tzeng, Eric, Hoffman, Judy, Zhang, Ning, Saenko, Kate,\\nand Darrell, Trevor. Deep domain confusion: Maximiz-\\ning for domain invariance. CoRR, abs/1412.3474, 2014.\\nvan der Maaten, Laurens. Barnes-hut-sne. CoRR,\\nabs/1301.3342, 2013.\\nV´azquez, David, L ´opez, Antonio Manuel, Mar ´ın, Javier,\\nPonsa, Daniel, and Gomez, David Ger ´onimo. Virtual\\nand real world adaptationfor pedestrian detection. IEEE\\nTrans. Pattern Anal. Mach. Intell., 36(4):797–809, 2014.\\nZeiler, Matthew D. and Fergus, Rob. Visualizing\\nand understanding convolutional networks. CoRR,\\nabs/1311.2901, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Simultaneous Deep Transfer Across Domains and Tasks\\nEric Tzeng∗, Judy Hoffman∗, Trevor Darrell\\nUC Berkeley, EECS & ICSI\\n{etzeng,jhoffman,trevor}@eecs.berkeley.edu\\nKate Saenko\\nUMass Lowell, CS\\nsaenko@cs.uml.edu\\nAbstract\\nRecent reports suggest that a generic supervised deep\\nCNN model trained on a large-scale dataset reduces, but\\ndoes not remove, dataset bias. Fine-tuning deep models in\\na new domain can require a signiﬁcant amount of labeled\\ndata, which for many applications is simply not available.\\nWe propose a new CNN architecture to exploit unlabeled and\\nsparsely labeled target domain data. Our approach simulta-\\nneously optimizes for domain invariance to facilitate domain\\ntransfer and uses a soft label distribution matching loss to\\ntransfer information between tasks. Our proposed adapta-\\ntion method offers empirical performance which exceeds\\npreviously published results on two standard benchmark vi-\\nsual domain adaptation tasks, evaluated across supervised'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='tion method offers empirical performance which exceeds\\npreviously published results on two standard benchmark vi-\\nsual domain adaptation tasks, evaluated across supervised\\nand semi-supervised adaptation settings.\\n1. Introduction\\nConsider a group of robots trained by the manufacturer\\nto recognize thousands of common objects using standard\\nimage databases, then shipped to households around the\\ncountry. As each robot starts to operate in its own unique\\nenvironment, it is likely to have degraded performance due\\nto the shift in domain. It is clear that, given enough ex-\\ntra supervised data from the new environment, the original\\nperformance could be recovered. However, state-of-the-art\\nrecognition algorithms rely on high capacity convolutional\\nneural network (CNN) models that require millions of su-\\npervised images for initial training. Even the traditional\\napproach for adapting deep models, ﬁne-tuning [ 14, 29],\\nmay require hundreds or thousands of labeled examples for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='pervised images for initial training. Even the traditional\\napproach for adapting deep models, ﬁne-tuning [ 14, 29],\\nmay require hundreds or thousands of labeled examples for\\neach object category that needs to be adapted.\\nIt is reasonable to assume that the robot’s new owner\\nwill label a handful of examples for a few types of objects,\\nbut completely unrealistic to presume full supervision in\\nthe new environment. Therefore, we propose an algorithm\\nthat effectively adapts between the training (source) and test\\n(target) environments by utilizing both generic statistics from\\n∗ Authors contributed equally.\\n!\\n!\\nBottleMugChairLaptopKeyboard\\nBottleMugChairLaptopKeyboardBottleMugChairLaptopKeyboard\\nBottleMugChairLaptopKeyboard\\n!\\n!\\nSource domain! Target domain! \\n!\\n!\\n !\\n!\\n!\\n\\x01!\\n!\\n\\x01!\\n!\\n\\x01!\\n!\\n\\x01!\\n1. Maximize domain confusion!\\n2. Transfer task correlation!\\n!\\n!\\nFigure 1. We transfer discriminative category information from\\na source domain to a target domain via two methods. First, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='!\\n\\x01!\\n!\\n\\x01!\\n1. Maximize domain confusion!\\n2. Transfer task correlation!\\n!\\n!\\nFigure 1. We transfer discriminative category information from\\na source domain to a target domain via two methods. First, we\\nmaximize domain confusion by making the marginal distributions\\nof the two domains as similar as possible. Second, we transfer cor-\\nrelations between classes learned on the source examples directly to\\nthe target examples, thereby preserving the relationships between\\nclasses.\\nunlabeled data collected in the new environment as well as a\\nfew human labeled examples from a subset of the categories\\nof interest. Our approach performs transfer learning both\\nacross domains and across tasks (see Figure 1). Intuitively,\\ndomain transfer is accomplished by making the marginal\\nfeature distributions of source and target as similar to each\\nother as possible. Task transfer is enabled by transferring\\nempirical category correlations learned on the source to the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='feature distributions of source and target as similar to each\\nother as possible. Task transfer is enabled by transferring\\nempirical category correlations learned on the source to the\\ntarget domain. This helps to preserve relationships between\\ncategories, e.g., bottle is similar to mug but different from\\nkeyboard. Previous work proposed techniques for domain\\ntransfer with CNN models [ 12, 24] but did not utilize the\\nlearned source semantic structure for task transfer.\\nTo enable domain transfer, we use the unlabeled target\\ndata to compute an estimated marginal distribution over the\\nnew environment and explicitly optimize a feature repre-\\n1\\narXiv:1510.02192v1  [cs.CV]  8 Oct 2015'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='sentation that minimizes the distance between the source\\nand target domain distributions. Dataset bias was classi-\\ncally illustrated in computer vision by the “name the dataset”\\ngame of Torralba and Efros [31], which trained a classiﬁer\\nto predict which dataset an image originates from, thereby\\nshowing that visual datasets are biased samples of the visual\\nworld. Indeed, this turns out to be formally connected to\\nmeasures of domain discrepancy [ 21, 5]. Optimizing for\\ndomain invariance, therefore, can be considered equivalent\\nto the task of learning to predict the class labels while simul-\\ntaneously ﬁnding a representation that makes the domains\\nappear as similar as possible. This principle forms the do-\\nmain transfer component of our proposed approach. We\\nlearn deep representations by optimizing over a loss which\\nincludes both classiﬁcation error on the labeled data as well\\nas a domain confusion loss which seeks to make the domains\\nindistinguishable.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='learn deep representations by optimizing over a loss which\\nincludes both classiﬁcation error on the labeled data as well\\nas a domain confusion loss which seeks to make the domains\\nindistinguishable.\\nHowever, while maximizing domain confusion pulls the\\nmarginal distributions of the domains together, it does not\\nnecessarily align the classes in the target with those in the\\nsource. Thus, we also explicitly transfer the similarity struc-\\nture amongst categories from the source to the target and\\nfurther optimize our representation to produce the same struc-\\nture in the target domain using the few target labeled exam-\\nples as reference points. We are inspired by prior work on\\ndistilling deep models [3, 16] and extend the ideas presented\\nin these works to a domain adaptation setting. We ﬁrst com-\\npute the average output probability distribution, or “soft\\nlabel,” over the source training examples in each category.\\nThen, for each target labeled example, we directly optimize'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='pute the average output probability distribution, or “soft\\nlabel,” over the source training examples in each category.\\nThen, for each target labeled example, we directly optimize\\nour model to match the distribution over classes to the soft\\nlabel. In this way we are able to perform task adaptation by\\ntransferring information to categories with no explicit labels\\nin the target domain.\\nWe solve the two problems jointly using a new CNN ar-\\nchitecture, outlined in Figure 2. We combine a domain con-\\nfusion and softmax cross-entropy losses to train the network\\nwith the target data. Our architecture can be used to solve su-\\npervised adaptation, when a small amount of target labeled\\ndata is available from each category, and semi-supervised\\nadaptation, when a small amount of target labeled data is\\navailable from a subset of the categories. We provide a com-\\nprehensive evaluation on the popular Ofﬁce benchmark [28]\\nand the recently introduced cross-dataset collection [30] for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='available from a subset of the categories. We provide a com-\\nprehensive evaluation on the popular Ofﬁce benchmark [28]\\nand the recently introduced cross-dataset collection [30] for\\nclassiﬁcation across visually distinct domains. We demon-\\nstrate that by jointly optimizing for domain confusion and\\nmatching soft labels, we are able to outperform the current\\nstate-of-the-art visual domain adaptation results.\\n2. Related work\\nThere have been many approaches proposed in recent\\nyears to solve the visual domain adaptation problem, which\\nis also commonly framed as the visual dataset bias prob-\\nlem [31]. All recognize that there is a shift in the distri-\\nbution of the source and target data representations. In\\nfact, the size of a domain shift is often measured by the\\ndistance between the source and target subspace representa-\\ntions [5, 11, 21, 25, 27]. A large number of methods have\\nsought to overcome this difference by learning a feature'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='distance between the source and target subspace representa-\\ntions [5, 11, 21, 25, 27]. A large number of methods have\\nsought to overcome this difference by learning a feature\\nspace transformation to align the source and target repre-\\nsentations [28, 23, 11, 15]. For the supervised adaptation\\nscenario, when a limited amount of labeled data is available\\nin the target domain, some approaches have been proposed\\nto learn a target classiﬁer regularized against the source clas-\\nsiﬁer [32, 2, 1]. Others have sought to both learn a feature\\ntransformation and regularize a target classiﬁer simultane-\\nously [18, 10].\\nRecently, supervised CNN based feature representations\\nhave been shown to be extremely effective for a variety of\\nvisual recognition tasks [22, 9, 14, 29]. In particular, using\\ndeep representations dramatically reduces the effect of reso-\\nlution and lighting on domain shifts [9, 19]. Parallel CNN\\narchitectures such as Siamese networks have been shown'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='deep representations dramatically reduces the effect of reso-\\nlution and lighting on domain shifts [9, 19]. Parallel CNN\\narchitectures such as Siamese networks have been shown\\nto be effective for learning invariant representations [6, 8].\\nHowever, training these networks requires labels for each\\ntraining instance, so it is unclear how to extend these meth-\\nods to unsupervised or semi-supervised settings. Multimodal\\ndeep learning architectures have also been explored to learn\\nrepresentations that are invariant to different input modal-\\nities [26]. However, this method operated primarily in a\\ngenerative context and therefore did not leverage the full\\nrepresentational power of supervised CNN representations.\\nTraining a joint source and target CNN architecture was\\nproposed by [7], but was limited to two layers and so was\\nsigniﬁcantly outperformed by the methods which used a\\ndeeper architecture [ 22], pre-trained on a large auxiliary'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='proposed by [7], but was limited to two layers and so was\\nsigniﬁcantly outperformed by the methods which used a\\ndeeper architecture [ 22], pre-trained on a large auxiliary\\ndata source (ex: ImageNet [4]). [13] proposed pre-training\\nwith a denoising auto-encoder, then training a two-layer net-\\nwork simultaneously with the MMD domain confusion loss.\\nThis effectively learns a domain invariant representation, but\\nagain, because the learned network is relatively shallow, it\\nlacks the strong semantic representation that is learned by di-\\nrectly optimizing a classiﬁcation objective with a supervised\\ndeep CNN.\\nUsing classiﬁer output distributions instead of category\\nlabels during training has been explored in the context of\\nmodel compression or distillation [3, 16]. However, we are\\nthe ﬁrst to apply this technique in a domain adaptation setting\\nin order to transfer class correlations between domains.\\nOther works have cotemporaneously explored the idea'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='the ﬁrst to apply this technique in a domain adaptation setting\\nin order to transfer class correlations between domains.\\nOther works have cotemporaneously explored the idea\\nof directly optimizing a representation for domain invari-\\nance [12, 24]. However, they either use weaker measures\\nof domain invariance or make use of optimization methods\\nthat are less robust than our proposed method, and they do\\nnot attempt to solve the task transfer problem in the semi-\\nsupervised setting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Source Data\\nbackpack chair\\n bike\\nTarget Databackpack\\n?\\nfc8conv1 conv5 fc6 fc7\\nSource softlabels\\nall target data\\nsource data\\nlabeled target data\\nfc8conv1 conv5\\nsource data\\nsoftmax \\nhigh temp\\nsoftlabel \\nloss\\nfcD\\nfc6 fc7\\nclassiﬁcation \\nloss\\ndomain \\nconfusion \\nloss\\ndomain \\nclassiﬁer \\nloss\\nshared\\nshared\\nshared\\nshared\\nshared\\nFigure 2. Our overall CNN architecture for domain and task transfer. We use a domain confusion loss over all source and target (both labeled\\nand unlabeled) data to learn a domain invariant representation. We simultaneously transfer the learned source semantic structure to the target\\ndomain by optimizing the network to produce activation distributions that match those learned for source data in the source only CNN. Best\\nviewed in color.\\n3. Joint CNN architecture for domain and task\\ntransfer\\nWe ﬁrst give an overview of our convolutional network\\n(CNN) architecture, depicted in Figure 2, that learns a rep-\\nresentation which both aligns visual domains and transfers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='transfer\\nWe ﬁrst give an overview of our convolutional network\\n(CNN) architecture, depicted in Figure 2, that learns a rep-\\nresentation which both aligns visual domains and transfers\\nthe semantic structure from a well labeled source domain to\\nthe sparsely labeled target domain. We assume access to a\\nlimited amount of labeled target data, potentially from only\\na subset of the categories of interest. With limited labels on\\na subset of the categories, the traditional domain transfer ap-\\nproach of ﬁne-tuning on the available target data [14, 29, 17]\\nis not effective. Instead, since the source labeled data shares\\nthe label space of our target domain, we use the source data\\nto guide training of the corresponding classiﬁers.\\nOur method takes as input the labeled source data\\n{xS,yS}(blue box Figure 2) and the target data {xT,yT}\\n(green box Figure 2), where the labels yT are only provided\\nfor a subset of the target examples. Our goal is to produce'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='{xS,yS}(blue box Figure 2) and the target data {xT,yT}\\n(green box Figure 2), where the labels yT are only provided\\nfor a subset of the target examples. Our goal is to produce\\na category classiﬁer θC that operates on an image feature\\nrepresentation f(x; θrepr) parameterized by representation\\nparameters θrepr and can correctly classify target examples\\nat test time.\\nFor a setting with Kcategories, let our desired classiﬁca-\\ntion objective be deﬁned as the standard softmax loss\\nLC(x,y; θrepr,θC) =−\\n∑\\nk\\n1 [y= k] logpk (1)\\nwhere p is the softmax of the classiﬁer activations,\\np= softmax(θT\\nCf(x; θrepr)).\\nWe could use the available source labeled data to train\\nour representation and classiﬁer parameters according to\\nEquation (1), but this often leads to overﬁtting to the source\\ndistribution, causing reduced performance at test time when\\nrecognizing in the target domain. However, we note that\\nif the source and target domains are very similar then the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='distribution, causing reduced performance at test time when\\nrecognizing in the target domain. However, we note that\\nif the source and target domains are very similar then the\\nclassiﬁer trained on the source will perform well on the\\ntarget. In fact, it is sufﬁcient for the source and target data to\\nbe similar under the learned representation, θrepr.\\nInspired by the “name the dataset” game of Torralba\\nand Efros [ 31], we can directly train a domain classiﬁer\\nθD to identify whether a training example originates from\\nthe source or target domain given its feature representation.\\nIntuitively, if our choice of representation suffers from do-\\nmain shift, then they will lie in distinct parts of the feature\\nspace, and a classiﬁer will be able to easily separate the\\ndomains. We use this notion to add a new domain confusion\\nloss Lconf(xS,xT,θD; θrepr) to our objective and directly op-\\ntimize our representation so as to minimize the discrepancy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='domains. We use this notion to add a new domain confusion\\nloss Lconf(xS,xT,θD; θrepr) to our objective and directly op-\\ntimize our representation so as to minimize the discrepancy\\nbetween the source and target distributions. This loss is\\ndescribed in more detail in Section 3.1.\\nDomain confusion can be applied to learn a representation\\nthat aligns source and target data without any target labeled\\ndata. However, we also presume a handful of sparse labels\\nin the target domain, yT. In this setting, a simple approach is\\nto incorporate the target labeled data along with the source\\nlabeled data into the classiﬁcation objective of Equation (1)1.\\nHowever, ﬁne-tuning with hard category labels limits the\\nimpact of a single training example, making it hard for the\\nnetwork to learn to generalize from the limited labeled data.\\nAdditionally, ﬁne-tuning with hard labels is ineffective when\\nlabeled data is available for only a subset of the categories.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='network to learn to generalize from the limited labeled data.\\nAdditionally, ﬁne-tuning with hard labels is ineffective when\\nlabeled data is available for only a subset of the categories.\\nFor our approach, we draw inspiration from recent net-\\nwork distillation works [ 3, 16], which demonstrate that a\\nlarge network can be “distilled” into a simpler model by re-\\nplacing the hard labels with the softmax activations from the\\noriginal large model. This modiﬁcation proves to be critical,\\nas the distribution holds key information about the relation-\\n1We present this approach as one of our baselines.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ships between categories and imposes additional structure\\nduring the training process. In essence, because each train-\\ning example is paired with an output distribution, it provides\\nvaluable information about not only the category it belongs\\nto, but also each other category the classiﬁer is trained to\\nrecognize.\\nThus, we propose using the labeled target data to op-\\ntimize the network parameters through a soft label loss,\\nLsoft(xT,yT; θrepr,θC). This loss will train the network pa-\\nrameters to produce a “soft label” activation that matches\\nthe average output distribution of source examples on a net-\\nwork trained to classify source data. This loss is described in\\nmore detail in Section 3.2. By training the network to match\\nthe expected source output distributions on target data, we\\ntransfer the learned inter-class correlations from the source\\ndomain to examples in the target domain. This directly trans-\\nfers useful information from source to target, such as the fact'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='transfer the learned inter-class correlations from the source\\ndomain to examples in the target domain. This directly trans-\\nfers useful information from source to target, such as the fact\\nthat bookshelves appear more similar to ﬁling cabinets than\\nto bicycles.\\nOur full method then minimizes the joint loss function\\nL(xS,yS,xT,yT,θD;θrepr,θC) =\\nLC(xS,yS,xT,yT; θrepr,θC)\\n+ λLconf(xS,xT,θD; θrepr)\\n+ νLsoft(xT,yT; θrepr,θC).\\n(2)\\nwhere the hyperparameters λand νdetermine how strongly\\ndomain confusion and soft labels inﬂuence the optimization.\\nOur ideas of domain confusion and soft label loss for task\\ntransfer are generic and can be applied to any CNN classiﬁ-\\ncation architecture. For our experiments and for the detailed\\ndiscussion in this paper we modify the standard Krizhevsky\\narchitecture [22], which has ﬁve convolutional layers (conv1–\\nconv5) and three fully connected layers (fc6–fc8). The rep-\\nresentation parameter θrepr corresponds to layers 1–7 of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='architecture [22], which has ﬁve convolutional layers (conv1–\\nconv5) and three fully connected layers (fc6–fc8). The rep-\\nresentation parameter θrepr corresponds to layers 1–7 of the\\nnetwork, and the classiﬁcation parameter θC corresponds to\\nlayer 8. For the remainder of this section, we provide further\\ndetails on our novel loss deﬁnitions and the implementation\\nof our model.\\n3.1. Aligning domains via domain confusion\\nIn this section we describe in detail our proposed domain\\nconfusion loss objective. Recall that we introduce the domain\\nconfusion loss as a means to learn a representation that is\\ndomain invariant, and thus will allow us to better utilize a\\nclassiﬁer trained using the labeled source data. We consider\\na representation to be domain invariant if a classiﬁer trained\\nusing that representation can not distinguish examples from\\nthe two domains.\\nTo this end, we add an additional domain classiﬁcation\\nlayer, denoted as fcD in Figure 2, with parameters θD. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='using that representation can not distinguish examples from\\nthe two domains.\\nTo this end, we add an additional domain classiﬁcation\\nlayer, denoted as fcD in Figure 2, with parameters θD. This\\nlayer simply performs binary classiﬁcation using the domain\\ncorresponding to an image as its label. For a particular fea-\\nture representation, θrepr, we evaluate its domain invariance\\nby learning the best domain classiﬁer on the representation.\\nThis can be learned by optimizing the following objective,\\nwhere yD denotes the domain that the example is drawn\\nfrom:\\nLD(xS,xT,θrepr; θD) =−\\n∑\\nd\\n1 [yD = d] logqd (3)\\nwith qcorresponding to the softmax of the domain classiﬁer\\nactivation: q= softmax(θT\\nDf(x; θrepr)).\\nFor a particular domain classiﬁer, θD, we can now in-\\ntroduce our loss which seeks to “maximally confuse” the\\ntwo domains by computing the cross entropy between the\\noutput predicted domain labels and a uniform distribution\\nover domain labels:\\nLconf(xS,xT,θD; θrepr) =−\\n∑\\nd\\n1\\nDlog qd. (4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='two domains by computing the cross entropy between the\\noutput predicted domain labels and a uniform distribution\\nover domain labels:\\nLconf(xS,xT,θD; θrepr) =−\\n∑\\nd\\n1\\nDlog qd. (4)\\nThis domain confusion loss seeks to learn domain invari-\\nance by ﬁnding a representation in which the best domain\\nclassiﬁer performs poorly.\\nIdeally, we want to simultaneously minimize Equa-\\ntions (3) and (4) for the representation and the domain clas-\\nsiﬁer parameters. However, the two losses stand in direct\\nopposition to one another: learning a fully domain invariant\\nrepresentation means the domain classiﬁer must do poorly,\\nand learning an effective domain classiﬁer means that the\\nrepresentation is not domain invariant. Rather than globally\\noptimizing θD and θrepr, we instead perform iterative updates\\nfor the following two objectives given the ﬁxed parameters\\nfrom the previous iteration:\\nmin\\nθD\\nLD(xS,xT,θrepr; θD) (5)\\nmin\\nθrepr\\nLconf(xS,xT,θD; θrepr). (6)\\nThese losses are readily implemented in standard deep'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='from the previous iteration:\\nmin\\nθD\\nLD(xS,xT,θrepr; θD) (5)\\nmin\\nθrepr\\nLconf(xS,xT,θD; θrepr). (6)\\nThese losses are readily implemented in standard deep\\nlearning frameworks, and after setting learning rates properly\\nso that Equation (5) only updates θD and Equation (6) only\\nupdates θrepr, the updates can be performed via standard\\nbackpropagation. Together, these updates ensure that we\\nlearn a representation that is domain invariant.\\n3.2. Aligning source and target classes via soft labels\\nWhile training the network to confuse the domains acts\\nto align their marginal distributions, there are no guarantees\\nabout the alignment of classes between each domain. To\\nensure that the relationships between classes are preserved\\nacross source and target, we ﬁne-tune the network against\\n“soft labels” rather than the image category hard label.\\nWe deﬁne a soft label for category kas the average over\\nthe softmax of all activations of source examples in category'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='“soft labels” rather than the image category hard label.\\nWe deﬁne a soft label for category kas the average over\\nthe softmax of all activations of source examples in category\\nk, depicted graphically in Figure 3, and denote this aver-\\nage as l(k). Note that, since the source network was trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Source \\nCNN\\nSource \\nCNN\\nSource \\nCNN\\nBottleMugChairLaptopKeyboard\\nBottle Mug ChairLaptopKeyboard\\nBottle Mug ChairLaptopKeyboard\\nBottle Mug ChairLaptopKeyboard\\n+\\nsoftmax \\nhigh \\ntemp\\nsoftmax \\nhigh \\ntemp\\nsoftmax \\nhigh \\ntemp\\nFigure 3. Soft label distributions are learned by averaging the per-\\ncategory activations of source training examples using the source\\nmodel. An example, with 5 categories, depicted here to demonstrate\\nthe ﬁnal soft activation for the bottle category will be primarily\\ndominated by bottle and mug with very little mass on chair, laptop,\\nand keyboard.\\nBottleMug ChairLaptopKeyboard\\nBottleMug ChairLaptopKeyboard\\nAdapt CNN\\n“Bottle”\\nSource Activations \\nPer Class\\nbackprop\\nCross Entropy Loss\\nsoftmax \\nhigh \\ntemp\\nFigure 4. Depiction of the use of source per-category soft activa-\\ntions with the cross entropy loss function over the current target\\nactivations.\\npurely to optimize a classiﬁcation objective, a simple soft-\\nmax over each zi\\nS will hide much of the useful information'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='activations.\\npurely to optimize a classiﬁcation objective, a simple soft-\\nmax over each zi\\nS will hide much of the useful information\\nby producing a very peaked distribution. Instead, we use a\\nsoftmax with a high temperature τ so that the related classes\\nhave enough probability mass to have an effect during ﬁne-\\ntuning. With our computed per-category soft labels we can\\nnow deﬁne our soft label loss:\\nLsoft(xT,yT; θrepr,θC) =−\\n∑\\ni\\nl(yT )\\ni log pi (7)\\nwhere p denotes the soft activation of the target image,\\np = softmax(θT\\nCf(xT; θrepr)/τ). The loss above corre-\\nsponds to the cross-entropy loss between the soft activation\\nof a particular target image and the soft label corresponding\\nto the category of that image, as shown in Figure 4.\\nTo see why this will help, consider the soft label for a\\nparticular category, such as bottle. The soft label l(bottle) is\\na K-dimensional vector, where each dimension indicates\\nthe similarity of bottles to each of the Kcategories. In this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='particular category, such as bottle. The soft label l(bottle) is\\na K-dimensional vector, where each dimension indicates\\nthe similarity of bottles to each of the Kcategories. In this\\nexample, the bottle soft label will have a higher weight on\\nmug than on keyboard, since bottles and mugs are more\\nvisually similar. Thus, soft label training with this particular\\nsoft label directly enforces the relationship that bottles and\\nmugs should be closer in feature space than bottles and\\nkeyboards.\\nOne important beneﬁt of using this soft label loss is that\\nwe ensure that the parameters for categories without any\\nlabeled target data are still updated to output non-zero proba-\\nbilities. We explore this beneﬁt in Section 4, where we train\\na network using labels from a subset of the target categories\\nand ﬁnd signiﬁcant performance improvement even when\\nevaluating only on the unlabeled categories.\\n4. Evaluation\\nTo analyze the effectiveness of our method, we evaluate it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='and ﬁnd signiﬁcant performance improvement even when\\nevaluating only on the unlabeled categories.\\n4. Evaluation\\nTo analyze the effectiveness of our method, we evaluate it\\non the Ofﬁce dataset, a standard benchmark dataset for visual\\ndomain adaptation, and on a new large-scale cross-dataset\\ndomain adaptation challenge.\\n4.1. Adaptation on the Ofﬁce dataset\\nThe Ofﬁce dataset is a collection of images from three\\ndistinct domains, Amazon, DSLR, and Webcam, the largest\\nof which has 2817 labeled images [28]. The 31 categories\\nin the dataset consist of objects commonly encountered in\\nofﬁce settings, such as keyboards, ﬁle cabinets, and laptops.\\nWe evaluate our method in two different settings:\\n•Supervised adaptation Labeled training data for all\\ncategories is available in source and sparsely in target.\\n•Semi-supervised adaptation (task adaptation) La-\\nbeled training data is available in source and sparsely\\nfor a subset of the target categories.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='•Semi-supervised adaptation (task adaptation) La-\\nbeled training data is available in source and sparsely\\nfor a subset of the target categories.\\nFor all experiments we initialize the parameters of conv1–\\nfc7 using the released CaffeNet [20] weights. We then fur-\\nther ﬁne-tune the network using the source labeled data in or-\\nder to produce the soft label distributions and use the learned\\nsource CNN weights as the initial parameters for training\\nour method. All implementations are produced using the\\nopen source Caffe [20] framework, and the network deﬁni-\\ntion ﬁles and cross entropy loss layer needed for training\\nwill be released upon acceptance. We optimize the network\\nusing a learning rate of 0.001 and set the hyper-parameters\\nto λ= 0.01 (confusion) and ν = 0.1 (soft).\\nFor each of the six domain shifts, we evaluate across ﬁve\\ntrain/test splits, which are generated by sampling examples\\nfrom the full set of images per domain. In the source domain,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='For each of the six domain shifts, we evaluate across ﬁve\\ntrain/test splits, which are generated by sampling examples\\nfrom the full set of images per domain. In the source domain,\\nwe follow the standard protocol for this dataset and generate\\nsplits by sampling 20 examples per category for the Amazon\\ndomain, and 8 examples per category for the DSLR and\\nWebcam domains.\\nWe ﬁrst present results for the supervised setting, where\\n3 labeled examples are provided for each category in the\\ntarget domain. We report accuracies on the remaining un-\\nlabeled images, following the standard protocol introduced\\nwith the dataset [28]. In addition to a variety of baselines, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='A→W A →D W →A W →D D →A D →W Average\\nDLID [7] 51.9 – – 89.9 – 78.2 –\\nDeCAF6 S+T [9] 80.7 ±2.3 – – – – 94.8 ±1.2 –\\nDaNN [13] 53.6 ±0.2 – – 83.5 ±0.0 – 71.2 ±0.0 –\\nSource CNN 56.5 ±0.3 64.6 ±0.4 42.7 ±0.1 93.6 ±0.2 47.6 ±0.1 92.4 ±0.3 66.22\\nTarget CNN 80.5 ±0.5 81.8 ±1.0 59.9 ±0.3 81.8 ±1.0 59.9 ±0.3 80.5 ±0.5 74.05\\nSource+Target CNN 82.5 ±0.9 85.2 ±1.1 65.2 ±0.7 96.3 ±0.5 65.8 ±0.5 93.9 ±0.5 81.50\\nOurs: dom confusion only 82.8 ±0.9 85.9 ±1.1 64.9 ±0.5 97.5 ±0.2 66.2 ±0.4 95.6 ±0.4 82.13\\nOurs: soft labels only 82.7 ±0.7 84.9 ±1.2 65.2 ±0.6 98.3 ±0.3 66.0 ±0.5 95.9 ±0.6 82.17\\nOurs: dom confusion+soft labels 82.7 ±0.8 86.1 ±1.2 65.0 ±0.5 97.6 ±0.2 66.2 ±0.3 95.7 ±0.5 82.22\\nTable 1. Multi-class accuracy evaluation on the standard supervised adaptation setting with theOfﬁce dataset. We evaluate on all 31 categories\\nusing the standard experimental protocol from [28]. Here, we compare against three state-of-the-art domain adaptation methods as well as a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='using the standard experimental protocol from [28]. Here, we compare against three state-of-the-art domain adaptation methods as well as a\\nCNN trained using only source data, only target data, or both source and target data together.\\nA→W A →D W →A W →D D →A D →W Average\\nMMDT [18] – 44.6 ±0.3 – 58.3 ±0.5 – – –\\nSource CNN 54.2 ±0.6 63.2 ±0.4 34.7 ±0.1 94.5 ±0.2 36.4 ±0.1 89.3 ±0.5 62.0\\nOurs: dom confusion only 55.2 ±0.6 63.7 ±0.9 41.1 ±0.0 96.5 ±0.1 41.2 ±0.1 91.3 ±0.4 64.8\\nOurs: soft labels only 56.8 ±0.4 65.2 ±0.9 38.8 ±0.4 96.5 ±0.2 41.7 ±0.3 89.6 ±0.1 64.8\\nOurs: dom confusion+soft labels 59.3 ±0.6 68.0 ±0.5 40.5 ±0.2 97.5 ±0.1 43.1 ±0.2 90.0 ±0.2 66.4\\nTable 2. Multi-class accuracy evaluation on the standard semi-supervised adaptation setting with the Ofﬁce dataset. We evaluate on 16\\nheld-out categories for which we have no access to target labeled data. We show results on these unsupervised categories for the source only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='held-out categories for which we have no access to target labeled data. We show results on these unsupervised categories for the source only\\nmodel, our model trained using only soft labels for the 15 auxiliary categories, and ﬁnally using domain confusion together with soft labels\\non the 15 auxiliary categories.\\nreport numbers for both soft label ﬁne-tuning alone as well\\nas soft labels with domain confusion in Table 1. Because the\\nOfﬁce dataset is imbalanced, we report multi-class accura-\\ncies, which are obtained by computing per-class accuracies\\nindependently, then averaging over all 31 categories.\\nWe see that ﬁne-tuning with soft labels or domain con-\\nfusion provides a consistent improvement over hard label\\ntraining in 5 of 6 shifts. Combining soft labels with do-\\nmain confusion produces marginally higher performance on\\naverage. This result follows the intuitive notion that when\\nenough target labeled examples are present, directly opti-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='main confusion produces marginally higher performance on\\naverage. This result follows the intuitive notion that when\\nenough target labeled examples are present, directly opti-\\nmizing for the joint source and target classiﬁcation objective\\n(Source+Target CNN) is a strong baseline and so using ei-\\nther of our new losses adds enough regularization to improve\\nperformance.\\nNext, we experiment with the semi-supervised adaptation\\nsetting. We consider the case in which training data and\\nlabels are available for some, but not all of the categories in\\nthe target domain. We are interested in seeing whether we\\ncan transfer information learned from the labeled classes to\\nthe unlabeled classes.\\nTo do this, we consider having 10 target labeled exam-\\nples per category from only 15 of the 31 total categories,\\nfollowing the standard protocol introduced with the Ofﬁce\\ndataset [28]. We then evaluate our classiﬁcation performance\\non the remaining 16 categories for which no data was avail-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='following the standard protocol introduced with the Ofﬁce\\ndataset [28]. We then evaluate our classiﬁcation performance\\non the remaining 16 categories for which no data was avail-\\nable at training time.\\nIn Table 2 we present multi-class accuracies over the 16\\nheld-out categories and compare our method to a previous\\ndomain adaptation method [ 18] as well as a source-only\\ntrained CNN. Note that, since the performance here is com-\\nputed over only a subset of the categories in the dataset, the\\nnumbers in this table should not be directly compared to the\\nsupervised setting in Table 1.\\nWe ﬁnd that all variations of our method (only soft label\\nloss, only domain confusion, and both together) outperform\\nthe baselines. Contrary to the fully supervised case, here we\\nnote that both domain confusion and soft labels contribute\\nsigniﬁcantly to the overall performance improvement of our\\nmethod. This stems from the fact that we are now evaluat-\\ning on categories which lack labeled target data, and thus'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='signiﬁcantly to the overall performance improvement of our\\nmethod. This stems from the fact that we are now evaluat-\\ning on categories which lack labeled target data, and thus\\nthe network can not implicitly enforce domain invariance\\nthrough the classiﬁcation objective alone. Separately, the\\nfact that we get improvement from the soft label training on\\nrelated tasks indicates that information is being effectively\\ntransferred between tasks.\\nIn Figure 5, we show examples for the\\nAmazon→Webcam shift where our method correctly\\nclassiﬁes images from held out object categories and the\\nbaseline does not. We ﬁnd that our method is able to\\nconsistently overcome error cases, such as the notebooks\\nthat were previously confused with letter trays, or the black'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ring binder\\nmonitor\\nlaptop computer\\nmonitor\\nspeaker\\nmonitor\\nscissors\\nmug\\nmouse\\nmug\\nmouse\\nmug\\nlaptop computer\\npaper notebook\\nletter tray\\npaper notebook\\nletter tray\\npaper notebook\\nletter tray\\npaper notebook\\nletter tray\\npaper notebook\\nlaptop computer\\npaper notebook\\ncalculator\\nphone\\ncalculator\\nphone\\nfile cabinet\\nprinter\\nfile cabinet\\nprinter\\nfile cabinet\\nprinter\\nlaptop computer\\nprojector\\nlaptop computer\\nprojector\\nfile cabinet\\nprojector\\nphone\\nprojector\\nkeyboard\\nprojector\\ntape dispenser\\npunchers\\nlaptop computer\\nring binder\\nkeyboard\\nring binder\\nkeyboard\\nring binder\\nletter tray\\nring binder\\nlaptop computer\\nring binder\\nFigure 5. Examples from the Amazon →Webcam shift in the\\nsemi-supervised adaptation setting, where our method (the bot-\\ntom turquoise label) correctly classiﬁes images while the baseline\\n(the top purple label) does not.\\nmugs that were confused with black computer mice.\\n4.2. Adaptation between diverse domains\\nFor an evaluation with larger, more distinct domains, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='(the top purple label) does not.\\nmugs that were confused with black computer mice.\\n4.2. Adaptation between diverse domains\\nFor an evaluation with larger, more distinct domains, we\\ntest on the recent testbed for cross-dataset analysis [ 30],\\nwhich collects images from classes shared in common among\\ncomputer vision datasets. We use the dense version of this\\ntestbed, which consists of 40 categories shared between\\nthe ImageNet, Caltech-256, SUN, and Bing datasets, and\\nevaluate speciﬁcally with ImageNet as source and Caltech-\\n256 as target.\\nWe follow the protocol outlined in [30] and generate 5\\nsplits by selecting 5534 images from ImageNet and 4366\\nimages from Caltech-256 across the 40 shared categories.\\nEach split is then equally divided into a train and test set.\\nHowever, since we are most interested in evaluating in the\\nsetting with limited target data, we further subsample the\\ntarget training set into smaller sets with only 1, 3, and 5\\nlabeled examples per category.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='setting with limited target data, we further subsample the\\ntarget training set into smaller sets with only 1, 3, and 5\\nlabeled examples per category.\\nResults from this evaluation are shown in Figure 6. We\\ncompare our method to both CNNs ﬁne-tuned using only\\nsource data using source and target labeled data. Contrary to\\nthe previous supervised adaptation experiment, our method\\nsigniﬁcantly outperforms both baselines. We see that our\\nfull architecture, combining domain confusion with the soft\\nlabel loss, performs the best overall and is able to operate\\nin the regime of no labeled examples in the target (corre-\\nsponding to the red line at point 0 on the x-axis). We ﬁnd\\nthat the most beneﬁt of our method arises when there are\\nfew labeled training examples per category in the target do-\\nmain. As we increase the number of labeled examples in\\nthe target, the standard ﬁne-tuning strategy begins to ap-\\nproach the performance of the adaptation approach. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='main. As we increase the number of labeled examples in\\nthe target, the standard ﬁne-tuning strategy begins to ap-\\nproach the performance of the adaptation approach. This\\nNumber Labeled Target Examples per Category\\n0 1 3 5\\nMulti-class Accuracy\\n72\\n73\\n74\\n75\\n76\\n77\\n78\\nSource CNN\\nSource+Target CNN\\nOurs: softlabels only\\nOurs: dom confusion+softlabels\\nFigure 6. ImageNet→Caltech supervised adaptation from the Cross-\\ndataset [30] testbed with varying numbers of labeled target exam-\\nples per category. We ﬁnd that our method using soft label loss\\n(with and without domain confusion) outperforms the baselines\\nof training on source data alone or using a standard ﬁne-tuning\\nstrategy to train with the source and target data. Best viewed in\\ncolor.\\nindicates that direct joint source and target ﬁne-tuning is\\na viable adaptation approach when you have a reasonable\\nnumber of training examples per category. In comparison,\\nﬁne-tuning on the target examples alone yields accuracies'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='a viable adaptation approach when you have a reasonable\\nnumber of training examples per category. In comparison,\\nﬁne-tuning on the target examples alone yields accuracies\\nof 36.6 ±0.6, 60.9 ±0.5, and 67.7 ±0.5 for the cases of 1,\\n3, and 5 labeled examples per category, respectively. All of\\nthese numbers underperform the source only model, indicat-\\ning that adaptation is crucial in the setting of limited training\\ndata.\\nFinally, we note that our results are signiﬁcantly higher\\nthan the 24.8% result reported in [ 30], despite the use of\\nmuch less training data. This difference is explained by their\\nuse of SURF BoW features, indicating that CNN features\\nare a much stronger feature for use in adaptation tasks.\\n5. Analysis\\nOur experimental results demonstrate that our method\\nimproves classiﬁcation performance in a variety of domain\\nadaptation settings. We now perform additional analysis on\\nour method by conﬁrming our claims that it exhibits domain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='improves classiﬁcation performance in a variety of domain\\nadaptation settings. We now perform additional analysis on\\nour method by conﬁrming our claims that it exhibits domain\\ninvariance and transfers information across tasks.\\n5.1. Domain confusion enforces domain invariance\\nWe begin by evaluating the effectiveness of domain con-\\nfusion at learning a domain invariant representation. As\\npreviously explained, we consider a representation to be\\ndomain invariant if an optimal classiﬁer has difﬁculty pre-\\ndicting which domain an image originates from. Thus, for\\nour representation learned with a domain confusion loss, we\\nexpect a trained domain classiﬁer to perform poorly.\\nWe train two support vector machines (SVMs) to clas-\\nsify images into domains: one using the baseline CaffeNet\\nfc7 representation, and the other using our fc7 learned with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Figure 7. We compare the baseline CaffeNet representation to\\nour representation learned with domain confusion by training a\\nsupport vector machine to predict the domains of Amazon and\\nWebcam images. For each representation, we plot a histogram of\\nthe classiﬁer decision scores of the test images. In the baseline\\nrepresentation, the classiﬁer is able to separate the two domains\\nwith 99% accuracy. In contrast, the representation learned with\\ndomain confusion is domain invariant, and the classiﬁer can do no\\nbetter than 56%.\\ndomain confusion. These SVMs are trained using 160 im-\\nages, 80 from Amazon and 80 from Webcam, then tested\\non the remaining images from those domains. We plot the\\nclassiﬁer scores for each test image in Figure 7. It is obvious\\nthat the domain confusion representation is domain invariant,\\nmaking it much harder to separate the two domains—the\\ntest accuracy on the domain confusion representation is only\\n56%, not much better than random. In contrast, on the base-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='making it much harder to separate the two domains—the\\ntest accuracy on the domain confusion representation is only\\n56%, not much better than random. In contrast, on the base-\\nline CaffeNet representation, the domain classiﬁer achieves\\n99% test accuracy.\\n5.2. Soft labels for task transfer\\nWe now examine the effect of soft labels in transfer-\\nring information between categories. We consider the\\nAmazon→Webcam shift from the semi-supervised adapta-\\ntion experiment in the previous section. Recall that in this\\nsetting, we have access to target labeled data for only half\\nof our categories. We use soft label information from the\\nsource domain to provide information about the held-out\\ncategories which lack labeled target examples. Figure 8\\nexamines one target example from the held-out category\\nmonitor. No labeled target monitors were available during\\ntraining; however, as shown in the upper right corner of Fig-\\nure 8, the soft labels for laptop computer was present during'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='monitor. No labeled target monitors were available during\\ntraining; however, as shown in the upper right corner of Fig-\\nure 8, the soft labels for laptop computer was present during\\ntraining and assigns a relatively high weight to the monitor\\nclass. Soft label ﬁne-tuning thus allows us to exploit the fact\\nthat these categories are similar. We see that the baseline\\nmodel misclassiﬁes this image as a ring binder, while our\\nsoft label model correctly assigns the monitor label.\\n6. Conclusion\\nWe have presented a CNN architecture that effectively\\nadapts to a new domain with limited or no labeled data per\\ntarget category. We accomplish this through a novel CNN\\narchitecture which simultaneously optimizes for domain in-\\nback pack\\nbike\\nbike helmetbookcasebottlecalculatordesk chairdesk lamp\\ndesktop computer\\nfile cabinetheadphoneskeyboard\\nlaptop computer\\nletter traymobile phone\\nmonitormousemug\\npaper notebook\\npenphoneprinterprojectorpunchersring binder\\nrulerscissorsspeakerstapler\\ntape dispenser'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='file cabinetheadphoneskeyboard\\nlaptop computer\\nletter traymobile phone\\nmonitormousemug\\npaper notebook\\npenphoneprinterprojectorpunchersring binder\\nrulerscissorsspeakerstapler\\ntape dispenser\\ntrash can\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\n0.06\\n0.07\\n0.08\\n0.09\\n0.1 Ours soft label\\nback pack\\nbike\\nbike helmetbookcasebottlecalculatordesk chairdesk lamp\\ndesktop computer\\nfile cabinetheadphoneskeyboard\\nlaptop computer\\nletter traymobile phone\\nmonitormousemug\\npaper notebook\\npenphoneprinterprojectorpunchersring binder\\nrulerscissorsspeakerstapler\\ntape dispenser\\ntrash can\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\n0.06\\n0.07\\n0.08\\n0.09\\n0.1 Baseline soft label\\nback pack bike bike helmet\\nbookcase bottle calculator\\ndesk chair desk lamp desktop computer\\nfile cabinet headphones keyboard\\nlaptop computer letter tray mobile phone\\nring binder\\nmonitor\\nBaseline soft activation Our soft activation\\nSource soft labelsTarget test image\\nFigure 8. Our method (bottom turquoise label) correctly predicts'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ring binder\\nmonitor\\nBaseline soft activation Our soft activation\\nSource soft labelsTarget test image\\nFigure 8. Our method (bottom turquoise label) correctly predicts\\nthe category of this image, whereas the baseline (top purple label)\\ndoes not. The source per-category soft labels for the 15 categories\\nwith labeled target data are shown in the upper right corner, where\\nthe x-axis of the plot represents the 31 categories and the y-axis is\\nthe output probability. We highlight the index corresponding to the\\nmonitor category in red. As no labeled target data is available for\\nthe correct category,monitor, we ﬁnd that in our method the related\\ncategory of laptop computer (outlined with yellow box) transfers\\ninformation to the monitor category. As a result, after training, our\\nmethod places the highest weight on the correct category. Probabil-\\nity score per category for the baseline and our method are shown\\nin the bottom left and right, respectively, training categories are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ity score per category for the baseline and our method are shown\\nin the bottom left and right, respectively, training categories are\\nopaque and correct test category is shown in red.\\nvariance, to facilitate domain transfer, while transferring\\ntask information between domains in the form of a cross\\nentropy soft label loss. We demonstrate the ability of our\\narchitecture to improve adaptation performance in the super-\\nvised and semi-supervised settings by experimenting with\\ntwo standard domain adaptation benchmark datasets. In the\\nsemi-supervised adaptation setting, we see an average rela-\\ntive improvement of 13% over the baselines on the four most\\nchallenging shifts in the Ofﬁce dataset. Overall, our method\\ncan be easily implemented as an alternative ﬁne-tuning strat-\\negy when limited or no labeled data is available per category\\nin the target domain.\\nAcknowledgements This work was supported by DARPA;\\nAFRL; DoD MURI award N000141110688; NSF awards'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='egy when limited or no labeled data is available per category\\nin the target domain.\\nAcknowledgements This work was supported by DARPA;\\nAFRL; DoD MURI award N000141110688; NSF awards\\n113629, IIS-1427425, and IIS-1212798; and the Berkeley\\nVision and Learning Center.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] L. T. Alessandro Bergamo. Exploiting weakly-labeled web\\nimages to improve object classiﬁcation: a domain adaptation\\napproach. In Neural Information Processing Systems (NIPS),\\nDec. 2010. 2\\n[2] Y . Aytar and A. Zisserman. Tabula rasa: Model transfer for\\nobject category detection. In Proc. ICCV, 2011. 2\\n[3] J. Ba and R. Caruana. Do deep nets really need to be deep?\\nIn Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and\\nK. Weinberger, editors,Advances in Neural Information Pro-\\ncessing Systems 27 , pages 2654–2662. Curran Associates,\\nInc., 2014. 2, 3\\n[4] A. Berg, J. Deng, and L. Fei-Fei. ImageNet Large Scale\\nVisual Recognition Challenge 2012. 2012. 2\\n[5] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel,\\nB. Sch¨olkopf, and A. J. Smola. Integrating structured biologi-\\ncal data by kernel maximum mean discrepancy. In Bioinfor-\\nmatics, 2006. 2\\n[6] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y . LeCun,\\nC. Moore, E. S¨ackinger, and R. Shah. Signature veriﬁcation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='cal data by kernel maximum mean discrepancy. In Bioinfor-\\nmatics, 2006. 2\\n[6] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y . LeCun,\\nC. Moore, E. S¨ackinger, and R. Shah. Signature veriﬁcation\\nusing a siamese time delay neural network. International\\nJournal of Pattern Recognition and Artiﬁcial Intelligence ,\\n7(04):669–688, 1993. 2\\n[7] S. Chopra, S. Balakrishnan, and R. Gopalan. DLID: Deep\\nlearning for domain adaptation by interpolating between do-\\nmains. In ICML Workshop on Challenges in Representation\\nLearning, 2013. 2, 6\\n[8] S. Chopra, R. Hadsell, and Y . LeCun. Learning a similar-\\nity metric discriminatively, with application to face veriﬁ-\\ncation. In Computer Vision and Pattern Recognition, 2005.\\nCVPR 2005. IEEE Computer Society Conference on , vol-\\nume 1, pages 539–546. IEEE, 2005. 2\\n[9] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. In Proc.\\nICML, 2014. 2, 6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='[9] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. In Proc.\\nICML, 2014. 2, 6\\n[10] L. Duan, D. Xu, and I. W. Tsang. Learning with augmented\\nfeatures for heterogeneous domain adaptation. In Proc. ICML,\\n2012. 2\\n[11] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsu-\\npervised visual domain adaptation using subspace alignment.\\nIn Proc. ICCV, 2013. 2\\n[12] Y . Ganin and V . Lempitsky. Unsupervised Domain Adap-\\ntation by Backpropagation. ArXiv e-prints, Sept. 2014. 1,\\n2\\n[13] M. Ghifary, W. B. Kleijn, and M. Zhang. Domain adaptive\\nneural networks for object recognition. CoRR, abs/1409.6041,\\n2014. 2, 6\\n[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\\nfeature hierarchies for accurate object detection and semantic\\nsegmentation. arXiv e-prints, 2013. 1, 2, 3\\n[15] B. Gong, Y . Shi, F. Sha, and K. Grauman. Geodesic ﬂow'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='feature hierarchies for accurate object detection and semantic\\nsegmentation. arXiv e-prints, 2013. 1, 2, 3\\n[15] B. Gong, Y . Shi, F. Sha, and K. Grauman. Geodesic ﬂow\\nkernel for unsupervised domain adaptation. In Proc. CVPR,\\n2012. 2\\n[16] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\\nin a neural network. In NIPS Deep Learning and Representa-\\ntion Learning Workshop, 2014. 2, 3\\n[17] J. Hoffman, S. Guadarrama, E. Tzeng, R. Hu, J. Donahue,\\nR. Girshick, T. Darrell, and K. Saenko. LSDA: Large scale de-\\ntection through adaptation. In Neural Information Processing\\nSystems (NIPS), 2014. 3\\n[18] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Darrell.\\nEfﬁcient learning of domain-invariant image representations.\\nIn Proc. ICLR, 2013. 2, 6\\n[19] J. Hoffman, E. Tzeng, J. Donahue, , Y . Jia, K. Saenko, and\\nT. Darrell. One-shot learning of supervised deep convolutional\\nmodels. In arXiv 1312.6204; presented at ICLR Workshop,\\n2014. 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='[19] J. Hoffman, E. Tzeng, J. Donahue, , Y . Jia, K. Saenko, and\\nT. Darrell. One-shot learning of supervised deep convolutional\\nmodels. In arXiv 1312.6204; presented at ICLR Workshop,\\n2014. 2\\n[20] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-\\ntional architecture for fast feature embedding. arXiv preprint\\narXiv:1408.5093, 2014. 5\\n[21] D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in\\ndata streams. In Proc. VLDB, 2004. 2\\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\\nclassiﬁcation with deep convolutional neural networks. In\\nProc. NIPS, 2012. 2, 4\\n[23] B. Kulis, K. Saenko, and T. Darrell. What you saw is not\\nwhat you get: Domain adaptation using asymmetric kernel\\ntransforms. In Proc. CVPR, 2011. 2\\n[24] M. Long and J. Wang. Learning transferable features with\\ndeep adaptation networks. CoRR, abs/1502.02791, 2015. 1, 2\\n[25] Y . Mansour, M. Mohri, and A. Rostamizadeh. Domain adap-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='[24] M. Long and J. Wang. Learning transferable features with\\ndeep adaptation networks. CoRR, abs/1502.02791, 2015. 1, 2\\n[25] Y . Mansour, M. Mohri, and A. Rostamizadeh. Domain adap-\\ntation: Learning bounds and algorithms. In COLT, 2009.\\n2\\n[26] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y .\\nNg. Multimodal deep learning. In Proceedings of the 28th\\nInternational Conference on Machine Learning (ICML-11),\\npages 689–696, 2011. 2\\n[27] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain\\nadaptation via transfer component analysis. In IJCA, 2009. 2\\n[28] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual\\ncategory models to new domains. In Proc. ECCV, 2010. 2, 5,\\n6\\n[29] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks. CoRR,\\nabs/1312.6229, 2013. 1, 2, 3\\n[30] T. Tommasi, T. Tuytelaars, and B. Caputo. A testbed for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='tion and detection using convolutional networks. CoRR,\\nabs/1312.6229, 2013. 1, 2, 3\\n[30] T. Tommasi, T. Tuytelaars, and B. Caputo. A testbed for\\ncross-dataset analysis. In TASK-CV Workshop, ECCV, 2014.\\n2, 7\\n[31] A. Torralba and A. Efros. Unbiased look at dataset bias. In\\nProc. CVPR, 2011. 2, 3\\n[32] J. Yang, R. Yan, and A. Hauptmann. Adapting SVM classi-\\nﬁers to data with shifted distributions. In ICDM Workshops,\\n2007. 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='On the Beneﬁt of Combining Neural, Statistical\\nand External Features for Fake News\\nIdentiﬁcation\\nGaurav Bhatt1, Aman Sharma1, Shivam Sharma1, Ankush Nagpal1,\\nBalasubramanian Raman1, and Ankush Mittal 2\\n1 Indian Institute of Technology, Roorkee, India,\\ngauravbhatt.cs.iitr@gmail.com\\n2 Graphic Era University, India\\nAbstract. Identifying the veracity of a news article is an interesting\\nproblem while automating this process can be a challenging task. Detec-\\ntion of a news article as fake is still an open question as it is contingent\\non many factors which the current state-of-the-art models fail to incor-\\nporate. In this paper, we explore a subtask to fake news identiﬁcation,\\nand that is stance detection. Given a news article, the task is to deter-\\nmine the relevance of the body and its claim. We present a novel idea\\nthat combines the neural, statistical and external features to provide\\nan eﬃcient solution to this problem. We compute the neural embedding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='that combines the neural, statistical and external features to provide\\nan eﬃcient solution to this problem. We compute the neural embedding\\nfrom the deep recurrent model, statistical features from the weighted\\nn-gram bag-of-words model and hand crafted external features with the\\nhelp of feature engineering heuristics. Finally, using deep neural layer all\\nthe features are combined, thereby classifying the headline-body news\\npair as agree, disagree, discuss, or unrelated. We compare our proposed\\ntechnique with the current state-of-the-art models on the fake news chal-\\nlenge dataset. Through extensive experiments, we ﬁnd that the proposed\\nmodel outperforms all the state-of-the-art techniques including the sub-\\nmissions to the fake news challenge.\\nKeywords: External features, Statistical features, Word embeddings,\\nFake news, Deep learning\\n1 Introduction\\nFake news being a potential threat towards journalism and public discourse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Keywords: External features, Statistical features, Word embeddings,\\nFake news, Deep learning\\n1 Introduction\\nFake news being a potential threat towards journalism and public discourse\\nhas created a buzz across the internet. With the recent advent of social media\\nplatforms such as Facebook and Twitter, it has become easier to propagate any\\ninformation to the masses within minutes. While the propagation of information\\nis proportional to growth of social media, there has been an aggravation in\\nthe authenticity of these news articles. These days it has become a lot easier\\nto mislead the masses using a single Facebook or Twitter fake post. For an\\ninstance, in the US presidential election of 2016, the fake news has been cited as\\nthe foremost contributing factor that aﬀected the outcome [24].\\narXiv:1712.03935v1  [cs.CL]  11 Dec 2017'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Headline ”Robert Plant Ripped up $800M Led Zeppelin Reunion Contract”Stance\\nBody 1 Led Zeppelin’s Robert Plant turned down 500 MILLION to reform supergroup.Agree\\nBody 2No, Robert Plant did not rip up an $800 million deal to get Led Zeppelin back together.Disagree\\nBody 3 Robert Plant reportedly tore up an $800 million Led Zeppelin reunion deal.Discuss\\nBody 4 Richard Branson’s Virgin Galactic is set to launch SpaceShipTwo today.Unrelated\\nTable 1.Headline-body pairs along with their relative stance.\\nThe root cause of this problem lies in the fact that none of the social net-\\nworking sites use any automatic system that can identify the veracity of news\\nﬂowing across these platforms. A possible reason for this failure is the open do-\\nmain nature of the problem that adds to the intricacies. The recently organized\\nFake News Challenge (FNC-1) [13] is an initiative in this direction. The aim of\\nthis challenge is to build an automatic system that has the capability to iden-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Fake News Challenge (FNC-1) [13] is an initiative in this direction. The aim of\\nthis challenge is to build an automatic system that has the capability to iden-\\ntify whether a news article is fake or not. More speciﬁcally, given a news article\\nthe task is to evaluate the relatedness of the news body towards its headline.\\nThe relatedness or stance is the relative perspective of a news article towards a\\nrelative claim (shown in Table 1).\\nThe idea behind building a countermeasure for fake news is to use machine\\nlearning and natural language processing (NLP) tools that can compute semantic\\nand contextual similarity between the headline and the body, and classify the\\npairs into one of four categories. Deep learning models have been eﬃcacious in\\nsolving many NLP problems that share similarities to fake news which includes\\nbut not limited to - computing semantic similarity between sentences [1, 18],\\ncommunity based question answering [31, 32], etc. The basic building blocks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='but not limited to - computing semantic similarity between sentences [1, 18],\\ncommunity based question answering [31, 32], etc. The basic building blocks\\nof all deep models are recurrent networks such as recurrent neural networks\\n(RNN) [23], long short-term memory networks (LSTM) [16] and gated recurrent\\nunits (GRU) [11], and convolution networks such as convolution neural networks\\n(CNN) [17]. A deep architecture encodes the given sequence of words into ﬁxed\\nlength vector representation which can be used to score the relevance of two\\ntextual entities, in our case, relevance of each headline-body pair.\\nStatistical information related to text can be encoded to vectors using the\\ntraditional bag-of-words (BOW) approach. The BOW approaches are often com-\\nbined with term frequency (TF) and inverse document frequency (IDF), and n-\\ngrams that helps to encode more information related to the text [28, 12]. These\\napproaches, however simple, have been used to ameliorate the performance of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='grams that helps to encode more information related to the text [28, 12]. These\\napproaches, however simple, have been used to ameliorate the performance of\\ndeep models in complex NLP problems such as community question answering\\n[32] and answer sentence selection [33]. Sometimes, it is beneﬁcial to leverage\\nfeature engineering heuristics when combined with statistical approaches. The\\nfeature engineering heuristics or the external features are used to aid the learn-\\ning model to successfully converge to a global solution [31, 32, 34]. The external\\nfeatures includes common observations such as number of n-grams, number of\\nwords match between headline and the body, cosine similarity between the head-\\nline and the body vector, etc. The FNC-1 baseline also includes a combination\\nof feature engineering heuristics that alone achieves a competitive performance,\\neven outperforming several widely used deep learning architectures. In this pa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='per, we combine external features introduced in the baseline with some more\\nheuristics that have been shown to be successful in other NLP tasks.\\nThese days it is common to use pre-trained word embeddings such as Word2vec\\n[20] and GloVe [25] along with deep models for NLP tasks. Similar to word em-\\nbedding, the recurrent models have been used to encode an entire sentence to a\\nvector. Some of the widely used sentence-to-vector models include doc2vec [21],\\nparagraph2vec [27] and skip-thought vectors [18]. These deep recurrent models\\nhelps to capture the semantic and contextual information of the textual pairs,\\nin our case, body and its claim. In our work, we use the skip-thought vector to\\nencode the headline and the body, and combine it with external features and\\nstatistical approaches.\\nFinally, the main contributions of the paper can be summarized as\\n1. We propose an approach that is based on the combination of statistical, neu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='statistical approaches.\\nFinally, the main contributions of the paper can be summarized as\\n1. We propose an approach that is based on the combination of statistical, neu-\\nral and feature engineering heuristics which achieves state-of-the-art perfor-\\nmance on the task of fake news identiﬁcation.\\n2. We evaluate the proposed approach on FNC-1 challenge, and compare our\\nresults with the top-4 submissions to the challenge. We also analyze the\\napplicability of several state-of-the-art deep models on FNC-1 dataset.\\nThe rest of the paper is organized as follows. In section 2, we brief the previous\\nidea over which our works builds, which is followed by applicability of state-\\nof-the-art deep architectures on the problem of stance detection. In section 4\\nwe describe the proposed approach in detail, followed by the experiment setup\\nin section 5, that includes dataset description, training parameters, evaluation\\nmetrics used and results. Finally, our work is concluded in section 6.\\n2 Related Work'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='in section 5, that includes dataset description, training parameters, evaluation\\nmetrics used and results. Finally, our work is concluded in section 6.\\n2 Related Work\\nIn this section, we discuss some previous work that is in relation to fake news\\nidentiﬁcation such as rumor detection in news articles and hoax news identiﬁca-\\ntion. We also discuss the use of deep learning architecture used by some of the\\nresearchers with whom our work shares some similarity.\\nFake news. From an NLP perspective, researchers have studied numerous\\naspects of credibility of online information. For example, [5] applied the time-\\nsensitive supervised approach by relying on the tweet content to address the\\ncredibility of a tweet in diﬀerent situations. [7] used LSTM in a similar problem\\nof early rumor detection. In an another work, [8] aimed at detecting the stance\\nof tweets and determining the veracity of the given rumor with convolution'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='of early rumor detection. In an another work, [8] aimed at detecting the stance\\nof tweets and determining the veracity of the given rumor with convolution\\nneural networks. A submission [3] to the SemEval 2016 Twitter Stance Detection\\ntask focuses on creating a bag-of-words auto encoder, and training it over the\\ntokenized tweets.\\nFNC-1 submissions. In their work, [26] achieved a preliminary score of\\n0.8080, slightly above the competition baseline of 0.7950. They experimented\\non four basic models on which the ﬁnal result was evaluated: Bag Of Words\\n(BOW), basic LSTM, LSTM with attention and conditional encoding LSTM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='with attention (CEA LSTM). In our work, instead of using the models separately,\\nwe combine the best of these models.\\nAnother team, [34], combined multiple models in an ensemble providing\\n50/50 weighted average between deep convolution neural network and a gradient-\\nboosted decision trees. Though this work seems to be similar to our work, the\\ndiﬀerence lies in the construction of ensemble of classiﬁers. In a similar attempt,\\na team [2] concatenated various features vectors and passed it through an MLP\\nmodel.\\nThe work by [28], focuses on generating lexical and similarity features using\\n(TF-IDF) representations of bag-of-words (BOW) which are then fed through\\na multi-layer perceptron (MLP) with one hidden layer. In their work, [6] di-\\nvided the problem into two groups: unrelated and related. They were able to\\nachieve 90% accuracy on the related/unrelated task by ﬁnding maximum and\\naverage Jaccard similarity score across all sentences in the article and choosing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='achieve 90% accuracy on the related/unrelated task by ﬁnding maximum and\\naverage Jaccard similarity score across all sentences in the article and choosing\\nappropriate threshold values. A similar work of splitting the problem into two\\nsubproblems (related and unrelated) is also performed by [10]. The work by [22]\\nfocuses on the use of recurrent models for fake news stance detection.\\n3 Technique Used\\n3.1 Deep Learning Architectures\\nTo predict the stance for a given sample in FNC-1 dataset, a multi-channel\\ndeep neural network can be used to encode a given headline-body pair, which\\ncan be classiﬁed into one of the four stances. This is achieved by using a multi\\nchannel convolution neural network with softmax layer at the output (shown in\\nFigure 1). Similarly, instead of using the convolution and pooling layers, LSTM\\nand GRU can be used to encode the headline-body pairs. The LSTMs and GRUs\\nencode the given sequence of words into ﬁxed length vector representation which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='and GRU can be used to encode the headline-body pairs. The LSTMs and GRUs\\nencode the given sequence of words into ﬁxed length vector representation which\\ncan be used to score the relevance of headline-body pair. However, for long\\nsequences, such as the body of a news article (which typically contain hundreds\\nof words), the RNN models fail to completely encode the entire information\\ninto a ﬁxed length vector. A solution to this problem is given in the form of\\nattentional mechanism [9] which computes a weighted sum of all the encoder\\nunits that are passed on to the decoder. The decoder is learned in such a way\\nthat it gives importance to only some of the words. The attention mechanism\\nalso alleviates the bottleneck of encoding input sequences to ﬁxed length vector\\nand have been shown to outperform other RNN based encoder-decoder models\\non longer sequences [4]. To alleviate the problem of limited memory we use\\nattention mechanism as described in [4].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='and have been shown to outperform other RNN based encoder-decoder models\\non longer sequences [4]. To alleviate the problem of limited memory we use\\nattention mechanism as described in [4].\\nWe experiment with some of the deep architectures that have been shown to\\nbe successful in NLP tasks (shown in Figure 1). Most of these architectures have\\nbeen proven to be eﬀective for non-factoid based question answering [30, 14].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Fig. 1.Deep architectures used for stance detection on the FNC-1 dataset.\\n4 Proposed Idea\\nThe unrelated headline-body pairs in the FNC-1 dataset are created by ran-\\ndomly assigning a news body to the given headline. This type of data augmen-\\ntation has been successfully used in NLP problems such as non-factoid question\\nanswering where it results in reasonable performance by the deep learning mod-\\nels [31, 19]. However, in the case of FNC-1 challenge, the agree, disagree, and\\ndiscuss headline-body pairs are relatively smaller in quantity than theunrelated\\nstance. This bias leads to a uneven distribution of dataset across the four classes,\\nwith the unrelated category being the least interesting. Interestingness of a\\nheadline-body pair is evaluated in terms of information that it contains; It is\\neasier to evaluate a unrelated pair, while the other three are contingent on\\nexploring contextual relationship between the headline and its body, and are\\nconsidered more interesting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='easier to evaluate a unrelated pair, while the other three are contingent on\\nexploring contextual relationship between the headline and its body, and are\\nconsidered more interesting.\\nThe uneven distribution of FNC-1 dataset thwarts the performance of deep\\nlearning architectures introduced in Section 3. Moreover, news articles are heav-\\nily inﬂuenced by some words that are generally associated with news to describe\\nits polarity. For example, words likecrime, accident, and scandal are often used\\nwith negative connotation. If such words are present in both the news headline,\\nor are present in one while absent from the other, then, it is easier to identify such\\na pair as agree or disagree. Deep learning models are dependent on a huge train-\\ning corpus (few million headline-body pairs) in order to identify such nuances\\nin patterns. The FNC-1 dataset, though the largest publicly available dataset\\non stance detection, does not satiate this criteria. For this reason, we introduce'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='in patterns. The FNC-1 dataset, though the largest publicly available dataset\\non stance detection, does not satiate this criteria. For this reason, we introduce\\na much simpler strategy that consists of heavy use of feature engineering. We\\nleveraged several widely used state-of-the-art features used in natural language\\nprocessing, and use a feed-forward deep neural network which aggregates all the\\nindividual features and computes a score for each headline-body pair.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Fig. 2.Combining the neural, statistical and external features using deep MLP.\\n4.1 Neural Embeddings\\nWe use skip-thought vectors which encodes sentences to vector embedding of\\nlength 4800 (shown in Figure 2). The skip-thought [18] is a encoder-decoder\\nbased recurrent model that computes the relative occurrence of sentences. In\\nour work, we use the pre-trained skip-thought embedding which is trained on\\nBookCorpus [35]. We make the use of a pre-trained model since the FNC-1\\ndataset is relatively smaller than the dataset required to eﬃciently train a re-\\ncurrent encoder-decoder model like skip-thought.\\nWe follow the work of [18, 1] and compute two features from the skip-thought\\nembeddings. These features have been shown to be eﬀective in evaluating con-\\ntextual similarity between sentences. The task of stance detection is analogous\\nto the computation of contextual similarity between two sentences - headline\\nand its body. We speculate that the features introduced by [18, 1] should be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='to the computation of contextual similarity between two sentences - headline\\nand its body. We speculate that the features introduced by [18, 1] should be\\neﬀective for stance detection as well. Given the skip-thought encoding of news\\nand headline as unews and vhead, we compute two features\\nfeat1 = unews.vhead (1)\\nfeat2 = |unews −vhead| (2)\\nwhere feat1 is the component-wise product and feat2 is the absolute diﬀer-\\nence between the skip-thought encoding of news and headlines. Both of these\\nfeatures results in a 4800 dimensional vector each.\\n4.2 Statistical Features\\nWe capture the statistical information from the text to vectors with the help of\\nBOW, TF-IDF and n-grams models. We follow the work of [28] and [12], and\\nproduce the following vectors for each headline-body pair\\n1. 1-gram TF vector of the headline.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='2. 1-gram TF vector of the body.\\nThis gives us a vector of 5000 dimension each. We concatenate both of the TF\\nvectors and pass it to a MLP layer (as shown in Figure 2).\\n4.3 External Features\\nThe external features include feature engineering heuristics such as number of\\nsimilar words in the headline and body, cosine similarity between vector encod-\\nings of headline-body pairs, number of n-grams matched between the pairs, etc.\\nWe leveraged ideas for computing the external features from the baseline and\\nadd some extra features, which includes\\n1. Number of characters n-grams match between the headline-body pair, where\\nn = 2, ··· , 16.\\n2. Number of words n-grams match between the headline-body pair, where\\nn = 2, ··· , 6.\\n3. Weighted TF-IDF score between headline and its body using the approach\\nmentioned in [33].\\n4. Sentiment diﬀerence between the headline-body pair, also termed as polarity\\nand is computed using lexicon based approach.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='mentioned in [33].\\n4. Sentiment diﬀerence between the headline-body pair, also termed as polarity\\nand is computed using lexicon based approach.\\n5. N-gram refuting feature which is constructed using BOW on a lexicon of n\\npre-deﬁned words. It is similar to polarity based features with an addition\\nof n-gram model.\\nAll the external features adds up to a 50-dimensional feature vector and is passed\\nto a MLP layer similar to neural and statistical features.\\n5 Experimentations\\n5.1 Dataset Description\\nWe use the dataset provided in the FNC-1 challenge which is derived from the\\nEmergent Dataset [15], provided by the fake news challenge administrators. The\\nformer consist of 49972 tuple with each tuple consisting of a headline-body pair\\nfollowed by a corresponding class label stance of either agree, disagree, unrelated\\nor discuss. Word counts roughly ranges between 8 to 40 for headlines and 600\\nto 7000 for article body. The distribution of FNC-1 dataset is shown in Table 2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='or discuss. Word counts roughly ranges between 8 to 40 for headlines and 600\\nto 7000 for article body. The distribution of FNC-1 dataset is shown in Table 2.\\nNews articles unrelated discuss agree disagree\\n49972 73.13 % 17.83 % 7.36 % 1.68 %\\nTable 2.FNC-1 dataset description.\\nThe ﬁnal results are evaluated over a test dataset provided by fake news\\norganization consisting of 25413 samples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Hyperparameter Skip-thought External FeaturesTF-IDF Vectors\\nMLP layers 2 1 2\\nMLP neurons 500 ; 100 50 500 ; 50\\nDropout 0.2 ; - - 0.4 ; -\\nActivation sigmoid ; sigmoid relu relu ; relu\\nRegularization L2 - 0.00000001 ; - - L2 - 0.00005 ; -\\nMLP Layers 1\\nMLP neurons 4\\nActivation Softmax\\nOptimizer Adam\\nLearning rate 0.001\\nBatch size 100\\nLoss Cross-entropy\\nTable 3.Values of hyper-parameters. The ﬁrst half of the table shows the parameters\\nused in architectures for extracting individual features. The second half shows the\\nparameter setting of the feature combination layer that is shown in Figure 2.\\n5.2 Training parameters\\nAs shown in Figure 2, the proposed model computes the feature vectors sepa-\\nrately and then combine these with the help of a MLP layer. We use cross-entropy\\nas the loss function to optimize our architecture with a softmax layer at the out-\\nput which classify the given headline-body pair into agree, disagree, discuss, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='as the loss function to optimize our architecture with a softmax layer at the out-\\nput which classify the given headline-body pair into agree, disagree, discuss, and\\nunrelated. The hyper-parameter setting is shown in Table 3.\\n5.3 Baselines and compared methods\\nOrganizers of FNC-1 have provided a baseline model that consists of a gradient-\\nboosting classiﬁer over n-gram subsequences between the headline and the body\\nalong with several external features such as word overlap, occurrence of sentiment\\nusing a lexicon of highly-polarized words (like fraud and hoax). With this simple\\nyet elegant baseline it is possible to outperform some of the highly used deep\\nlearning architectures that we have used in our work. Following the work of [26],\\nwe also introduce three new baselines for the FNC-1 dataset: word2vec+external\\nfeatures baseline, skip-thought baseline, and TF-IDF baseline. All these baselines\\nfocuses on performance of neural, statistical, and external features, when used'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='features baseline, skip-thought baseline, and TF-IDF baseline. All these baselines\\nfocuses on performance of neural, statistical, and external features, when used\\nindividually.\\nWe compare our proposed approach with the submissions of top 4 teams at\\nFNC-1 3, which includes the work by [34], [26], [2] and [28]. Apart from the\\ntop submissions at FNC-1, we also compare the proposed architecture with four\\ndeep learning architectures introduced in Section 3, namely, CNN, biLSTM,\\nBiLSTM+Attention and CNN+biLSTM.\\n3 http://www.fakenewschallenge.org/\\nhttps://competitions.codalab.org/competitions/16843#results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='5.4 Evaluation metrics\\nFrom Table 2 it is evident that the FNC-1 dataset shows a heavy bias towards\\nunrelated headline-body pairs. Recognizing this data bias and the simpler na-\\nture of the related/unrelated classiﬁcation problems, the organizers of FNC-1\\nintroduced the following weighted accuracy score as their ﬁnal evaluation metric.\\nScore1 = AccuracyRelated,Unrelated (3)\\nScore2 = AccuracyAgree,Disagree,Discuss (4)\\nScoreFNC = 0.25 ∗Score1 + 0.75 ∗Score2 (5)\\nWe use the ScoreFNC as the main evaluation criteria while comparing the\\nproposed model with other related techniques. We also use the class-wise accu-\\nracy for further evaluation of the performance of all the techniques.\\n5.5 Results\\nThe results on FNC-1 test dataset are shown in Table 4. The ﬁrst part of the ta-\\nble shows the performance of the baselines used in our work. The FNC-1 baseline\\nachieves a score of 75 .2 which is better than the performance of all deep archi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='ble shows the performance of the baselines used in our work. The FNC-1 baseline\\nachieves a score of 75 .2 which is better than the performance of all deep archi-\\ntectures introduced in Section 3. The FNC-1 baseline is comprised of training\\ngradient tree classiﬁer on the hand crafted features (described in Section 4.3).\\nProvided the simplicity of this baseline, it is indeed remarkable to achieve such a\\nhigh score. The FNC-1 baselines achieves approx 7% higher class-wise accuracy\\non unrelated stance as compared to skip-thought baseline, whereas the latter\\nreceiving a higher ScoreFNC . Skip-thought baselines achieves a higher accuracy\\non agree and discuss than the unrelated stance. Since the interestingness of\\nagree and discuss is higher than the unrealted stance, therefore, skip-thought\\nachieves a higher ScoreFNC . This also explains the reason for the introduction\\nof new scoring criterion by the FNC organizers (see Section 5.4). Finally, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='achieves a higher ScoreFNC . This also explains the reason for the introduction\\nof new scoring criterion by the FNC organizers (see Section 5.4). Finally, the\\nScoreFNC by skip-thought, external features, and TF-IDF baselines are higher\\nthan the FNC-1 baseline. Therefore, our speculation to combine these three base-\\nlines models, is guaranteed to achieve a higher score on ScoreFNC evaluation\\nmetric. Moreover, all the baselines achieves very low or zero score on the dis-\\nagree stance. Therefore, apart from the ScoreFNC , the class-wise performance\\nis worth considering as a performance criterion.\\nThe performance of top-4 teams that participated in FNC-1 are shown in the\\nmiddle part of Table 4, with SOLAT in the SWEN[34] winning the challenge\\nachieving a score of 82.05. All the teams achieved higher score and class-wise\\naccuracy on all stances except for the disagree stance. This should be a concern,\\nsince the importance of disagree is equivalent to the agree and discuss stance.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='accuracy on all stances except for the disagree stance. This should be a concern,\\nsince the importance of disagree is equivalent to the agree and discuss stance.\\nWe observed that the news pairs in the disagree category are not only very few,\\nbut also consists of divergent news articles. This is one of the reason for poor\\nperformance of most of the deep models, including the top teams, on identifying\\ndiagree stance.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Method ScoreFNC Agree Disagree Discuss Unrelated Overall\\nFNC-1 baseline 75.20 9.09 1.00 79.65 97.97 85.44\\nWord2vec + External Features 75.78 50.70 9.61 53.38 96.05 82.79\\nSkip-thought baseline 76.18 31.8 0.00 81.20 91.18 82.48\\nTF-IDF baseline 81.72 44.04 6.60 81.38 97.90 88.46\\nSOLAT in the SWEN [34] 82.05 58.50 1.86 76.18 98.70 89.08\\nAthene [2] 81.97 44.72 9.47 80.89 99.25 89.50\\nUCL Machine Reading [28] 81.72 44.04 6.60 81.38 97.90 88.46\\nChips Ahoy! [29] 80.12 55.96 0.28 70.29 98.98 88.01\\nCNN 60.91 35.89 2.10 46.77 88.47 74.84\\nbiLSTM 63.11 38.04 4.59 58.13 78.27 69.88\\nbiLSTM + Attention 63.17 58.74 0.03 63.48 77.49 73.27\\nCNN + biLSTM 64.95 74.09 2.46 57.85 74.87 72.89\\nProposed 83.08 43.82 6.31 85.68 98.04 89.29\\nTable 4.Performance of diﬀerent models on FNC-1 Test Dataset. The ﬁrst half of the\\ntable shows the baselines, followed by the top-4 submissions, and diﬀerent architectures\\nused in our work. Column 2-5 shows the class-wise accuracy in % while the last column'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='table shows the baselines, followed by the top-4 submissions, and diﬀerent architectures\\nused in our work. Column 2-5 shows the class-wise accuracy in % while the last column\\nshows the overall accuracy.\\nAgree Disagree Discuss Unrelated Overall\\nAgree 834 15 945 109 43.82\\nDisagree 208 44 328 117 6.31\\nDiscuss 401 23 3825 215 85.68\\nUnrelated 22 12 325 17990 98.04\\nTable 5.Confusion matrix for the proposed model on FNC-1 testset.\\nThe lowest section in Table 4 shows the performance of the proposed model\\nalong with other architectures used in our work. The proposed model achieves\\nhighest score and highest class-wise accuracy ondiscuss stance whereas achieving\\nhigh accuracy on other stances that is comparable to top submissions at FNC-1.\\nFrom Table 5, it is evident that the overall accuracy achieved by the proposed\\nmodel is slightly lower than [2], although the proposed model outperformed all\\nthe other techniques by a clear margin (in terms of ScoreFNC ). The possible'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='model is slightly lower than [2], although the proposed model outperformed all\\nthe other techniques by a clear margin (in terms of ScoreFNC ). The possible\\nreason for this deviation is that the [2] gives more focus to the classiﬁcation of\\nunrelated stances rather than the rest, which is the reason for highest overall\\naccuracy. Since unrelated stances are of least interest to us, this results in lower\\nScoreFNC . Finally, a confusion matrix is given in Table 5 that provides in-detail\\nanalysis of the performance of our approach.\\n6 Conclusion\\nIn this paper, we explore the beneﬁt of incorporating neural, statistical and\\nexternal features to deep neural networks on the task of fake news stance de-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='tection. We also presented in-depth analysis of several state-of-the-art recurrent\\nand convolution architectures (shown in Figure 1). The presented idea lever-\\nages features extracted using skip-thought embeddings, n-gram TF-vectors and\\nseveral introduced hand crafted features.\\nWe found that the uneven distribution of FNC-1 dataset undermines the\\nperformance of most deep learning architectures. The fewer training samples\\nadds further to this aggravation. Creating a dataset for a complex NLP problems\\nsuch as fake news identiﬁcation is indeed a cumbersome task, and we appreciate\\nthe work by the FNC organizers, yet, a more detailed and elaborate dataset\\nshould make this challenge more suitable to evaluate.\\nReferences\\n[1] E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W. Guo. sem\\n2013 shared task: Semantic textual similarity, including a pilot on typed-\\nsimilarity. In In* SEM 2013: The Second Joint Conference on Lexical and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='2013 shared task: Semantic textual similarity, including a pilot on typed-\\nsimilarity. In In* SEM 2013: The Second Joint Conference on Lexical and\\nComputational Semantics. Association for Computational Linguistics. Cite-\\nseer, 2013.\\n[2] B. S. Andreas Hanselowski, Avinesh PVS and F. Caspelherr. Team athene\\non the fake news challenge. 2017.\\n[3] I. Augenstein, A. Vlachos, and K. Bontcheva. Usfd at semeval-2016 task\\n6: Any-target stance detection on twitter with autoencoders. In SemEval@\\nNAACL-HLT, pages 389–393, 2016.\\n[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014. URL http:\\n//arxiv.org/abs/1409.0473.\\n[5] C. Castillo, M. Mendoza, and B. Poblete. Predicting information credibility\\nin time-sensitive social media. Internet Research, 23(5):560–588, 2013.\\n[6] A. K. Chaudhry, D. Baker, and P. Thun-Hohenstein. Stance detection for\\nthe fake news challenge: Identifying textual relationships with deep neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='[6] A. K. Chaudhry, D. Baker, and P. Thun-Hohenstein. Stance detection for\\nthe fake news challenge: Identifying textual relationships with deep neural\\nnets. 2017.\\n[7] T. Chen, L. Wu, X. Li, J. Zhang, H. Yin, and Y. Wang. Call attention\\nto rumors: Deep attention based recurrent neural networks for early rumor\\ndetection. arXiv preprint arXiv:1704.05973, 2017.\\n[8] Y.-C. Chen, Z.-Y. Liu, and H.-Y. Kao. Ikm at semeval-2017 task 8: Convo-\\nlutional neural networks for stance detection and rumor veriﬁcation. Pro-\\nceedings of SemEval. ACL, 2017.\\n[9] K. Cho, B. van Merrienboer, C ¸ . G¨ ul¸ cehre, F. Bougares, H. Schwenk, and\\nY. Bengio. Learning phrase representations using RNN encoder-decoder for\\nstatistical machine translation. CoRR, abs/1406.1078, 2014. URL http:\\n//arxiv.org/abs/1406.1078.\\n[10] S. Chopra, S. Jain, and J. M. Sholar. Towards automatic identiﬁcation\\nof fake news: Headline-article stance detection with lstm attention models,\\n2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='[11] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of\\ngated recurrent neural networks on sequence modeling. arXiv preprint\\narXiv:1412.3555, 2014.\\n[12] R. Davis and C. Proctor. Fake news, real consequences: Recruiting neural\\nnetworks for the ﬁght against fake news. 2017.\\n[13] D. R. Dean Pomerleau. Fake news challenge. 2017.\\n[14] M. Feng, B. Xiang, M. R. Glass, L. Wang, and B. Zhou. Applying deep\\nlearning to answer selection: A study and an open task. InAutomatic Speech\\nRecognition and Understanding (ASRU), 2015 IEEE Workshop on, pages\\n813–820. IEEE, 2015.\\n[15] W. Ferreira and A. Vlachos. Emergent: a novel data-set for stance classiﬁca-\\ntion. In Proceedings of the 2016 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Tech-\\nnologies. ACL, 2016.\\n[16] A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with\\nbidirectional lstm and other neural network architectures. Neural Networks,\\n18(5):602–610, 2005.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='nologies. ACL, 2016.\\n[16] A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with\\nbidirectional lstm and other neural network architectures. Neural Networks,\\n18(5):602–610, 2005.\\n[17] H. He, K. Gimpel, and J. J. Lin. Multi-perspective sentence similarity\\nmodeling with convolutional neural networks. InEMNLP, pages 1576–1586,\\n2015.\\n[18] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba,\\nand S. Fidler. Skip-thought vectors. In Advances in neural information\\nprocessing systems, pages 3294–3302, 2015.\\n[19] T. Mihaylov and P. Nakov. Semanticz at semeval-2016 task 3: Ranking\\nrelevant answers in community question answering using semantic similarity\\nbased on ﬁne-tuned word embeddings. In SemEval@ NAACL-HLT, pages\\n879–886, 2016.\\n[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word\\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed\\nrepresentations of words and phrases and their compositionality. In Ad-\\nvances in neural information processing systems, pages 3111–3119, 2013.\\n[22] K. Miller and A. Oswalt. Fake news headline classiﬁcation using neural\\nnetworks with attention. 2017.\\n[23] P. Neculoiu, M. Versteegh, M. Rotaru, and T. B. Amsterdam. Learning\\ntext similarity with siamese recurrent networks. ACL 2016, page 148, 2016.\\n[24] NYTimes. As fake news spreads lies, more readers shrug at the truth. 2016.\\n[25] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for\\nword representation. In EMNLP, volume 14, pages 1532–1543, 2014.\\n[26] S. Pfohl, O. Triebe, and F. Legros. Stance detection for the fake news\\nchallenge with attention and conditional encoding.\\n[27] R. ˇReh˚ uˇ rek and P. Sojka. Software Framework for Topic Modelling with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='challenge with attention and conditional encoding.\\n[27] R. ˇReh˚ uˇ rek and P. Sojka. Software Framework for Topic Modelling with\\nLarge Corpora. In Proceedings of the LREC 2010 Workshop on New\\nChallenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010.\\nELRA. http://is.muni.cz/publication/884893/en.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='[28] B. Riedel, I. Augenstein, G. P. Spithourakis, and S. Riedel. A simple but\\ntough-to-beat baseline for the fake news challenge stance detection task.\\narXiv preprint arXiv:1707.03264, 2017.\\n[29] J. Shang. Chips ahoy! at fake news challenge. 2017.\\n[30] M. Tan, C. d. Santos, B. Xiang, and B. Zhou. Lstm-based deep learning\\nmodels for non-factoid answer selection. arXiv preprint arXiv:1511.04108,\\n2015.\\n[31] L. Yang, Q. Ai, D. Spina, R.-C. Chen, L. Pang, W. B. Croft, J. Guo, and\\nF. Scholer. Beyond factoid qa: Eﬀective methods for non-factoid answer\\nsentence retrieval. In European Conference on Information Retrieval, pages\\n115–128. Springer, 2016.\\n[32] Y. Yang, W.-t. Yih, and C. Meek. Wikiqa: A challenge dataset for open-\\ndomain question answering. In EMNLP, pages 2013–2018, 2015.\\n[33] L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman. Deep learning for\\nanswer sentence selection. arXiv preprint arXiv:1412.1632, 2014.\\n[34] S. B. Yuxi Pan, Doug Sibley. Talos. http://blog.talosintelligence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='answer sentence selection. arXiv preprint arXiv:1412.1632, 2014.\\n[34] S. B. Yuxi Pan, Doug Sibley. Talos. http://blog.talosintelligence.\\ncom/2017/06/, 2017.\\n[35] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and\\nS. Fidler. Aligning books and movies: Towards story-like visual explanations\\nby watching movies and reading books. arXiv preprint arXiv:1506.06724,\\n2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nN-BEATS: N EURAL BASIS EXPANSION ANALYSIS FOR\\nINTERPRETABLE TIME SERIES FORECASTING\\nBoris N. Oreshkin\\nElement AI\\nboris.oreshkin@gmail.com\\nDmitri Carpov\\nElement AI\\ndmitri.carpov@elementai.com\\nNicolas Chapados\\nElement AI\\nchapados@elementai.com\\nYoshua Bengio\\nMila\\nyoshua.bengio@mila.quebec\\nABSTRACT\\nWe focus on solving the univariate times series point forecasting problem using\\ndeep learning. We propose a deep neural architecture based on backward and\\nforward residual links and a very deep stack of fully-connected layers. The ar-\\nchitecture has a number of desirable properties, being interpretable, applicable\\nwithout modiﬁcation to a wide array of target domains, and fast to train. We test\\nthe proposed architecture on several well-known datasets, including M3, M4 and\\nTOURISM competition datasets containing time series from diverse domains. We\\ndemonstrate state-of-the-art performance for two conﬁgurations of N-BEATS for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='TOURISM competition datasets containing time series from diverse domains. We\\ndemonstrate state-of-the-art performance for two conﬁgurations of N-BEATS for\\nall the datasets, improving forecast accuracy by 11% over a statistical benchmark\\nand by 3% over last year’s winner of the M4 competition, a domain-adjusted\\nhand-crafted hybrid between neural network and statistical time series models.\\nThe ﬁrst conﬁguration of our model does not employ any time-series-speciﬁc\\ncomponents and its performance on heterogeneous datasets strongly suggests that,\\ncontrarily to received wisdom, deep learning primitives such as residual blocks are\\nby themselves sufﬁcient to solve a wide range of forecasting problems. Finally, we\\ndemonstrate how the proposed architecture can be augmented to provide outputs\\nthat are interpretable without considerable loss in accuracy.\\n1 I NTRODUCTION\\nTime series (TS) forecasting is an important business problem and a fruitful application area for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='that are interpretable without considerable loss in accuracy.\\n1 I NTRODUCTION\\nTime series (TS) forecasting is an important business problem and a fruitful application area for\\nmachine learning (ML). It underlies most aspects of modern business, including such critical areas as\\ninventory control and customer management, as well as business planning going from production and\\ndistribution to ﬁnance and marketing. As such, it has a considerable ﬁnancial impact, often ranging\\nin the millions of dollars for every point of forecasting accuracy gained (Jain, 2017; Kahn, 2003).\\nAnd yet, unlike areas such as computer vision or natural language processing where deep learning\\n(DL) techniques are now well entrenched, there still exists evidence that ML and DL struggle to\\noutperform classical statistical TS forecasting approaches (Makridakis et al., 2018a;b). For instance,\\nthe rankings of the six “pure” ML methods submitted to M4 competition were 23, 37, 38, 48, 54,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='outperform classical statistical TS forecasting approaches (Makridakis et al., 2018a;b). For instance,\\nthe rankings of the six “pure” ML methods submitted to M4 competition were 23, 37, 38, 48, 54,\\nand 57 out of a total of 60 entries, and most of the best-ranking methods were ensembles of classical\\nstatistical techniques (Makridakis et al., 2018b).\\nOn the other hand, the M4 competition winner (Smyl, 2020), was based on a hybrid between\\nneural residual/attention dilated LSTM stack with a classical Holt-Winters statistical model (Holt,\\n1957; 2004; Winters, 1960) with learnable parameters. Since Smyl’s approach heavily depends on\\nthis Holt-Winters component, Makridakis et al. (2018b) further argue that “hybrid approaches and\\ncombinations of method are the way forward for improving the forecasting accuracy and making\\nforecasting more valuable”. In this work we aspire to challenge this conclusion by exploring the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='combinations of method are the way forward for improving the forecasting accuracy and making\\nforecasting more valuable”. In this work we aspire to challenge this conclusion by exploring the\\npotential of pure DL architectures in the context of the TS forecasting. Moreover, in the context of\\ninterpretable DL architecture design, we are interested in answering the following question: can we\\n1\\narXiv:1905.10437v4  [cs.LG]  20 Feb 2020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\ninject a suitable inductive bias in the model to make its internal operations more interpretable, in the\\nsense of extracting some explainable driving factors combining to produce a given forecast?\\n1.1 S UMMARY OF CONTRIBUTIONS\\nDeep Neural Architecture:To the best of our knowledge, this is the ﬁrst work to empirically\\ndemonstrate that pure DL using no time-series speciﬁc components outperforms well-established\\nstatistical approaches on M3, M4 and TOURISM datasets (on M4, by 11% over statistical benchmark,\\nby 7% over the best statistical entry, and by 3% over the M4 competition winner). In our view, this\\nprovides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens\\nmotivation to continue advancing the research in this area.\\nInterpretable DL for Time Series:In addition to accuracy beneﬁts, we also show that it is fea-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='motivation to continue advancing the research in this area.\\nInterpretable DL for Time Series:In addition to accuracy beneﬁts, we also show that it is fea-\\nsible to design an architecture with interpretable outputs that can be used by practitioners in very\\nmuch the same way as traditional decomposition techniques such as the “seasonality-trend-level”\\napproach (Cleveland et al., 1990).\\n2 P ROBLEM STATEMENT\\nWe consider the univariate point forecasting problem in discrete time. Given a length- H forecast\\nhorizon a length- T observed series history [y1,..., yT ] ∈RT , the task is to predict the vector of\\nfuture values y ∈RH = [yT +1,yT +2,..., yT +H ]. For simplicity, we will later consider a lookback\\nwindow of length t ≤T ending with the last observed value yT to serve as model input, and denoted\\nx ∈Rt = [yT −t+1,..., yT ]. We denote ˆy the forecast of y. The following metrics are commonly\\nused to evaluate forecasting performance (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='x ∈Rt = [yT −t+1,..., yT ]. We denote ˆy the forecast of y. The following metrics are commonly\\nused to evaluate forecasting performance (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000;\\nMakridakis et al., 2018b; Athanasopoulos et al., 2011):\\nsMAPE = 200\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\n|yT +i|+|ˆyT +i|, MAPE = 100\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\n|yT +i| ,\\nMASE = 1\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\n1\\nT +H−m ∑T +H\\nj=m+1 |yj −yj−m|, OWA = 1\\n2\\n[ sMAPE\\nsMAPE Naïve2\\n+\\nMASE\\nMASE Naïve2\\n]\\n.\\nHere m is the periodicity of the data (e.g., 12 for monthly series). MAPE (Mean Absolute Percentage\\nError), sMAPE (symmetric MAPE ) and MASE (Mean Absolute Scaled Error) are standard scale-free\\nmetrics in the practice of forecasting (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000):\\nwhereas sMAPE scales the error by the average between the forecast and ground truth, the MASE\\nscales by the average error of the naïve predictor that simply copies the observation measured m'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='whereas sMAPE scales the error by the average between the forecast and ground truth, the MASE\\nscales by the average error of the naïve predictor that simply copies the observation measured m\\nperiods in the past, thereby accounting for seasonality. OWA (overall weighted average) is a M4-\\nspeciﬁc metric used to rank competition entries (M4 Team, 2018b), where sMAPE and MASE metrics\\nare normalized such that a seasonally-adjusted naïve forecast obtains OWA = 1.0.\\n3 N-BEATS\\nOur architecture design methodology relies on a few key principles. First, the base architecture\\nshould be simple and generic, yet expressive (deep). Second, the architecture should not rely on time-\\nseries-speciﬁc feature engineering or input scaling. These prerequisites let us explore the potential\\nof pure DL architecture in TS forecasting. Finally, as a prerequisite to explore interpretability, the\\narchitecture should be extendable towards making its outputs human interpretable. We now discuss'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='of pure DL architecture in TS forecasting. Finally, as a prerequisite to explore interpretability, the\\narchitecture should be extendable towards making its outputs human interpretable. We now discuss\\nhow those principles converge to the proposed architecture.\\n3.1 B ASIC BLOCK\\nThe proposed basic building block has a fork architecture and is depicted in Fig. 1 (left). We focus on\\ndescribing the operation of ℓ-th block in this section in detail (note that the block index ℓ is dropped\\nin Fig. 1 for brevity). The ℓ-th block accepts its respective input xℓ and outputs two vectors, ˆxℓ and\\nˆyℓ. For the very ﬁrst block in the model, its respective xℓ is the overall model input — a history\\nlookback window of certain length ending with the last measured observation. We set the length of\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nFC Stack\\n(4 layers)\\nFC FC\\n( )\\x00\\x00 \\x00\\x00( )\\x00\\x00 \\x00\\x00\\nForecastBackcast\\n\\x00\\x00\\nBlock Input\\n\\x00\\x00\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)\\nLookback window\\n(model input)\\nForecast Period\\nHorizon H\\nLookback\\xa0Period\\nHorizon nH (here n=3)\\nFigure 1: Proposed architecture. The basic building block is a multi-layer FC network with RELU\\nnonlinearities. It predicts basis expansion coefﬁcients both forward, θ f , (forecast) and backward, θb,\\n(backcast). Blocks are organized into stacks using doubly residual stacking principle. A stack may\\nhave layers with shared gb and gf . Forecasts are aggregated in hierarchical fashion. This enables\\nbuilding a very deep neural network with interpretable outputs.\\ninput window to a multiple of the forecast horizon H, and typical lengths of x in our setup range from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='building a very deep neural network with interpretable outputs.\\ninput window to a multiple of the forecast horizon H, and typical lengths of x in our setup range from\\n2H to 7H. For the rest of the blocks, their inputs xℓ are residual outputs of the previous blocks. Each\\nblock has two outputs: ˆyℓ, the block’s forward forecast of lengthH; and ˆxℓ, the block’s best estimate\\nof xℓ, also known as the ‘backcast’, given the constraints on the functional space that the block can\\nuse to approximate signals.\\nInternally, the basic building block consists of two parts. The ﬁrst part is a fully connected network\\nthat produces the forward θ f\\nℓ and the backward θb\\nℓ predictors of expansion coefﬁcients (again, note\\nthat the block index ℓ is dropped for θb\\nℓ , θ f\\nℓ , gb\\nℓ, gf\\nℓ in Fig. 1 for brevity). The second part consists of\\nthe backward gb\\nℓ and the forward gf\\nℓ basis layers that accept the respective forward θ f\\nℓ and backward\\nθb'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='ℓ , θ f\\nℓ , gb\\nℓ, gf\\nℓ in Fig. 1 for brevity). The second part consists of\\nthe backward gb\\nℓ and the forward gf\\nℓ basis layers that accept the respective forward θ f\\nℓ and backward\\nθb\\nℓ expansion coefﬁcients, project them internally on the set of basis functions and produce the\\nbackcast ˆxℓ and the forecast outputs ˆyℓ deﬁned in the previous paragraph.\\nThe operation of the ﬁrst part of the ℓ-th block is described by the following equations:\\nhℓ,1 = FCℓ,1(xℓ), hℓ,2 = FCℓ,2(hℓ,1), hℓ,3 = FCℓ,3(hℓ,2), hℓ,4 = FCℓ,4(hℓ,3).\\nθb\\nℓ = LINEAR b\\nℓ(hℓ,4), θ f\\nℓ = LINEAR f\\nℓ (hℓ,4).\\n(1)\\nHere LINEAR layer is simply a linear projection layer, i.e. θ f\\nℓ = Wf\\nℓ hℓ,4. The FC layer is a standard\\nfully connected layer with RELU non-linearity (Nair & Hinton, 2010), such that for FCℓ,1 we have,\\nfor example: hℓ,1 = RELU(Wℓ,1xℓ +bℓ,1). One task of this part of the architecture is to predict the\\nforward expansion coefﬁcients θ f\\nℓ with the ultimate goal of optimizing the accuracy of the partial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='forward expansion coefﬁcients θ f\\nℓ with the ultimate goal of optimizing the accuracy of the partial\\nforecast ˆyℓ by properly mixing the basis vectors supplied by gf\\nℓ . Additionally, this sub-network\\npredicts backward expansion coefﬁcients θb\\nℓ used by gb\\nℓ to produce an estimate of xℓ with the ultimate\\ngoal of helping the downstream blocks by removing components of their input that are not helpful for\\nforecasting.\\nThe second part of the network maps expansion coefﬁcients θ f\\nℓ and θb\\nℓ to outputs via basis layers,\\nˆyℓ = gf\\nℓ (θ f\\nℓ ) and ˆxℓ = gb\\nℓ(θb\\nℓ ). Its operation is described by the following equations:\\nˆyℓ =\\ndim(θ f\\nℓ )\\n∑\\ni=1\\nθ f\\nℓ,ivf\\ni , ˆxℓ =\\ndim(θb\\nℓ )\\n∑\\ni=1\\nθb\\nℓ,ivb\\ni .\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nHere vf\\ni and vb\\ni are forecast and backcast basis vectors, θ f\\nℓ,i is the i-th element of θ f\\nℓ . The function\\nof gb\\nℓ and gf\\nℓ is to provide sufﬁciently rich sets {vf\\ni }\\ndim(θ f\\nℓ )\\ni=1 and {vb\\ni }\\ndim(θb\\nℓ )\\ni=1 such that their respective\\noutputs can be represented adequately via varying expansion coefﬁcientsθ f\\nℓ and θb\\nℓ . As shown below,\\ngb\\nℓ and gf\\nℓ can either be chosen to be learnable or can be set to speciﬁc functional forms to reﬂect\\ncertain problem-speciﬁc inductive biases in order to appropriately constrain the structure of outputs.\\nConcrete examples of gb\\nℓ and gf\\nℓ are discussed in Section 3.3.\\n3.2 D OUBLY RESIDUAL STACKING\\nThe classical residual network architecture adds the input of the stack of layers to its output before\\npassing the result to the next stack (He et al., 2016). The DenseNet architecture proposed by Huang\\net al. (2017) extends this principle by introducing extra connections from the output of each stack to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='et al. (2017) extends this principle by introducing extra connections from the output of each stack to\\nthe input of every other stack that follows it. These approaches provide clear advantages in improving\\nthe trainability of deep architectures. Their disadvantage in the context of this work is that they result\\nin network structures that are difﬁcult to interpret. We propose a novel hierarchical doubly residual\\ntopology depicted in Fig. 1 (middle and right). The proposed architecture has two residual branches,\\none running over backcast prediction of each layer and the other one is running over the forecast\\nbranch of each layer. Its operation is described by the following equations:\\nxℓ = xℓ−1 −ˆxℓ−1, ˆy = ∑\\nℓ\\nˆyℓ.\\nAs previously mentioned, in the special case of the very ﬁrst block, its input is the model level\\ninput x, x1 ≡x. For all other blocks, the backcast residual branch xℓ can be thought of as running a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='As previously mentioned, in the special case of the very ﬁrst block, its input is the model level\\ninput x, x1 ≡x. For all other blocks, the backcast residual branch xℓ can be thought of as running a\\nsequential analysis of the input signal. Previous block removes the portion of the signal ˆxℓ−1 that\\nit can approximate well, making the forecast job of the downstream blocks easier. This structure\\nalso facilitates more ﬂuid gradient backpropagation. More importantly, each block outputs a partial\\nforecast ˆyℓ that is ﬁrst aggregated at the stack level and then at the overall network level, providing a\\nhierarchical decomposition. The ﬁnal forecast ˆy is the sum of all partial forecasts. In a generic model\\ncontext, when stacks are allowed to have arbitrary gb\\nℓ and gf\\nℓ for each layer, this makes the network\\nmore transparent to gradient ﬂows. In a special situation of deliberate structure enforced in gb\\nℓ and gf\\nℓ'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='ℓ and gf\\nℓ for each layer, this makes the network\\nmore transparent to gradient ﬂows. In a special situation of deliberate structure enforced in gb\\nℓ and gf\\nℓ\\nshared over a stack, explained next, this has the critical importance of enabling interpretability via the\\naggregation of meaningful partial forecasts.\\n3.3 I NTERPRETABILITY\\nWe propose two conﬁgurations of the architecture, based on the selection of gb\\nℓ and gf\\nℓ . One of them\\nis generic DL, the other one is augmented with certain inductive biases to be interpretable.\\nThe generic architecturedoes not rely on TS-speciﬁc knowledge. We set gb\\nℓ and gf\\nℓ to be a linear\\nprojection of the previous layer output. In this case the outputs of block ℓ are described as:\\nˆyℓ = Vf\\nℓ θ f\\nℓ +bf\\nℓ , ˆxℓ = Vb\\nℓθb\\nℓ +bb\\nℓ.\\nThe interpretation of this model is that the FC layers in the basic building block depicted in Fig. 1 learn\\nthe predictive decomposition of the partial forecast ˆyℓ in the basis Vf\\nℓ learned by the network. Matrix\\nVf'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='the predictive decomposition of the partial forecast ˆyℓ in the basis Vf\\nℓ learned by the network. Matrix\\nVf\\nℓ has dimensionality H ×dim(θ f\\nℓ ). Therefore, the ﬁrst dimension of Vf\\nℓ has the interpretation of\\ndiscrete time index in the forecast domain. The second dimension of the matrix has the interpretation\\nof the indices of the basis functions, with θ f\\nℓ being the expansion coefﬁcients for this basis. Thus the\\ncolumns of Vf\\nℓ can be thought of as waveforms in the time domain. Because no additional constraints\\nare imposed on the form of Vf\\nℓ , the waveforms learned by the deep model do not have inherent\\nstructure (and none is apparent in our experiments). This leads to ˆyℓ not being interpretable.\\nThe interpretable architecturecan be constructed by reusing the overall architectural approach in\\nFig. 1 and by adding structure to basis layers at stack level. Forecasting practitioners often use the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Fig. 1 and by adding structure to basis layers at stack level. Forecasting practitioners often use the\\ndecomposition of time series into trend and seasonality, such as those performed by theSTL (Cleveland\\net al., 1990) and X13-ARIMA (U.S. Census Bureau, 2013). We propose to design the trend and\\nseasonality decomposition into the model to make the stack outputs more easily interpretable. Note\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nthat for the generic model the notion of stack was not necessary and the stack level indexing was\\nomitted for clarity. Now we will consider both stack level and block level indexing. For example, ˆys,ℓ\\nwill denote the partial forecast of block ℓ within stack s.\\nTrend model.A typical characteristic of trend is that most of the time it is a monotonic function, or\\nat least a slowly varying function. In order to mimic this behaviour we propose to constrain gb\\ns,ℓ and\\ngf\\ns,ℓ to be a polynomial of small degree p, a function slowly varying across forecast window:\\nˆys,ℓ =\\np\\n∑\\ni=0\\nθ f\\ns,ℓ,iti. (2)\\nHere time vector t = [0,1,2,..., H −2,H −1]T /H is deﬁned on a discrete grid running from 0 to\\n(H −1)/H, forecasting H steps ahead. Alternatively, the trend forecast in matrix form will then be:\\nˆytr\\ns,ℓ = Tθ f\\ns,ℓ,\\nwhere θ f\\ns,ℓ are polynomial coefﬁcients predicted by a FC network of layer ℓ of stack s described by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='ˆytr\\ns,ℓ = Tθ f\\ns,ℓ,\\nwhere θ f\\ns,ℓ are polynomial coefﬁcients predicted by a FC network of layer ℓ of stack s described by\\nequations (1); and T = [1,t,..., tp] is the matrix of powers of t. If p is low, e.g. 2 or 3, it forces ˆytr\\ns,ℓ\\nto mimic trend.\\nSeasonality model. Typical characteristic of seasonality is that it is a regular, cyclical, recurring\\nﬂuctuation. Therefore, to model seasonality, we propose to constrain gb\\ns,ℓ and gf\\ns,ℓ to belong to the\\nclass of periodic functions, i.e. yt = yt−∆, where ∆ is a seasonality period. A natural choice for the\\nbasis to model periodic function is the Fourier series:\\nˆys,ℓ =\\n⌊H/2−1⌋\\n∑\\ni=0\\nθ f\\ns,ℓ,i cos(2πit)+ θ f\\ns,ℓ,i+⌊H/2⌋sin(2πit), (3)\\nThe seasonality forecast will then have the matrix form as follows:\\nˆyseas\\ns,ℓ = Sθ f\\ns,ℓ,\\nwhere θ f\\ns,ℓ are Fourier coefﬁcients predicted by a FC network of layer ℓ of stack s described by\\nequations (1); and S = [1,cos(2πt),... cos(2π⌊H/2−1⌋t)),sin(2πt),..., sin(2π⌊H/2−1⌋t))] is the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='s,ℓ,\\nwhere θ f\\ns,ℓ are Fourier coefﬁcients predicted by a FC network of layer ℓ of stack s described by\\nequations (1); and S = [1,cos(2πt),... cos(2π⌊H/2−1⌋t)),sin(2πt),..., sin(2π⌊H/2−1⌋t))] is the\\nmatrix of sinusoidal waveforms. The forecast ˆyseas\\ns,ℓ is then a periodic function mimicking typical\\nseasonal patterns.\\nThe overall interpretable architecture consists of two stacks: the trend stack is followed by the\\nseasonality stack. The doubly residual stacking combined with the forecast/backcast principle result\\nin (i) the trend component being removed from the input windowx before it is fed into the seasonality\\nstack and (ii) the partial forecasts of trend and seasonality are available as separate interpretable\\noutputs. Structurally, each of the stacks consists of several blocks connected with residual connections\\nas depicted in Fig. 1 and each of them shares its respective, non-learnable gb\\ns,ℓ and gf\\ns,ℓ. The number'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='as depicted in Fig. 1 and each of them shares its respective, non-learnable gb\\ns,ℓ and gf\\ns,ℓ. The number\\nof blocks is 3 for both trend and seasonality. We found that on top of sharing gb\\ns,ℓ and gf\\ns,ℓ, sharing all\\nthe weights across blocks in a stack resulted in better validation performance.\\n3.4 E NSEMBLING\\nEnsembling is used by all the top entries in the M4-competition. We rely on ensembling as well\\nto be comparable. We found that ensembling is a much more powerful regularization technique\\nthan the popular alternatives, e.g. dropout or L2-norm penalty. The addition of those methods\\nimproved individual models, but was hurting the performance of the ensemble. The core property of\\nan ensemble is diversity. We build an ensemble using several sources of diversity. First, the ensemble\\nmodels are ﬁt on three different metrics: sMAPE ,MASE and MAPE , a version of sMAPE that has only\\nthe ground truth value in the denominator. Second, for every horizonH, individual models are trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='models are ﬁt on three different metrics: sMAPE ,MASE and MAPE , a version of sMAPE that has only\\nthe ground truth value in the denominator. Second, for every horizonH, individual models are trained\\non input windows of different length: 2H,3H,..., 7H, for a total of six window lengths. Thus the\\noverall ensemble exhibits a multi-scale aspect. Finally, we perform a bagging procedure (Breiman,\\n1996) by including models trained with different random initializations. We use 180 total models to\\nreport results on the test set (please refer to Appendix B for the ablation of ensemble size). We use\\nthe median as ensemble aggregation function.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 1: Performance on the M4, M3, TOURISM test sets, aggregated over each dataset. Evaluation\\nmetrics are speciﬁed for each dataset; lower values are better. The number of time series in each\\ndataset is provided in brackets.\\nM4 Average (100,000) M3 Average (3,003) TOURISM Average (1,311)\\nsMAPE OWA sMAPE MAPE\\nPure ML 12.894 0.915 Comb S-H-D 13.52 ETS 20.88\\nStatistical 11.986 0.861 ForecastPro 13.19 Theta 20.88\\nProLogistica 11.845 0.841 Theta 13.01 ForePro 19.84\\nML/TS combination 11.720 0.838 DOTM 12.90 Stratometrics 19.52\\nDL/TS hybrid 11.374 0.821 EXP 12.71 LeeCBaker 19.35\\nN-BEATS-G 11.168 0.797 12.47 18.47\\nN-BEATS-I 11.174 0.798 12.43 18.97\\nN-BEATS-I+G 11.135 0.795 12.37 18.52\\n4 R ELATED WORK\\nThe approaches to TS forecasting can be split in a few distinct categories. The statistical model-\\ning approaches based on exponential smoothing and its different ﬂavors are well established and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='The approaches to TS forecasting can be split in a few distinct categories. The statistical model-\\ning approaches based on exponential smoothing and its different ﬂavors are well established and\\nare often considered a default choice in the industry (Holt, 1957; 2004; Winters, 1960). More\\nadvanced variations of exponential smoothing include the winner of M3 competition, the Theta\\nmethod (Assimakopoulos & Nikolopoulos, 2000) that decomposes the forecast into several theta-lines\\nand statistically combines them. The pinnacle of the statistical approach encapsulates ARIMA,\\nauto-ARIMA and in general, the uniﬁed state-space modeling approach, that can be used to ex-\\nplain and analyze all of the approaches mentioned above (see Hyndman & Khandakar (2008) for\\nan overview). More recently, ML/TS combination approaches started inﬁltrating the domain with\\ngreat success, showing promising results by using the outputs of statistical engines as features. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='an overview). More recently, ML/TS combination approaches started inﬁltrating the domain with\\ngreat success, showing promising results by using the outputs of statistical engines as features. In\\nfact, 2 out of top-5 entries in the M4 competition are approaches of this type, including the second\\nentry (Montero-Manso et al., 2019). The second entry computes the outputs of several statistical\\nmethods on the M4 dataset and combines them using gradient boosted tree (Chen & Guestrin, 2016).\\nSomewhat independently, the work in the modern deep learning TS forecasting developed based on\\nvariations of recurrent neural networks (Flunkert et al., 2017; Rangapuram et al., 2018b; Toubeau\\net al., 2019; Zia & Razzaq, 2018) being largely dominated by the electricity load forecasting in the\\nmulti-variate setup. A few earlier works explored the combinations of recurrent neural networks\\nwith dilation, residual connections and attention (Chang et al., 2017; Kim et al., 2017; Qin et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='multi-variate setup. A few earlier works explored the combinations of recurrent neural networks\\nwith dilation, residual connections and attention (Chang et al., 2017; Kim et al., 2017; Qin et al.,\\n2017). These served as a basis for the winner of the M4 competition (Smyl, 2020). The winning\\nentry combines a Holt-Winters style seasonality model with its parameters ﬁtted to a given TS via\\ngradient descent and a unique combination of dilation/residual/attention approaches for each forecast\\nhorizon. The resulting model is a hybrid model that architecturally heavily relies on a time-series\\nengine. It is hand crafted to each speciﬁc horizon of M4, making this approach hard to generalize to\\nother datasets.\\n5 E XPERIMENTAL RESULTS\\nOur key empirical results based on aggregate performance metrics over several datasets—M4 (M4\\nTeam, 2018b; Makridakis et al., 2018b), M3 (Makridakis & Hibon, 2000; Makridakis et al., 2018a)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Our key empirical results based on aggregate performance metrics over several datasets—M4 (M4\\nTeam, 2018b; Makridakis et al., 2018b), M3 (Makridakis & Hibon, 2000; Makridakis et al., 2018a)\\nand TOURISM (Athanasopoulos et al., 2011)—appear in Table 1. More detailed descriptions of the\\ndatasets are provided in Section 5.1 and Appendix A. For each dataset, we compare our results with\\nbest 5 entries for this dataset reported in the literature, according to the customary metrics speciﬁc to\\neach dataset (M4: OWA and sMAPE , M3: sMAPE , TOURISM : MAPE ). More granular dataset-speciﬁc\\nresults with data splits over forecast horizons and types of time series appear in respective appendices\\n(M4: Appendix C.1; M3: Appendix C.2; TOURISM : Appendix C.3).\\nIn Table 1, we study the performance of two N-BEATS conﬁgurations: generic (N-BEATS-G) and\\ninterpretable (N-BEATS-I), as well as N-BEATS-I+G (ensemble of all models from N-BEATS-G and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='In Table 1, we study the performance of two N-BEATS conﬁgurations: generic (N-BEATS-G) and\\ninterpretable (N-BEATS-I), as well as N-BEATS-I+G (ensemble of all models from N-BEATS-G and\\nN-BEATS-I).On M4 dataset, we compare against 5 representatives from the M4 competition (Makri-\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\ndakis et al., 2018b): each best in their respective model class. Pure ML is the submission by B. Trotta,\\nthe best entry among the 6 pure ML models. Statistical is the best pure statistical model by N.Z.\\nLegaki and K. Koutsouri. ML/TS combination is the model by P. Montero-Manso, T. Talagala, R.J.\\nHyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few statistical time\\nseries models. ProLogistica is the third entry in M4 based on the weighted ensemble of statistical\\nmethods. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020). On the M3 dataset,\\nwe compare against the Theta method (Assimakopoulos & Nikolopoulos, 2000), the winner of M3;\\nDOTA, a dynamically optimized Theta model (Fiorucci et al., 2016); EXP, the most resent statistical\\napproach and the previous state-of-the-art on M3 (Spiliotis et al., 2019); as well as ForecastPro, an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='approach and the previous state-of-the-art on M3 (Spiliotis et al., 2019); as well as ForecastPro, an\\noff-the-shelf forecasting software that is based on model selection between exponential smoothing,\\nARIMA and moving average (Athanasopoulos et al., 2011; Assimakopoulos & Nikolopoulos, 2000).\\nOn theTOURISM dataset, we compare against 3 statistical benchmarks (Athanasopoulos et al.,\\n2011): ETS, exponential smoothing with cross-validated additive/multiplicative model;Theta method;\\nForePro, same as ForecastProin M3; as well as top 2 entries from the TOURISM Kaggle competi-\\ntion (Athanasopoulos & Hyndman, 2011): Stratometrics, an unknown technique; LeeCBaker (Baker\\n& Howard, 2011), a weighted combination of Naïve, linear trend model, and exponentially weighted\\nleast squares regression trend.\\nAccording to Table 1, N-BEATS demonstrates state-of-the-art performance on three challenging\\nnon-overlapping datasets containing time series from very different domains, sampling frequencies'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='According to Table 1, N-BEATS demonstrates state-of-the-art performance on three challenging\\nnon-overlapping datasets containing time series from very different domains, sampling frequencies\\nand seasonalities. As an example, on M4 dataset, the OWA gap between N-BEATS and the M4\\nwinner (0.821 −0.795 = 0.026) is greater than the gap between the M4 winner and the second entry\\n(0.838 −0.821 = 0.017). Generic N-BEATS model uses as little prior knowledge as possible, with\\nno feature engineering, no scaling and no internal architectural components that may be considered\\nTS-speciﬁc. Thus the result in Table 1 leads us to the conclusion that DL does not need support\\nfrom the statistical approaches or hand-crafted feature engineering and domain knowledge to perform\\nextremely well on a wide array of TS forecasting tasks. On top of that, the proposed general\\narchitecture performs very well on three different datasets outperforming a wide variety of models,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='extremely well on a wide array of TS forecasting tasks. On top of that, the proposed general\\narchitecture performs very well on three different datasets outperforming a wide variety of models,\\nboth generic and manually crafted to respective dataset, including the winner of M4, a model\\narchitecturally adjusted by hand to each forecast-horizon subset of the M4 data.\\n5.1 D ATASETS\\nM4 (M4 Team, 2018b; Makridakis et al., 2018b) is the latest in an inﬂuential series of forecasting\\ncompetitions organized by Spyros Makridakis since 1982 (Makridakis et al., 1982). The 100k-series\\ndataset is large and diverse, consisting of data frequently encountered in business, ﬁnancial and\\neconomic forecasting, and sampling frequencies ranging from hourly to yearly. A table with summary\\nstatistics is presented in Appendix A.1, showing wide variability in TS characteristics.\\nM3 (Makridakis & Hibon, 2000) is similar in its composition to M4, but has a smaller overall scale'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='statistics is presented in Appendix A.1, showing wide variability in TS characteristics.\\nM3 (Makridakis & Hibon, 2000) is similar in its composition to M4, but has a smaller overall scale\\n(3003 time series total vs. 100k in M4). A table with summary statistics is presented in Appendix A.2.\\nOver the past 20 years, this dataset has supported signiﬁcant efforts in the design of more optimal\\nstatistical models, e.g. Theta and its variants (Assimakopoulos & Nikolopoulos, 2000; Fiorucci et al.,\\n2016; Spiliotis et al., 2019). Furthermore, a recent publication (Makridakis et al., 2018a) based on a\\nsubset of M3 presented evidence that ML models are inferior to the classical statistical models.\\nTOURISM (Athanasopoulos et al., 2011) dataset was released as part of the respective Kaggle\\ncompetition conducted by Athanasopoulos & Hyndman (2011). The data include monthly, quarterly\\nand yearly series supplied by both governmental tourism organizations (e.g. Tourism Australia, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='competition conducted by Athanasopoulos & Hyndman (2011). The data include monthly, quarterly\\nand yearly series supplied by both governmental tourism organizations (e.g. Tourism Australia, the\\nHong Kong Tourism Board and Tourism New Zealand) as well as various academics, who had used\\nthem in previous studies. A table with summary statistics is presented in Appendix A.3.\\n5.2 T RAINING METHODOLOGY\\nWe split each dataset into train, validation and test subsets. The test subset is the standard test set\\npreviously deﬁned for each dataset (M4 Team, 2018a; Makridakis & Hibon, 2000; Athanasopoulos\\net al., 2011). The validation and train subsets for each dataset are obtained by splitting their full train\\nsets at the boundary of the last horizon of each time series. We use the train and validation subsets to\\ntune hyperparameters. Once the hyperparameters are determined, we train the model on the full train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='tune hyperparameters. Once the hyperparameters are determined, we train the model on the full train\\nset and report results on the test set. Please refer to Appendix D for detailed hyperparameter settings\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nat the block level. N-BEATS is implemented and trained in Tensorﬂow (Abadi et al., 2015). We\\nshare parameters of the network across horizons, therefore we train one model per horizon for each\\ndataset. If every time series is interpreted as a separate task, this can be linked back to the multitask\\nlearning and furthermore to meta-learning (see discussion in Section 6), in which a neural network\\nis regularized by learning on multiple tasks to improve generalization. We would like to stress that\\nmodels for different horizons and datasets reuse the same architecture. Architectural hyperparameters\\n(width, number of layers, number of stacks, etc.) are ﬁxed to the same values across horizons and\\nacross datasets (see Appendix D). The fact that we can reuse architecture and even hyperparameters\\nacross horizons indicates that the proposed architecture design generalizes well across time series of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='across horizons indicates that the proposed architecture design generalizes well across time series of\\ndifferent nature. The same architecture is successfully trained on the M4 Monthly subset with 48k\\ntime series and the M3 Others subset with 174 time series. This is a much stronger result than e.g. the\\nresult of S. Smyl (Makridakis et al., 2018b) who had to use very different architectures hand crafted\\nfor different horizons.\\nTo update network parameters for one horizon, we sample train batches of ﬁxed size 1024. We pick\\n1024 TS ids from this horizon, uniformly at random with replacement. For each selected TS id we\\npick a random forecast point from the historical range of length LH immediately preceding the last\\npoint in the train part of the TS. LH is a cross-validated hyperparameter. We observed that for subsets\\nwith large number of time series it tends to be smaller and for subsets with smaller number of time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='with large number of time series it tends to be smaller and for subsets with smaller number of time\\nseries it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4LH is\\nequal to 1.5; and in moderate to small Weekly, Daily, Hourly subsets of M4LH is equal to 10. Given\\na sampled forecast point, we set one horizon worth of points following it to be the target forecast\\nwindow y and we set the history of points of one of lengths 2H,3H,..., 7H preceding it to be the\\ninput x to the network. We use the Adam optimizer with default settings and initial learning rate\\n0.001. While optimising the ensemble members relying on the minimization of sMAPE metric, we\\nstop the gradient ﬂows in the denominator to make training numerically stable. The neural network\\ntraining is run with early stopping and the number of batches is determined on the validation set. The\\nGPU based training of one ensemble member for entire M4 dataset takes between 30 min and 2 hours'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='training is run with early stopping and the number of batches is determined on the validation set. The\\nGPU based training of one ensemble member for entire M4 dataset takes between 30 min and 2 hours\\ndepending on neural network settings and hardware.\\n5.3 I NTERPRETABILITY RESULTS\\nFig. 2 studies the outputs of the proposed model in the generic and the interpretable conﬁgurations.\\nAs discussed in Section 3.3, to make the generic architecture presented in Fig. 1 interpretable, we\\nconstrain gθ in the ﬁrst stack to have the form of polynomial (2) while the second one has the form\\nof Fourier basis (3). Furthermore, we use the outputs of the generic conﬁguration of N-BEATS as\\ncontrol group (the generic model of 30 residual blocks depicted in Fig. 1 is divided into two stacks)\\nand we plot both generic (sufﬁx “-G”) and interpretable (sufﬁx “-I”) stack outputs side by side in\\nFig. 2. The outputs of generic model are arbitrary and non-interpretable: either trend or seasonality'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Fig. 2. The outputs of generic model are arbitrary and non-interpretable: either trend or seasonality\\nor both of them are present at the output of both stacks. The magnitude of the output (peak-to-peak)\\nis generally smaller at the output of the second stack. The outputs of the interpretable model exhibit\\ndistinct properties: the trend output is monotonic and slowly moving, the seasonality output is\\nregular, cyclical and has recurring ﬂuctuations. The peak-to-peak magnitude of the seasonality output\\nis signiﬁcantly larger than that of the trend, if signiﬁcant seasonality is present in the time series.\\nSimilarly, the peak-to-peak magnitude of trend output tends to be small when no obvious trend\\nis present in the ground truth signal. Thus the proposed interpretable architecture decomposes its\\nforecast into two distinct components. Our conclusion is that the outputs of the DL model can be\\nmade interpretable by encoding a sensible inductive bias in the architecture. Table 1 conﬁrms that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='forecast into two distinct components. Our conclusion is that the outputs of the DL model can be\\nmade interpretable by encoding a sensible inductive bias in the architecture. Table 1 conﬁrms that\\nthis does not result in performance drop.\\n6 D ISCUSSION : C ONNECTIONS TO META-LEARNING\\nMeta-learning deﬁnes an inner learning procedure and an outer learning procedure. The inner\\nlearning procedure is parameterized, conditioned or otherwise inﬂuenced by the outer learning\\nprocedure (Bengio et al., 1991). The prototypical inner vs. outer learning is individual learning in\\nthe lifetime of an animal vs. evolution of the inner learning procedure itself over many generations\\nof individuals. To see the two levels, it often helps to refer to two sets of parameters, the inner\\nparameters (e.g. synaptic weights) which are modiﬁed inside the inner learning procedure, and the\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n0 1 2 3 4 5\\nt\\n0.8\\n0.9\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90 STACK1-G\\n0 1 2 3 4 5\\nt\\n0.025\\n0.050\\n0.075 STACK2-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90\\n0.95\\nSTACK1-I\\n0 1 2 3 4 5\\nt\\n0.02\\n0.03\\n0.04\\n0.05 STACK2-I\\n0 2 4 6\\nt\\n0.85\\n0.90\\n0.95\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6\\nt\\n0.86\\n0.88\\n0.90 STACK1-G\\n0 2 4 6\\nt\\n0.025\\n0.000\\n0.025\\n0.050\\nSTACK2-G\\n0 2 4 6\\nt\\n0.88\\n0.89\\n0.90\\nSTACK1-I\\n0 2 4 6\\nt\\n0.05\\n0.00\\n0.05\\nSTACK2-I\\n0 5 10 15\\nt\\n0.4\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 5 10 15\\nt\\n0.8\\n0.9 STACK1-G\\n0 5 10 15\\nt\\n0.1\\n0.0\\nSTACK2-G\\n0 5 10 15\\nt\\n0.85\\n0.90\\nSTACK1-I\\n0 5 10 15\\nt\\n0.3\\n0.2\\n0.1\\n0.0 STACK2-I\\n0 2 4 6 8 10 12\\nt\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80\\nSTACK1-G\\n0 2 4 6 8 10 12\\nt\\n0.000\\n0.025\\n0.050\\n0.075\\nSTACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='STACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.002\\n0.001\\n STACK2-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.0003\\n0.0002\\n0.0001\\n STACK2-I\\n0 10 20 30 40\\nt\\n0.25\\n0.50\\n0.75\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n(a) Combined\\n0 10 20 30 40\\nt\\n0.2\\n0.4\\n0.6\\nSTACK1-G (b) Stack1-G\\n0 10 20 30 40\\nt\\n0.02\\n0.00\\nSTACK2-G (c) Stack2-G\\n0 10 20 30 40\\nt\\n0.36\\n0.38\\n0.40\\nSTACK1-I (d) StackT-I\\n0 10 20 30 40\\nt\\n0.2\\n0.0\\n0.2\\nSTACK2-I (e) StackS-I\\nFigure 2: The outputs of generic and the interpretable conﬁgurations, M4 dataset. Each row is one\\ntime series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='time series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\\nare normalized by the maximal value of the actual time series for convenience. Column (a) shows the\\nactual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\\nforecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\\nrespectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\\nand the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\\nouter parameters or meta-parameters (e.g. genes) which get modiﬁed only in the outer learning\\nprocedure.\\nN-BEATS can be cast as an instance of meta-learning by drawing the following parallels. The outer\\nlearning procedure is encapsulated in the parameters of the whole network, learned by gradient'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='N-BEATS can be cast as an instance of meta-learning by drawing the following parallels. The outer\\nlearning procedure is encapsulated in the parameters of the whole network, learned by gradient\\ndescent. The inner learning procedure is encapsulated in the set of basic building blocks and modiﬁes\\nthe expansion coefﬁcients θ f that basis gf takes as inputs. The inner learning proceeds through a\\nsequence of stages, each corresponding to a block within the stack of the architecture. Each of the\\nblocks can be thought of as performing the equivalent of an update step which gradually modiﬁes\\nthe expansion coefﬁcients θ f which eventually feed into gf in each block (which get added together\\nto form the ﬁnal prediction). The inner learning procedure takes a single history from a piece of a\\nTS and sees that history as a training set. It produces forward expansion coefﬁcients θ f (see Fig. 1),\\nwhich parametrically map inputs to predictions. In addition, each preceding block modiﬁes the input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='which parametrically map inputs to predictions. In addition, each preceding block modiﬁes the input\\nto the next block by producing backward expansion coefﬁcients θb, thus conditioning the learning\\nand the output of the next block. In the case of the interpretable model, the meta-parameters are only\\nin the FC layers because the gf ’s are ﬁxed. In the case of the generic model, the meta-parameters\\nalso include the V’s which deﬁne thegf non-parametrically. This point of view is further reinforced\\nby the results of the ablation study reported in Appendix B showing that increasing the number of\\nblocks in the stack, as well as the number of stacks improves generalization performance, and can be\\ninterpreted as more iterations of the inner learning procedure.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n7 C ONCLUSIONS\\nWe proposed and empirically validated a novel architecture for univariate TS forecasting. We showed\\nthat the architecture is general, ﬂexible and it performs well on a wide array of TS forecasting prob-\\nlems. We applied it to three non-overlapping challenging competition datasets: M4, M3 andTOURISM\\nand demonstrated state-of-the-art performance in two conﬁgurations: generic and interpretable. This\\nallowed us to validate two important hypotheses: (i) the generic DL approach performs exceptionally\\nwell on heterogeneous univariate TS forecasting problems using no TS domain knowledge, (ii) it is\\nviable to additionally constrain a DL model to force it to decompose its forecast into distinct human\\ninterpretable outputs. We also demonstrated that the DL models can be trained on multiple time series\\nin a multi-task fashion, successfully transferring and sharing individual learnings. We speculate that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='in a multi-task fashion, successfully transferring and sharing individual learnings. We speculate that\\nN-BEATS’s performance can be attributed in part to it carrying out a form of meta-learning, a deeper\\ninvestigation of which should be the subject of future work.\\nREFERENCES\\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\\nKudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike\\nSchuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent\\nVanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,\\nMartin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,\\nMartin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\\non heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from\\ntensorﬂow.org.\\nV . Assimakopoulos and K. Nikolopoulos. The theta model: a decomposition approach to forecasting.\\nInternational Journal of Forecasting, 16(4):521–530, 2000.\\nGeorge Athanasopoulos and Rob J. Hyndman. The value of feedback in forecasting competitions.\\nInternational Journal of Forecasting, 27(3):845–849, 2011.\\nGeorge Athanasopoulos, Rob J. Hyndman, Haiyan Song, and Doris C. Wu. The tourism forecasting\\ncompetition. International Journal of Forecasting, 27(3):822–844, 2011.\\nLee C. Baker and Jeremy Howard. Winning methods for forecasting tourism time series.International\\nJournal of Forecasting, 27(3):850–852, 2011.\\nYoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. InProceedings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Journal of Forecasting, 27(3):850–852, 2011.\\nYoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. InProceedings\\nof the International Joint Conference on Neural Networks, pp. II–A969, Seattle, USA, 1991.\\nChristoph Bergmeir, Rob J. Hyndman, and José M. Benítez. Bagging exponential smoothing methods\\nusing STL decomposition and Box–Cox transformation. International Journal of Forecasting, 32\\n(2):303–312, 2016.\\nLeo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, Aug 1996.\\nPhil Brierley. Winning methods for forecasting seasonal tourism time series. International Journal of\\nForecasting, 27(3):853–854, 2011.\\nShiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael\\nWitbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.\\nIn NIPS, pp. 77–87, 2017.\\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In ACM SIGKDD, pp.\\n785–794, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='In NIPS, pp. 77–87, 2017.\\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In ACM SIGKDD, pp.\\n785–794, 2016.\\nRobert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning. STL: A seasonal-\\ntrend decomposition procedure based on Loess (with discussion). Journal of Ofﬁcial Statistics, 6:\\n3–73, 1990.\\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.\\nuci.edu/ml.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, and Anne B. Koehler.\\nModels for optimising the Theta method and their relationship to state space models. International\\nJournal of Forecasting, 32(4):1151–1161, 2016.\\nValentin Flunkert, David Salinas, and Jan Gasthaus. DeepAR: Probabilistic forecasting with autore-\\ngressive recurrent networks. CoRR, abs/1704.04110, 2017.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In CVPR, pp. 770–778. IEEE Computer Society, 2016.\\nC. C. Holt. Forecasting trends and seasonals by exponentially weighted averages. Technical Report\\nONR memorandum no. 5, Carnegie Institute of Technology, Pittsburgh, PA, 1957.\\nCharles C. Holt. Forecasting seasonals and trends by exponentially weighted moving averages.\\nInternational Journal of Forecasting, 20(1):5–10, 2004.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Charles C. Holt. Forecasting seasonals and trends by exponentially weighted moving averages.\\nInternational Journal of Forecasting, 20(1):5–10, 2004.\\nGao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected\\nconvolutional networks. In CVPR, pp. 2261–2269. IEEE Computer Society, 2017.\\nRob Hyndman and Anne B. Koehler. Another look at measures of forecast accuracy. International\\nJournal of Forecasting, 22(4):679–688, 2006.\\nRob J Hyndman and Yeasmin Khandakar. Automatic time series forecasting: the forecast package\\nfor R. Journal of Statistical Software, 26(3):1–22, 2008.\\nChaman L. Jain. Answers to your forecasting questions. Journal of Business Forecasting, 36, Spring\\n2017.\\nKenneth B. Kahn. How to measure the impact of a forecast error on an enterprise? The Journal of\\nBusiness Forecasting Methods & Systems, 22(1), Spring 2003.\\nJaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. Residual lstm: Design of a deep recurrent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Business Forecasting Methods & Systems, 22(1), Spring 2003.\\nJaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. Residual lstm: Design of a deep recurrent\\narchitecture for distant speech recognition. In Interspeech 2017, pp. 1591–1595, 2017.\\nM4 Team. M4 dataset, 2018a. URLhttps://github.com/M4Competition/M4-methods/tree/\\nmaster/Dataset.\\nM4 Team. M4 competitor’s guide: prizes and rules, 2018b. URL www.m4.unic.ac.cy/\\nwp-content/uploads/2018/03/M4-CompetitorsGuide.pdf.\\nS Makridakis, E Spiliotis, and V Assimakopoulos. Statistical and machine learning forecasting\\nmethods: Concerns and ways forward. PLoS ONE, 13(3), 2018a.\\nSpyros Makridakis and Michèle Hibon. The M3-Competition: results, conclusions and implications.\\nInternational Journal of Forecasting, 16(4):451–476, 2000.\\nSpyros Makridakis, A Andersen, Robert Carbone, Robert Fildes, Michele Hibon, Rudolf\\nLewandowski, Joseph Newton, Emanuel Parzen, and Robert Winkler. The accuracy of extrapo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Spyros Makridakis, A Andersen, Robert Carbone, Robert Fildes, Michele Hibon, Rudolf\\nLewandowski, Joseph Newton, Emanuel Parzen, and Robert Winkler. The accuracy of extrapo-\\nlation (time series) methods: Results of a forecasting competition. Journal of forecasting, 1(2):\\n111–153, 1982.\\nSpyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The M4-Competition:\\nResults, ﬁndings, conclusion and way forward. International Journal of Forecasting , 34(4):\\n802–808, 2018b.\\nPablo Montero-Manso, George Athanasopoulos, Rob J Hyndman, and Thiyanga S Talagala.\\nFFORMA: Feature-based Forecast Model Averaging. International Journal of Forecasting, 2019.\\nto appear.\\nVinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\\nICML, pp. 807–814, 2010.\\nYao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison W. Cottrell. A\\ndual-stage attention-based recurrent neural network for time series prediction. In IJCAI-17, pp.\\n2627–2633, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='dual-stage attention-based recurrent neural network for time series prediction. In IJCAI-17, pp.\\n2627–2633, 2017.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 11, 'page_label': '12', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nSyama Sundar Rangapuram, Matthias Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim\\nJanuschowski. Deep state space models for time series forecasting. In NeurIPS, 2018a.\\nSyama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and\\nTim Januschowski. Deep state space models for time series forecasting. In NeurIPS 31, pp.\\n7785–7794, 2018b.\\nSlawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time\\nseries forecasting. International Journal of Forecasting, 36(1):75 – 85, 2020.\\nSlawek Smyl and Karthik Kuber. Data preprocessing and augmentation for multiple short time series\\nforecasting with recurrent neural networks. In 36th International Symposium on Forecasting, 2016.\\nEvangelos Spiliotis, Vassilios Assimakopoulos, and Konstantinos Nikolopoulos. Forecasting with a\\nhybrid method utilizing data smoothing, a variation of the theta method and shrinkage of seasonal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 11, 'page_label': '12', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Evangelos Spiliotis, Vassilios Assimakopoulos, and Konstantinos Nikolopoulos. Forecasting with a\\nhybrid method utilizing data smoothing, a variation of the theta method and shrinkage of seasonal\\nfactors. International Journal of Production Economics, 209:92–102, 2019.\\nA. A. Syntetos, J. E. Boylan, and J. D. Croston. On the categorization of demand patterns. Journal of\\nthe Operational Research Society, 56(5):495–503, 2005.\\nJ. Toubeau, J. Bottieau, F. Vallée, and Z. De Grève. Deep learning-based multivariate probabilistic\\nforecasting for short-term scheduling in power markets. IEEE Transactions on Power Systems, 34\\n(2):1203–1215, March 2019.\\nU.S. Census Bureau. Reference manual for the X-13ARIMA-SEATS Program, version 1.0, 2013.\\nURL http://www.census.gov/ts/x13as/docX13AS.pdf.\\nYuyang Wang, Alex Smola, Danielle C. Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.\\nDeep factors for forecasting. In ICML, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 11, 'page_label': '12', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='URL http://www.census.gov/ts/x13as/docX13AS.pdf.\\nYuyang Wang, Alex Smola, Danielle C. Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.\\nDeep factors for forecasting. In ICML, 2019.\\nPeter R. Winters. Forecasting sales by exponentially weighted moving averages. Management\\nScience, 6(3):324–342, 1960.\\nHsiang-Fu Yu, Nikhil Rao, and Inderjit S. Dhillon. Temporal regularized matrix factorization for\\nhigh-dimensional time series prediction. In NIPS, 2016.\\nTehseen Zia and Saad Razzaq. Residual recurrent highway networks for learning deep sequence\\nprediction models. Journal of Grid Computing, Jun 2018.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 2: Composition of the M4 dataset: the number of time series based on their sampling frequency\\nand type.\\nFrequency / Horizon\\nType Yearly/6 Qtly/8 Monthly/18 Wkly/13 Daily/14 Hrly/48 Total\\nDemographic 1,088 1,858 5,728 24 10 0 8,708\\nFinance 6,519 5,305 10,987 164 1,559 0 24,534\\nIndustry 3,716 4,637 10,017 6 422 0 18,798\\nMacro 3,903 5,315 10,016 41 127 0 19,402\\nMicro 6,538 6,020 10,975 112 1,476 0 25,121\\nOther 1,236 865 277 12 633 414 3,437\\nTotal 23,000 24,000 48,000 359 4,227 414 100,000\\nMin. Length 19 24 60 93 107 748\\nMax. Length 841 874 2812 2610 9933 1008\\nMean Length 37.3 100.2 234.3 1035.0 2371.4 901.9\\nSD Length 24.5 51.1 137.4 707.1 1756.6 127.9\\n% Smooth 82% 89% 94% 84% 98% 83%\\n% Erratic 18% 11% 6% 16% 2% 17%\\nA D ATASET DETAILS\\nA.1 M4 D ATASET DETAILS\\nTable 2 outlines the composition of the M4 dataset across domains and forecast horizons by listing the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='% Erratic 18% 11% 6% 16% 2% 17%\\nA D ATASET DETAILS\\nA.1 M4 D ATASET DETAILS\\nTable 2 outlines the composition of the M4 dataset across domains and forecast horizons by listing the\\nnumber of time series based on their frequency and type (M4 Team, 2018b). The M4 dataset is large\\nand diverse: all forecast horizons are composed of heterogeneous time series types (with exception of\\nHourly) frequently encountered in business, ﬁnancial and economic forecasting. Summary statistics\\non series lengths are also listed, showing wide variability therein, as well as a characterization (smooth\\nvs erratic) that follows Syntetos et al. (2005), and is based on the squared coefﬁcient of variation of\\nthe series. All series have positive observed values at all time-steps; as such, none can be considered\\nintermittent or lumpy per Syntetos et al. (2005).\\nA.2 M3 D ATASET DETAILS\\nTable 3 outlines the composition of the M3 dataset across domains and forecast horizons by listing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='intermittent or lumpy per Syntetos et al. (2005).\\nA.2 M3 D ATASET DETAILS\\nTable 3 outlines the composition of the M3 dataset across domains and forecast horizons by listing\\nthe number of time series based on their frequency and type (Makridakis & Hibon, 2000). The\\nM3 is smaller than the M4, but it is still large and diverse: all forecast horizons are composed\\nof heterogeneous time series types frequently encountered in business, ﬁnancial and economic\\nforecasting. Summary statistics on series lengths are also listed, showing wide variability in length,\\nas well as a characterization ( smooth vs erratic) that follows Syntetos et al. (2005), and is based\\non the squared coefﬁcient of variation of the series. All series have positive observed values at all\\ntime-steps; as such, none can be considered intermittent or lumpy per Syntetos et al. (2005).\\nA.3 TOURISM DATASET DETAILS\\nTable 4 outlines the composition of the TOURISM dataset across forecast horizons by listing the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='A.3 TOURISM DATASET DETAILS\\nTable 4 outlines the composition of the TOURISM dataset across forecast horizons by listing the\\nnumber of time series based on their frequency. Summary statistics on series lengths are listed,\\nshowing wide variability in length. All series have positive observed values at all time-steps. In\\ncontrast to M4 and M3 datasets, TOURISM includes a much higher fraction of erratic series.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 13, 'page_label': '14', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 3: Composition of the M3 dataset: the number of time series based on their sampling frequency\\nand type.\\nFrequency / Horizon\\nType Yearly/6 Quarterly/8 Monthly/18 Other/8 Total\\nDemographic 245 57 111 0 413\\nFinance 58 76 145 29 308\\nIndustry 102 83 334 0 519\\nMacro 83 336 312 0 731\\nMicro 146 204 474 4 828\\nOther 11 0 52 141 204\\nTotal 645 756 1,428 174 3,003\\nMin. Length 20 24 66 71\\nMax. Length 47 72 144 104\\nMean Length 28.4 48.9 117.3 76.6\\nSD Length 9.9 10.6 28.5 10.9\\n% Smooth 90% 99% 98% 100%\\n% Erratic 10% 1% 2% 0%\\nTable 4: Composition of the TOURISM dataset: the number of time series based on their sampling\\nfrequency.\\nFrequency / Horizon\\nYearly/4 Quarterly/8 Monthly/24 Total\\n518 427 366 1,311\\nMin. Length 11 30 91\\nMax. Length 47 130 333\\nMean Length 24.4 99.6 298\\nSD Length 5.5 20.3 55.7\\n% Smooth 77% 61% 49%\\n% Erratic 23% 39% 51%\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 5: sMAPE on the validation set, generic ar-\\nchitecture. sMAPE for varying number of stacks,\\neach having one residual block.\\nStacks s MAPE\\n1 11.154\\n3 11.061\\n9 10.998\\n18 10.950\\n30 10.937\\nTable 6: sMAPE on the validation set, inter-\\npretable architecture. Ablation of the synergy\\nof the layers with different basis functions and\\nmulti-block stack gain.\\nDetrend Seasonality s MAPE\\n0 2 11.189\\n2 0 11.572\\n1 1 11.040\\n3 3 10.986\\nB A BLATION STUDIES\\nB.1 L AYER STACKING AND BASIS SYNERGY\\nWe performed an ablation study on the validation set, using sMAPE metric as performance criterion.\\nWe addressed two speciﬁc questions with this study. First, Is stacking layers helpful? Second, Does\\nthe architecture based on the combination of layers with different basis functions results in better\\nperformance than the architecture using only one layer type?\\nLayer stacking. We start our study with the generic architecture that consists of stacks of one'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='performance than the architecture using only one layer type?\\nLayer stacking. We start our study with the generic architecture that consists of stacks of one\\nresidual block of 5 FC layers each of the form Fig. 1 and we increase the number of stacks. Results\\npresented in Table 5 conﬁrm that increasing the number of stacks decreases error and at certain point\\nthe gain saturates. We would like to mention that the network having 30 stack of depth 5 is in fact a\\nvery deep network of total depth 150 layers.\\nBasis synergy. Stacking works well for the interpretable architecture as can be seen in Table 6\\ndepicting the results of ablating the interpretable architecture conﬁguration. Here we experiment\\nwith the architecture that is composed of 2 stacks, stack one is trend model and stack two is the\\nseasonality model. Each stack has variable number of residual blocks and each residual block has 5\\nFC layers. We found that this architecture works best when all weights are shared within stack. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='seasonality model. Each stack has variable number of residual blocks and each residual block has 5\\nFC layers. We found that this architecture works best when all weights are shared within stack. We\\nclearly see that increasing the number of layers improves performance. The largest network is 60\\nlayers deep. On top of that, we observe that the architecture that consists of stacks based on different\\nbasis functions wins over the architecture based on the same stack. It looks like chaining stacks of\\ndifferent nature results in synergistic effects. This is logical as function classes that can be modelled\\nby trend and seasonality stacks have small overlap.\\nB.2 E NSEMBLE SIZE\\nFigure 3 demonstrates that increasing the ensemble size results in improved performance. Most\\nimportantly, according to Figure 3, N-BEATS achieves state-of-the-art performance even if compara-\\ntively small ensemble size of 18 models is used. Therefore, computational efﬁciency of N-BEATS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='importantly, according to Figure 3, N-BEATS achieves state-of-the-art performance even if compara-\\ntively small ensemble size of 18 models is used. Therefore, computational efﬁciency of N-BEATS\\ncan be traded very effectively for performance and there is no over-reliance of the results on large\\nensemble size.\\nB.3 D OUBLY RESIDUAL STACKING\\nIn Section 3.2 we described the proposed doubly residual stacking (DRESS) principle, which is the\\ntopological foundation of N-BEATS. The topology is based on both (i) running a residual backcast\\nconnection and (ii) producing partial block-level forecasts that are further aggregated at stack and\\nmodel levels to produce the ﬁnal model-level forecast. In this section we conduct a study to conﬁrm\\nthe accuracy effectiveness of this topology compared to several alternatives. The methodology\\nunderlying this study is that we remove either the backcast or partial forecast links or both and track'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='the accuracy effectiveness of this topology compared to several alternatives. The methodology\\nunderlying this study is that we remove either the backcast or partial forecast links or both and track\\nhow this affects the forecasting metrics. We keep the number of parameters in the network for each\\nof the architectural alternatives ﬁxed by using the same number of layers in the network (we used\\ndefault hyperparameter settings reported in Table 18). The architectural alternatives are depicted in\\nFigure 4 and described in detail below.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 15, 'page_label': '16', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n18 36 90 180\\nEnsemble Size\\n0.797\\n0.798\\n0.799\\n0.800\\n0.801\\n0.802OWA\\nFigure 3: M4 test performance (OWA) as a function of ensemble size, based on N-BEATS-G. This\\nﬁgure shows that N-BEATS loses less than 0.5% in terms of OWA performance even if 10 times\\nsmaller ensemble size is used.\\nN-BEATS-DRESSis depicted in Fig. 4a. This is the default conﬁguration of N-BEATS using doubly\\nresidual stacking described in Section 3.2.\\nPARALLEL is depicted in Fig. 4b. This is the alternative where the backward residual connection is\\ndisabled and the overall model input is fed to every block. The blocks then forecast in parallel using\\nthe same input and their individual outputs are summed to make the ﬁnal forecast.\\nNO-RESIDUAL is depicted in Fig. 4c. This is the alternative where the backward residual connection\\nis disabled. Unlike PARALLEL, in this case the backcast forecast of the previous block is fed as input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 15, 'page_label': '16', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='is disabled. Unlike PARALLEL, in this case the backcast forecast of the previous block is fed as input\\nto the next block. Unlike the usual feed-forward network, in the NO-RESIDUAL architecture, each\\nblock makes a partial forecast and their individual outputs are summed to make the ﬁnal forecast.\\nLAST-FORWARDis depicted in Fig. 4d. This is the alternative where the backward residual\\nconnection is active, however the model level forecast is derived only from the last block. So, the\\npartial forward forecasts are disabled. This is the architecture that is closest to the classical residual\\nnetwork.\\nNO-RESIDUAL-LAST-FORWARDis depicted in Fig. 4f. This is the alternative where both\\nbackward residual and the partial forward connections are disabled. This is therefore a simple\\nfeed-forward network, but very deep.\\nThe quantitative ablation study results on the M4 dataset are reported in Tables 7–10. N-BEATS-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 15, 'page_label': '16', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='feed-forward network, but very deep.\\nThe quantitative ablation study results on the M4 dataset are reported in Tables 7–10. N-BEATS-\\nDRESS model is essentially N-BEATS model in this study. For this study we used ensemble size\\nof 18. Since the ensemble size is 18 for N-BEATS-DRESS, as opposed to 180 used for N-BEATS,\\nthe OWA metric reported in Table 9 for N-BEATS-DRESS is higher than the OWA reported for\\nN-BEATS-G in Table 12. Note that both results align well withOWA reported in Figure 3 for different\\nensemble sizes, as part of the ensemble size ablation conducted in Section B.2.\\nThe results presented in Tables 7–10 demonstrate that the doubly residual stacking topology provides\\na clear overall advantage over the alternative architectures in which either backcast residual links or\\nthe partial forward forecast links are disabled.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 16, 'page_label': '17', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input\\nStack output\\n(to next stack)\\n(a) N-BEATS-DRESS\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n+\\nStack output\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input (b) PARALLEL\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n+\\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input (c) NO-RESIDUAL\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\nGlobal forecast\\n(model output)Model Input\\n(d) LAST-FORW ARD\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\nGlobal forecast\\n(model output)Model Input (e) NO-RESIDUAL-LAST-\\nFORW ARD\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 16, 'page_label': '17', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Stack\\nforecast\\nStack 1\\nStack 2\\nStack M \\nGlobal forecast\\n(model output)Model Input (e) NO-RESIDUAL-LAST-\\nFORW ARD\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input\\nStack output\\n(to next stack)\\nModel Input\\n(f) RESIDUAL-INPUT\\nFigure 4: The architectural conﬁgurations used in the ablation study of the doubly residual stack.\\nSymbol ⋄denotes unconnected output.\\nTable 7: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\\nthe ensemble of 18 generic models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-G 13.279 9.558 12.510 3.691 11.538\\nNO-RESIDUAL-G 13.195 9.555 12.451 3.759 11.493\\nLAST-FORW ARD-G 13.200 9.322 12.352 3.703 11.387\\nNO-RESIDUAL-LAST-FORW ARD-G 15.386 11.346 15.282 6.673 13.931\\nRESIDUAL-INPUT-G 13.264 9.545 12.316 3.692 11.438\\nN-BEATS-DRESS-G 13.211 9.217 12.122 3.636 11.251'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 16, 'page_label': '17', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='NO-RESIDUAL-LAST-FORW ARD-G 15.386 11.346 15.282 6.673 13.931\\nRESIDUAL-INPUT-G 13.264 9.545 12.316 3.692 11.438\\nN-BEATS-DRESS-G 13.211 9.217 12.122 3.636 11.251\\nTable 8: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\\nthe ensemble of 18 interpretable models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-I 13.207 9.530 12.500 3.710 11.510\\nNO-RESIDUAL-I 13.075 9.707 12.708 4.007 11.637\\nLAST-FORW ARD-I 13.168 9.547 12.111 3.599 11.313\\nNO-RESIDUAL-LAST-FORW ARD-I 13.067 10.207 15.177 4.912 12.986\\nRESIDUAL-INPUT-I 13.104 9.716 12.814 4.005 11.697\\nN-BEATS-DRESS-I 13.155 9.286 12.009 3.642 11.201\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 17, 'page_label': '18', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 9: Performance on the M4 test set, OWA. Lower values are better. The results are obtained on\\nthe ensemble of 18 generic models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-G 0.780 0.832 0.852 0.844 0.822\\nNO-RESIDUAL-G 0.774 0.831 0.851 0.853 0.819\\nLAST-FORW ARD-G 0.774 0.808 0.840 0.846 0.811\\nNO-RESIDUAL-LAST-FORW ARD-G 0.948 1.029 1.095 1.296 1.030\\nRESIDUAL-INPUT-G 0.779 0.831 0.840 0.844 0.817\\nN-BEATS-DRESS-G 0.776 0.800 0.823 0.835 0.803\\nTable 10: Performance on the M4 test set, OWA. Lower values are better. The results are obtained on\\nthe ensemble of 18 interpretable models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-I 0.776 0.831 0.857 0.845 0.821\\nNO-RESIDUAL-I 0.769 0.848 0.886 0.886 0.833\\nLAST-FORW ARD-I 0.773 0.836 0.825 0.817 0.808\\nNO-RESIDUAL-LAST-FORW ARD-I 0.771 0.900 1.085 1.016 0.922\\nRESIDUAL-INPUT-I 0.771 0.848 0.892 0.887 0.836'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 17, 'page_label': '18', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='NO-RESIDUAL-I 0.769 0.848 0.886 0.886 0.833\\nLAST-FORW ARD-I 0.773 0.836 0.825 0.817 0.808\\nNO-RESIDUAL-LAST-FORW ARD-I 0.771 0.900 1.085 1.016 0.922\\nRESIDUAL-INPUT-I 0.771 0.848 0.892 0.887 0.836\\nN-BEATS-DRESS-I 0.771 0.805 0.819 0.836 0.800\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 11: Performance on the M4 test set, sMAPE . Lower values are better. Red – second best.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nBest pure ML 14.397 11.031 13.973 4.566 12.894\\nBest statistical 13.366 10.155 13.002 4.682 11.986\\nBest ML/TS combination 13.528 9.733 12.639 4.118 11.720\\nDL/TS hybrid, M4 winner 13.176 9.679 12.126 4.014 11.374\\nN-BEATS-G 13.023 9.212 12.048 3.574 11.168\\nN-BEATS-I 12.924 9.287 12.059 3.684 11.174\\nN-BEATS-I+G 12.913 9.213 12.024 3.643 11.135\\nTable 12: Performance on the M4 test set, OWA and M4 rank. Lower values are better. Red – second\\nbest.\\nYearly Quarterly Monthly Others Average Rank\\n(23k) (24k) (48k) (5k) (100k)\\nBest pure ML 0.859 0.939 0.941 0.991 0.915 23\\nBest statistical 0.788 0.898 0.905 0.989 0.861 8\\nBest ML/TS combination 0.799 0.847 0.858 0.914 0.838 2\\nDL/TS hybrid, M4 winner 0.778 0.847 0.836 0.920 0.821 1\\nN-BEATS-G 0.765 0.800 0.820 0.822 0.797'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Best statistical 0.788 0.898 0.905 0.989 0.861 8\\nBest ML/TS combination 0.799 0.847 0.858 0.914 0.838 2\\nDL/TS hybrid, M4 winner 0.778 0.847 0.836 0.920 0.821 1\\nN-BEATS-G 0.765 0.800 0.820 0.822 0.797\\nN-BEATS-I 0.758 0.807 0.824 0.849 0.798\\nN-BEATS-I+G 0.758 0.800 0.819 0.840 0.795\\nC D ETAILED EMPIRICAL RESULTS\\nC.1 D ETAILED RESULTS : M4 D ATASET\\nTables 11 and 12 present our key quantitative empirical results showing that the proposed model\\nachieves the state of the art performance on the challenging M4 benchmark. We study the performance\\nof two model conﬁgurations: generic (Ours-G) and interpretable (Ours-I), as well as Ours-I+G\\n(ensemble of all models from Ours-G and Ours-I). We compare against 4 representatives from the\\nM4 competition: each best in their respective model class. Best pure ML is the submission by B.\\nTrotta, the best entry among the 6 pure ML models. Best statistical is the best pure statistical model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='M4 competition: each best in their respective model class. Best pure ML is the submission by B.\\nTrotta, the best entry among the 6 pure ML models. Best statistical is the best pure statistical model\\nby N.Z. Legaki and K. Koutsouri. Best ML/TS combination is the model by P. Montero-Manso, T.\\nTalagala, R.J. Hyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few\\nstatistical time series models. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020).\\nN-BEATS outperforms all other approaches on all the studied subsets of time series. The average\\nOWA gap between our generic model and the M4 winner (0.821 −0.795 = 0.026) is greater than the\\ngap between the M4 winner and the second entry (0.838 −0.821 = 0.017).\\nA more granular and detailed statistical analysis of our results on M4 is provided in Table 13. This\\ntable ﬁrst presents the sMAPE for N-BEATS, decomposed by M4 time series sub-type and sampling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='A more granular and detailed statistical analysis of our results on M4 is provided in Table 13. This\\ntable ﬁrst presents the sMAPE for N-BEATS, decomposed by M4 time series sub-type and sampling\\nfrequency (upper part). Then (lower part), it shows the average sMAPE difference between the\\nN-BEATS results and the M4 winner (TS/DL hybrid by S. Smyl), adding the standard error of that\\ndifference (in parentheses); bold entries indicate statistical signiﬁcance at the 99% level based on a\\ntwo-sided paired t-test.\\nWe note that each cross-section of the M4 dataset into horizon and type may be regarded as an\\nindependent mini-dataset. We observe that over those mini-datasets there is a preponderance of\\nstatistically signiﬁcant differences between N-BEATS and Smyl (18 cases out of 31) to the advantage\\nof N-BEATS. This provides evidence that (i) the improvement observed on average in Tables 11\\nand 12 is statistically signiﬁcant and consistent over smaller subsets of M4 and (ii) N-BEATS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='of N-BEATS. This provides evidence that (i) the improvement observed on average in Tables 11\\nand 12 is statistically signiﬁcant and consistent over smaller subsets of M4 and (ii) N-BEATS\\ngeneralizes well over time series of different types and sampling frequencies.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 19, 'page_label': '20', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 13: Performance decomposition on non-overlapping subsets of the M4 test set and comparison\\nwith the Smyl model results.\\nDemographic Finance Industry Macro Micro Other\\nsMAPE per M4 series type and sampling frequency\\nYearly 8 .931 13 .741 16 .317 13 .327 10 .489 13 .320\\nQuarterly 9 .219 10 .787 8 .628 8 .576 9 .264 6 .250\\nMonthly 4 .357 13 .353 12 .657 12 .571 13 .627 11 .595\\nWeekly 4 .580 3 .004 9 .258 7 .220 10 .425 6 .183\\nDaily 6 .351 3 .467 3 .835 2 .525 2 .299 2 .885\\nHourly 8 .197\\nAverage sMAPE difference vs Smyl model, computed as N-BEATS – Smyl.\\nStandard error of the mean displayed in parenthesis.\\nBold entries are signiﬁcant at the 99% level (2-sided paired t-test).\\nYearly −0.749 −0.337 −0.065 −0.386 −0.168 −0.157\\n(0.119) ( 0.065) ( 0.087) ( 0.085) ( 0.056) ( 0.140)\\nQuarterly −0.651 −0.281 −0.328 −0.712 −0.523 −0.029\\n(0.085) ( 0.047) ( 0.043) ( 0.060) ( 0.051) ( 0.083)\\nMonthly −0.185 −0.379 −0.419 0.089 0.338 −0.279'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 19, 'page_label': '20', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Quarterly −0.651 −0.281 −0.328 −0.712 −0.523 −0.029\\n(0.085) ( 0.047) ( 0.043) ( 0.060) ( 0.051) ( 0.083)\\nMonthly −0.185 −0.379 −0.419 0.089 0.338 −0.279\\n(0.023) ( 0.034) ( 0.036) ( 0.039) ( 0.034) ( 0.162)\\nWeekly −0.336 −1.075 −0.937 −1.627 −3.029 −1.193\\n(0.270) ( 0.221) ( 1.399) ( 0.770) ( 0.378) ( 0.772)\\nDaily 0 .191 −0.098 −0.124 −0.026 −0.367 −0.037\\n(0.231) ( 0.018) ( 0.025) ( 0.057) ( 0.013) ( 0.015)\\nHourly −1.132\\n(0.163)\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 14: Performance on the M3 test set, Average sMAPE , aggregate over all forecast horizons\\n(Yearly: 1-6, Quarterly: 1-8, Monthly: 1-18, Other: 1-8, Average: 1-18). Lower values are better.\\nRed – second best. †Numbers are computed by us.\\nYearly Quarterly Monthly Others Average\\n(645) (756) (1428) (174) (3003)\\nNaïve2 17.88 9.95 16.91 6.30 15.47\\nARIMA (B–J automatic) 17.73 10.26 14.81 5.06 14.01\\nComb S-H-D 17.07 9.22 14.48 4.56 13.52\\nForecastPro 17.14 9.77 13.86 4.60 13.19\\nTheta 16.90 8.96 13.85 4.41 13.01\\nDOTM (Fiorucci et al., 2016) 15.94 9.28 13.74 4.58 12.90\\nEXP (Spiliotis et al., 2019) 16.39 8.98 13.43 5.46 12 .71†\\nLGT (Smyl & Kuber, 2016) 15.23 n/a n/a 4.26 n/a\\nBaggedETS.BC (Bergmeir et al., 2016) 17.49 9.89 13.74 n/a n/a\\nN-BEATS-G 16.2 8.92 13.19 4.19 12.47\\nN-BEATS-I 15.84 9.03 13.15 4.30 12.43\\nN-BEATS-I+G 15.93 8.84 13.11 4.24 12.37\\nC.2 D ETAILED RESULTS : M3 D ATASET'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='N-BEATS-G 16.2 8.92 13.19 4.19 12.47\\nN-BEATS-I 15.84 9.03 13.15 4.30 12.43\\nN-BEATS-I+G 15.93 8.84 13.11 4.24 12.37\\nC.2 D ETAILED RESULTS : M3 D ATASET\\nResults for M3 dataset are provided in Table 14. The performance metric is calculated using the\\nearlier version of sMAPE , deﬁned speciﬁcally for the M3 competition:1\\nsMAPE = 200\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\nyT +i + ˆyT +i\\n. (4)\\nFor some of the methods, either average sMAPE was not reported or sMAPE for some of the splits was\\nnot reported in their respective publications. Below, we list those cases. BaggedETS.BC (Bergmeir\\net al., 2016) has not reported numbers on Others. LGT (Smyl & Kuber, 2016) did not report results on\\nMonthly and Quarterly data. According to the authors, the underlying RNN had problems dealing with\\nraw seasonal data, the ETS based pre-processing was not effective and the LGT pre-processing was\\nnot computationally feasible given comparatively large number of time series and their comparatively'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='raw seasonal data, the ETS based pre-processing was not effective and the LGT pre-processing was\\nnot computationally feasible given comparatively large number of time series and their comparatively\\nlarge length (Smyl & Kuber, 2016). Finally, EXP (Spiliotis et al., 2019) reported average performance\\ncomputed using a different methodology than the default M3 and M4 methodology (source: personal\\ncommunication with the authors). For the latter method we recomputed the Average sMAPE based on\\nthe previously reported Yearly, Quarterly and Monthly splits. To calculate it, we follow the M3, M4\\nand TOURISM competition methodology and compute the average metric as the average over all time\\nseries and over all forecast horizons. Given the performance metric values aggregated over Yearly,\\nQuarterly and Monthly splits, the average can be computed straightforwardly as:\\nsMAPE Average = NYear\\nNTot\\nsMAPE Year+NQuart\\nNTot\\nsMAPE Quart +NMonth\\nNTot\\nsMAPE Month +NOthers\\nNTot\\nsMAPE Others .\\n(5)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Quarterly and Monthly splits, the average can be computed straightforwardly as:\\nsMAPE Average = NYear\\nNTot\\nsMAPE Year+NQuart\\nNTot\\nsMAPE Quart +NMonth\\nNTot\\nsMAPE Month +NOthers\\nNTot\\nsMAPE Others .\\n(5)\\nHere NTot = NYear +NQuart +NMonth +NOthers and NYear = 6 ×645,NQuart = 8 ×756,NMonth = 18 ×\\n1428,NOthers = 8 ×174. It is clear that for each split, its N is the product of its respective number of\\ntime series and its largest forecast horizon.\\n1With minor differences compared to the sMAPE deﬁnition used for M4. Please refer to Appendix A\\nin (Makridakis & Hibon, 2000) for the mathematical deﬁnition.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 15: Performance on the TOURISM test set, Average MAPE , aggregate over all forecast horizons\\n(Yearly: 1-4, Quarterly: 1-8, Monthly: 1-24, Average: 1-24). Lower values are better. Red – second\\nbest.\\nYearly Quarterly Monthly Average\\n(518) (427) (366) (1311)\\nStatistical benchmarks (Athanasopoulos et al., 2011)\\nSNaïve 23.61 16.46 22.56 21.25\\nTheta 23.45 16.15 22.11 20.88\\nForePro 26.36 15.72 19.91 19.84\\nETS 27.68 16.05 21.15 20.88\\nDamped 28.15 15.56 23.47 22.26\\nARIMA 28.03 16.23 21.13 20.96\\nKaggle competitors (Athanasopoulos & Hyndman, 2011)\\nSaliMali n/a 14.83 19.64 n/a\\nLeeCBaker 22.73 15.14 20.19 19.35\\nStratometrics 23.15 15.14 20.37 19.52\\nRobert n/a 14.96 20.28 n/a\\nIdalgo n/a 15.07 20.55 n/a\\nN-BEATS-G (Ours) 21.67 14.71 19.17 18.47\\nN-BEATS-I (Ours) 21.55 15.22 19.82 18.97\\nN-BEATS-I+G (Ours) 21.44 14.78 19.29 18.52\\nC.3 D ETAILED RESULTS : TOURISM DATASET'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Idalgo n/a 15.07 20.55 n/a\\nN-BEATS-G (Ours) 21.67 14.71 19.17 18.47\\nN-BEATS-I (Ours) 21.55 15.22 19.82 18.97\\nN-BEATS-I+G (Ours) 21.44 14.78 19.29 18.52\\nC.3 D ETAILED RESULTS : TOURISM DATASET\\nDetailed results for the TOURISM competition dataset are provided in Table 15. The respective Kaggle\\ncompetition was divided into two parts: (i) Yearly time series forecasting and (ii) Quarterly/Monthly\\ntime series forecasting (Athanasopoulos & Hyndman, 2011). Some of the participants chose to\\ntake part only in the second part. Therefore, In addition to entries present in Table 1, we report\\ncompetitors from (Athanasopoulos & Hyndman, 2011) that have missing results in Yearly compe-\\ntition. In particular, SaliMali team is the winner of the Quarterly/Monthly time series forecasting\\ncompetition (Brierley, 2011). Their approach is based on a weighted ensemble of statistical methods.\\nTeams Robert and Idalgo used unknown approaches. We can see from Table 15 that N-BEATS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='competition (Brierley, 2011). Their approach is based on a weighted ensemble of statistical methods.\\nTeams Robert and Idalgo used unknown approaches. We can see from Table 15 that N-BEATS\\nachieves state-of-the-art performance on all subsets of TOURISM dataset. On average, it is state of the\\nart and it gains 4.2% over the best-known approach LeeCBaker, and 11.5% over auto-ARIMA.\\nThe average metrics have not been reported in the original competition results (Athanasopoulos et al.,\\n2011; Athanasopoulos & Hyndman, 2011). Therefore, in Table 15, we present the Average MAPE\\nmetric calculated by us based on the previously reported Yearly, Quarterly and Monthly splits. To\\ncalculate it, we follow the M4 competition methodology and compute the average metric as the\\naverage over all time series and over all forecast horizons. Given the performance metric values\\naggregated over Yearly, Quarterly and Monthly splits, the average can be computed straightforwardly\\nas:\\nMAPE Average = NYear\\nNTot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='aggregated over Yearly, Quarterly and Monthly splits, the average can be computed straightforwardly\\nas:\\nMAPE Average = NYear\\nNTot\\nMAPE Year+NQuart\\nNTot\\nMAPE Quart +NMonth\\nNTot\\nMAPE Month . (6)\\nHere NTot = NYear +NQuart +NMonth and NYear = 4 ×518,NQuart = 8 ×427,NMonth = 24 ×366. It is\\nclear that for each split, its N is the product of its respective number of time series and its largest\\nforecast horizon.\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nC.4 D ETAILED RESULTS : ELECTRICITY AND TRAFFIC DATASETS\\nIn this experiment we are comparing the performances of MatFact (Yu et al., 2016), DeepAR (Flunkert\\net al., 2017) (Amazon Labs), Deep State (Rangapuram et al., 2018a) (Amazon Labs), Deep Fac-\\ntors (Wang et al., 2019) (Amazon Labs), and N-BEATS models on ELECTRICITY 2 (Dua & Graff,\\n2017) and TRAFFIC 3 (Dua & Graff, 2017) datasets. The results are presented in in Table 16.\\nBoth datasets are aggregated to hourly data, but using different aggregation operations: sum for\\nELECTRICITY and mean for TRAFFIC . The hourly aggregation is done so that all the points available\\nin (h −1 : 00,h : 00] hours are aggregated to hour h, thus if original dataset starts on 2011-01-01\\n00:15 then the ﬁrst time point after aggregation will be 2011-01-01 01:00. For the ELECTRICITY\\ndataset we removed the ﬁrst year from training set, to match the training set used in (Yu et al., 2016),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='00:15 then the ﬁrst time point after aggregation will be 2011-01-01 01:00. For the ELECTRICITY\\ndataset we removed the ﬁrst year from training set, to match the training set used in (Yu et al., 2016),\\nbased on the aggregated dataset downloaded from, presumable authors’, github repository4. We also\\nmade sure that data points for both ELECTRICITY and TRAFFIC datasets after aggregation match\\nthose used in (Yu et al., 2016). The authors of MatFact model were using the last 7 days of datasets\\nas test set, but papers from Amazon are using different splits, where the split points are provided by a\\ndate. Changing split points without a well grounded reason adds uncertainties to the comparability of\\nthe models performances and creates challenges to the reproducibility of the results, thus we were\\ntrying to match all different splits in our experiments. It was especially challenging on TRAFFIC\\ndataset, where we had to use some heuristics to ﬁnd records dates; the dataset authors state: “ The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='trying to match all different splits in our experiments. It was especially challenging on TRAFFIC\\ndataset, where we had to use some heuristics to ﬁnd records dates; the dataset authors state: “ The\\nmeasurements cover the period from Jan. 1st 2008 to Mar. 30th 2009” and “ We remove public\\nholidays from the dataset, as well as two days with anomalies (March 8th 2009 and March 9th 2008)\\nwhere all sensors were muted between 2:00 and 3:00 AM. ” , but we failed to match a part of the\\nprovided labels of week days to actual dates. Therefore, we had to assume that the actual list of gaps,\\nwhich include holidays and anomalous days, is the following:\\n1. Jan. 1, 2008 (New Year’s Day)\\n2. Jan. 21, 2008 (Martin Luther King Jr. Day)\\n3. Feb. 18, 2008 (Washington’s Birthday)\\n4. Mar. 9, 2008 (Anomaly day)\\n5. May 26, 2008 (Memorial Day)\\n6. Jul. 4, 2008 (Independence Day)\\n7. Sep. 1, 2008 (Labor Day)\\n8. Oct. 13, 2008 (Columbus Day)\\n9. Nov. 11, 2008 (Veterans Day)\\n10. Nov. 27, 2008 (Thanksgiving)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='5. May 26, 2008 (Memorial Day)\\n6. Jul. 4, 2008 (Independence Day)\\n7. Sep. 1, 2008 (Labor Day)\\n8. Oct. 13, 2008 (Columbus Day)\\n9. Nov. 11, 2008 (Veterans Day)\\n10. Nov. 27, 2008 (Thanksgiving)\\n11. Dec. 25, 2008 (Christmas Day)\\n12. Jan. 1, 2009 (New Year’s Day)\\n13. Jan. 19, 2009 (Martin Luther King Jr. Day)\\n14. Feb. 16, 2009 (Washington’s Birthday)\\n15. Mar. 8, 2009 (Anomaly day)\\nThe ﬁrst 6 gaps were conﬁrmed by the gaps in labels, but the rest were more than 1 day apart from any\\npublic holiday of years 2008 and 2009 in San Francisco, California and US. More over the number of\\ngaps we found in the labels provided by dataset authors is 10, while the number of days between Jan.\\n1st 2008 and Mar. 30th 2009 is 455, assuming that Jan. 1st 2008 was skipped from the values and\\nlabels we should end up with either 454 −10 = 444 instead of 440 days or different end date.\\nThe metric is reported in Normalized deviation (ND) as in (Yu et al., 2016) which is equal to p50'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='labels we should end up with either 454 −10 = 444 instead of 440 days or different end date.\\nThe metric is reported in Normalized deviation (ND) as in (Yu et al., 2016) which is equal to p50\\nloss used in DeepAR, Deep State, and Deep Factors papers.\\n2https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\\n3https://archive.ics.uci.edu/ml/datasets/PEMS-SF\\n4https://github.com/rofuyu/exp-trmf-nips16/blob/master/python/exp-scripts/datasets/download-data.sh\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 23, 'page_label': '24', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nND = ∑i,t |ˆYit −Yit|\\n∑i,t |Yit| (7)\\nTable 16: ND Performance on the ELECTRICITY and TRAFFIC test sets.\\n1 Split used in DeepAR (Flunkert et al., 2017) and Deep State (Rangapuram et al., 2018a).\\n2 Split used in Deep Factors (Wang et al., 2019).\\n†Numbers reported by (Flunkert et al., 2017), which are different from the original MatFact paper,\\nhypothetically due to changed split point.\\nELECTRICITY TRAFFIC\\n2014-09-011 2014-03-312 last 7 days 2008-06-15 1 2008-01-142 last 7 days\\nMatFact 0.16 † n/a 0.255 0.20† n/a 0.187\\nDeepAR 0.07 0.272 n/a 0.17 0.296 n/a\\nDeep State 0.083 n/a n/a 0.167 n/a n/a\\nDeep Factors n/a 0.112 n/a n/a 0.225 n/a\\nN-BEATS-G (ours) 0.064 0.065 0.171 0.114 0.230 0.112\\nN-BEATS-I (ours) 0.073 0.072 0.185 0.114 0.231 0.110\\nN-BEATS-I+G (ours) 0.067 0.067 0.178 0.114 0.230 0.111\\nContrary to Amazon models N-BEATS does not use any covariates, like day-of-week, hour-of-day,\\netc.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 23, 'page_label': '24', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='N-BEATS-I+G (ours) 0.067 0.067 0.178 0.114 0.230 0.111\\nContrary to Amazon models N-BEATS does not use any covariates, like day-of-week, hour-of-day,\\netc.\\nThe N-BEATS architecture used in this experiment is exactly the same as used in M4, M3 and\\nTOURISM datasets, the only difference is history size and the number of iterations. These parameters\\nwere chosen based on performance on validation set. Where the validation set consists of 7 consecutive\\ndays right before the test set. After the parameters are chosen the model is retrained on training set\\nwhich includes the validation set, then tested on test set. The model is trained once and tested on test\\nset using rolling window operation described in (Yu et al., 2016).\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 24, 'page_label': '25', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nC.5 D ETAILED RESULTS : COMPARE TO DEEPAR, D EEP STATE SPACE MODELS\\nTable 17 compares ND (7) performance of DeepAR, DeepState models published in (Rangapuram\\net al., 2018a) and N-BEATS.\\nTable 17: ND Performance of DeepAR, Deep State Space, and N-BEATS models on M4-Hourly and\\nTOURISM datasets\\nM4 (Hourly) TOURISM (Monthly) TOURISM (Quarterly)\\nDeepAR 0.09 0.107 0.11\\nDeepState 0.044 0.138 0.098\\nN-BEATS-G (ours) 0.023 0.097 0.080\\nN-BEATS-I (ours) 0.027 0.103 0.079\\nN-BEATS-I+G (ours) 0.025 0.099 0.077\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 25, 'page_label': '26', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 18: Settings of hyperparameters across subsets of M4, M3, TOURISM datasets.\\nM4 M3 TOURISM\\nYly Qly Mly Wly Dly Hly Yly Qly Mly Other Yly Qly Mly\\nParameter N-BEATS-I\\nLH 1.5 1.5 1.5 10 10 10 20 5 5 20 20 10 20\\nIterations 15K 15K 15K 5K 5K 5K 50 6K 6K 250 30 500 300\\nLosses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\\nS-width 2048\\nS-blocks 3\\nS-block-layers 4\\nT-width 256\\nT-degree 2\\nT-blocks 3\\nT-block-layers 4\\nSharing STACK LEVEL\\nLookback period 2 H,3H,4H,5H,6H,7H\\nBatch 1024\\nParameter N-BEATS-G\\nLH 1.5 1.5 1.5 10 10 10 20 20 20 10 5 10 20\\nIterations 15K 15K 15K 5K 5K 5K 20 250 10K 250 30 100 100\\nLosses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\\nWidth 512\\nBlocks 1\\nBlock-layers 4\\nStacks 30\\nSharing NO\\nLookback period 2 H,3H,4H,5H,6H,7H\\nBatch 1024\\nD H YPER -PARAMETER SETTINGS\\nTable 18 presents the hyperparameter settings used to train models on different subsets of M4, M3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 25, 'page_label': '26', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Stacks 30\\nSharing NO\\nLookback period 2 H,3H,4H,5H,6H,7H\\nBatch 1024\\nD H YPER -PARAMETER SETTINGS\\nTable 18 presents the hyperparameter settings used to train models on different subsets of M4, M3\\nand TOURISM datasets. A brief discussion of ﬁeld names in the table is warranted.\\nSubset names Yly, Qly, Mly, Wly, Dly, Hly, Othercorrespond to yearly, quarterly, monthly, weekly,\\ndaily, hourly and other frequency subsets deﬁned in the original datasets.\\nN-BEATS-I and N-BEATS-G correspond to the interpretable and generic model conﬁgurations\\ndeﬁned in Section 3.3.\\nD.1 C OMMON PARAMETERS\\nLH is the coefﬁcient deﬁning the length of training history immediately preceding the last point in\\nthe train part of the TS that is used to generate training samples. For example, if for M4 Yearly the\\nforecast horizon is 6 and LH is 1.5, then we consider 1.5 ·6 = 9 most recent points in the train dataset\\nfor each time series to generate training samples. A training sample from a given TS in M4 Yearly is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 25, 'page_label': '26', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='for each time series to generate training samples. A training sample from a given TS in M4 Yearly is\\nthen generated by choosing one of the most recent 9 points as an anchor. All the points preceding the\\nanchor are used to create the input to N-BEATS, while the points following and including the anchor\\nbecome training target. Target and history points that fall outside of the time series limits given the\\nanchor position are ﬁlled with zeros and masked during the training. We observed that for subsets\\nwith large number of time series LH tends to be smaller and for subsets with smaller number of time\\nseries it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4LH is\\nequal to 1.5; and in moderate to small Weekly, Daily, Hourly subsets of M4LH is equal to 10.\\nIterations is the number of batches used to train N-BEATS.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nLosses is the set of loss functions that is used to build ensemble. We observed on the respective\\nvalidation sets that for M4 and M3 mixing models trained on a variety of metrics resulted in\\nperformance gain. In the case of TOURISM dataset training only on MAPE led to the best validation\\nscores.\\nSharing deﬁnes whether the coefﬁcients in the fully-connected layers are shared. We observed that\\nthe interpretable model works best when weights are shared across stack, while generic model works\\nbest when none of the weights are shared.\\nLookback periodis the length of the history window forming the input to the model (please refer to\\nFigure 1). This is the function of the forecast horizon length, H. In our experiments we mixed models\\nwith lookback periods 2H,3H,4H,5H,6H,7H in one ensemble. As an example, for a forecast\\nhorizon length H = 8 and a lookback period 7H, the model’s input will consist of the history window\\nof 7 ·8 = 56 samples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='horizon length H = 8 and a lookback period 7H, the model’s input will consist of the history window\\nof 7 ·8 = 56 samples.\\nBatch is the batch size. We used batch size of 1024. We observed that the training was faster with\\nlarger batch sizes, however in our setup little gain was observed with batch sizes beyond 1024.\\nD.2 N-BEATS-I PARAMETERS\\nS-width is the width of the fully connected layers in the blocks comprising the seasonality stack of\\nthe interpretable model (please refer to Figure 1).\\nS-blocks is the number of blocks comprising the seasonality stack of the interpretable model (please\\nrefer to Figure 1).\\nS-block-layers is the number of fully-connected layers comprising one block in the seasonality\\nstack of the interpretable model (preceding the ﬁnal fully-connected projection layers forming the\\nbackcast/forecast fork, please refer to Figure 1).\\nT-width is the width of the fully connected layers in the blocks comprising the trend stack of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='backcast/forecast fork, please refer to Figure 1).\\nT-width is the width of the fully connected layers in the blocks comprising the trend stack of the\\ninterpretable model (please refer to Figure 1).\\nT-degree is the degree p of polynomial in the trend stack of the interpretable model (please refer to\\nequation (2)).\\nT-blocks is the number of blocks comprising the trend stack of the interpretable model (please refer\\nto Figure 1).\\nT-block-layers is the number of fully-connected layers comprising one block in the trend stack\\nof the interpretable model (preceding the ﬁnal fully-connected projection layers forming the back-\\ncast/forecast fork, please refer to Figure 1).\\nD.3 N-BEATS-G PARAMETERS\\nWidth is the width of the fully connected layers in the blocks comprising the stacks of the generic\\nmodel (please refer to Figure 1).\\nBlocks is the number of blocks comprising the stack of the generic model (please refer to Figure 1).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='model (please refer to Figure 1).\\nBlocks is the number of blocks comprising the stack of the generic model (please refer to Figure 1).\\nBlock-layers is the number of fully-connected layers comprising one block in the stack of the generic\\nmodel (preceding the ﬁnal fully-connected projection layers forming the backcast/forecast fork,\\nplease refer to Figure 1).\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n0 1 2 3 4 5\\nt\\n0.8\\n0.9\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90 STACK1-G\\n0 1 2 3 4 5\\nt\\n0.025\\n0.050\\n0.075 STACK2-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90\\n0.95\\nSTACK1-I\\n0 1 2 3 4 5\\nt\\n0.02\\n0.03\\n0.04\\n0.05 STACK2-I\\n0 2 4 6\\nt\\n0.85\\n0.90\\n0.95\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6\\nt\\n0.86\\n0.88\\n0.90 STACK1-G\\n0 2 4 6\\nt\\n0.025\\n0.000\\n0.025\\n0.050\\nSTACK2-G\\n0 2 4 6\\nt\\n0.88\\n0.89\\n0.90\\nSTACK1-I\\n0 2 4 6\\nt\\n0.05\\n0.00\\n0.05\\nSTACK2-I\\n0 5 10 15\\nt\\n0.4\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 5 10 15\\nt\\n0.8\\n0.9 STACK1-G\\n0 5 10 15\\nt\\n0.1\\n0.0\\nSTACK2-G\\n0 5 10 15\\nt\\n0.85\\n0.90\\nSTACK1-I\\n0 5 10 15\\nt\\n0.3\\n0.2\\n0.1\\n0.0 STACK2-I\\n0 2 4 6 8 10 12\\nt\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80\\nSTACK1-G\\n0 2 4 6 8 10 12\\nt\\n0.000\\n0.025\\n0.050\\n0.075\\nSTACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='STACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.002\\n0.001\\n STACK2-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.0003\\n0.0002\\n0.0001\\n STACK2-I\\n0 10 20 30 40\\nt\\n0.25\\n0.50\\n0.75\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n(a) Combined\\n0 10 20 30 40\\nt\\n0.2\\n0.4\\n0.6\\nSTACK1-G (b) Stack1-G\\n0 10 20 30 40\\nt\\n0.02\\n0.00\\nSTACK2-G (c) Stack2-G\\n0 10 20 30 40\\nt\\n0.36\\n0.38\\n0.40\\nSTACK1-I (d) StackT-I\\n0 10 20 30 40\\nt\\n0.2\\n0.0\\n0.2\\nSTACK2-I (e) StackS-I\\nFigure 5: The outputs of generic and the interpretable conﬁgurations, M4 dataset. Each row is one\\ntime series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='time series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\\nare normalized by the maximal value of the actual time series for convenience. Column (a) shows the\\nactual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\\nforecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\\nrespectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\\nand the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\\nE D ETAILED SIGNAL TRACES OF INTERPRETABLE INPUTS PRESENTED IN\\nFIGURE 2\\nThe goal of this section is to show the detailed traces (numeric values) of signals visualized in Fig. 2.\\nThis is to demonstrate that even though the StackT-I (Fig. 2 (d)) and StackS-I (Fig. 2 (e)) provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='This is to demonstrate that even though the StackT-I (Fig. 2 (d)) and StackS-I (Fig. 2 (e)) provide\\nresponse lines different from the counterparts in Stack1-G (Fig. 2 (b)) and Stack2-G (Fig. 2 (c)), the\\nsummations in the combined line (Fig. 2 (a)) can still be very similar.\\nFirst, we reproduce Fig. 5 for the convenience of the reader. Second, for each row in the ﬁgure, we\\nproduce a table showing the numeric values of each signal depicted in corresponding plots (please\\nrefer to Tables 19– 24). We make sure that the names of signals in ﬁgure legends and in the table\\ncolumns match, such that they can easily be cross-referenced. It can be clearly seen in Tables 19– 24\\nthat (i) traces STACK1-I and STACK2-I sum up to trace FORECAST-I, (ii) traces STACK1-G and\\nSTACK2-G sum up to trace FORECAST-G, (iii) traces FORECAST-I and FORECAST-G are overall\\nvery similar even though their components may signiﬁcantly differ from each other.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 19: Detailed traces of signals depicted in row 1 of Fig. 5, corresponding to the time series\\nYearly: id Y3974.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.780182 0.802068 0.806608 0.781290 0.020778 0.801294 0.005314\\n1 0.802337 0.829223 0.841406 0.798422 0.030801 0.825271 0.016135\\n2 0.840317 0.863683 0.883136 0.820196 0.043487 0.853114 0.030022\\n3 0.889376 0.905962 0.929258 0.850250 0.055712 0.880833 0.048425\\n4 0.930521 0.947028 0.967846 0.892221 0.054807 0.904393 0.063453\\n5 0.976414 0.982307 1.000000 0.949748 0.032559 0.921360 0.078640\\nTable 20: Detailed traces of signals depicted in row 2 of Fig. 5, corresponding to the time series\\nQuarterly: id Q11588.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.830068 0.835964 0.829417 0.880435 -0.044471 0.852018 -0.022601\\n1 0.927155 0.898949 0.891168 0.881626 0.017324 0.880124 0.011044'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='ACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.830068 0.835964 0.829417 0.880435 -0.044471 0.852018 -0.022601\\n1 0.927155 0.898949 0.891168 0.881626 0.017324 0.880124 0.011044\\n2 0.979204 0.957379 0.948799 0.882549 0.074831 0.907149 0.041650\\n3 0.857250 0.900612 0.891967 0.883830 0.016782 0.877959 0.014008\\n4 0.895082 0.857230 0.847029 0.886096 -0.028866 0.852232 -0.005204\\n5 0.981590 0.923832 0.911001 0.889972 0.033860 0.881140 0.029861\\n6 1.000000 0.978128 0.965236 0.896085 0.082043 0.907475 0.057761\\n7 0.910528 0.920632 0.915460 0.905062 0.015571 0.886941 0.028519\\nTable 21: Detailed traces of signals depicted in row 3 of Fig. 5, corresponding to the time series\\nMonthly: id M19006.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 1.000000 0.923394 0.928279 0.944660 -0.021266 0.922835 0.005444\\n1 0.865248 0.822588 0.829924 0.937575 -0.114987 0.867619 -0.037695\\n2 0.638298 0.693820 0.717119 0.930295 -0.236475 0.810818 -0.093699'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='0 1.000000 0.923394 0.928279 0.944660 -0.021266 0.922835 0.005444\\n1 0.865248 0.822588 0.829924 0.937575 -0.114987 0.867619 -0.037695\\n2 0.638298 0.693820 0.717119 0.930295 -0.236475 0.810818 -0.093699\\n3 0.531915 0.594375 0.612377 0.922890 -0.328515 0.757199 -0.144823\\n4 0.468085 0.579403 0.595221 0.915428 -0.336025 0.747151 -0.151930\\n5 0.539007 0.602615 0.620809 0.907977 -0.305362 0.755078 -0.134269\\n6 0.581560 0.653387 0.682669 0.900606 -0.247219 0.774561 -0.091891\\n7 0.666667 0.747440 0.765814 0.893385 -0.145945 0.799594 -0.033781\\n8 0.737589 0.817883 0.835577 0.886382 -0.068498 0.817218 0.018359\\n9 0.765957 0.862568 0.856962 0.879665 -0.017097 0.822099 0.034862\\n10 0.851064 0.873448 0.880074 0.873304 0.000145 0.833473 0.046601\\n11 0.893617 0.878186 0.871103 0.867367 0.010819 0.829537 0.041566\\n12 0.858156 0.834448 0.853549 0.861923 -0.027475 0.816527 0.037022\\n13 0.695035 0.785341 0.776687 0.857040 -0.071699 0.782536 -0.005850'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='12 0.858156 0.834448 0.853549 0.861923 -0.027475 0.816527 0.037022\\n13 0.695035 0.785341 0.776687 0.857040 -0.071699 0.782536 -0.005850\\n14 0.446809 0.662443 0.697788 0.852789 -0.190345 0.745623 -0.047835\\n15 0.382979 0.623196 0.624614 0.849236 -0.226040 0.711553 -0.086939\\n16 0.453901 0.598511 0.625150 0.846451 -0.247941 0.712130 -0.086980\\n17 0.539007 0.668231 0.652175 0.844504 -0.176272 0.716925 -0.064750\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 29, 'page_label': '30', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 22: Detailed traces of signals depicted in row 4 of Fig. 5, corresponding to the time series\\nWeekly: id W246.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.630056 0.629703 0.625108 0.639236 -0.009534 0.625416 -0.000309\\n1 0.607536 0.643509 0.639846 0.647549 -0.004039 0.639592 0.000254\\n2 0.641731 0.656171 0.652584 0.656696 -0.000526 0.643665 0.008919\\n3 0.628783 0.669636 0.661163 0.666739 0.002897 0.652107 0.009056\\n4 0.816799 0.687287 0.683860 0.677738 0.009549 0.662176 0.021683\\n5 0.817020 0.709211 0.717187 0.689752 0.019459 0.686589 0.030598\\n6 0.766724 0.731732 0.742824 0.702841 0.028891 0.705234 0.037590\\n7 0.770320 0.750834 0.755154 0.717066 0.033768 0.716986 0.038167\\n8 0.794113 0.769671 0.778460 0.732487 0.037184 0.731113 0.047347\\n9 0.874011 0.793373 0.810332 0.749164 0.044209 0.750939 0.059392\\n10 1.000000 0.816386 0.847545 0.767157 0.049229 0.776405 0.071140'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 29, 'page_label': '30', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='8 0.794113 0.769671 0.778460 0.732487 0.037184 0.731113 0.047347\\n9 0.874011 0.793373 0.810332 0.749164 0.044209 0.750939 0.059392\\n10 1.000000 0.816386 0.847545 0.767157 0.049229 0.776405 0.071140\\n11 0.979251 0.834532 0.858604 0.786526 0.048006 0.783939 0.074665\\n12 0.933160 0.850010 0.866116 0.807332 0.042678 0.792134 0.073982\\nTable 23: Detailed traces of signals depicted in row 5 of Fig. 5, corresponding to the time series\\nDaily: id D404.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.968704 0.972314 0.971950 0.972589 -0.000275 0.972964 -0.001014\\n1 0.954319 0.972637 0.972131 0.972808 -0.000171 0.972822 -0.000690\\n2 0.954599 0.972972 0.972188 0.973060 -0.000088 0.973798 -0.001610\\n3 0.959959 0.973230 0.972140 0.973341 -0.000112 0.973686 -0.001546\\n4 0.975472 0.973481 0.972125 0.973649 -0.000168 0.974060 -0.001934\\n5 0.970391 0.973715 0.972174 0.973979 -0.000264 0.974800 -0.002626\\n6 0.977728 0.974056 0.972403 0.974328 -0.000272 0.974368 -0.001965'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 29, 'page_label': '30', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='5 0.970391 0.973715 0.972174 0.973979 -0.000264 0.974800 -0.002626\\n6 0.977728 0.974056 0.972403 0.974328 -0.000272 0.974368 -0.001965\\n7 0.985624 0.974445 0.972428 0.974693 -0.000248 0.973870 -0.001442\\n8 0.979695 0.974823 0.972567 0.975069 -0.000246 0.974870 -0.002303\\n9 0.985345 0.975079 0.973089 0.975455 -0.000376 0.975970 -0.002881\\n10 0.983088 0.975547 0.973881 0.975845 -0.000298 0.975796 -0.001915\\n11 0.983368 0.975991 0.974537 0.976238 -0.000247 0.976757 -0.002220\\n12 0.998312 0.976365 0.974924 0.976628 -0.000263 0.977579 -0.002655\\n13 1.000000 0.976821 0.975291 0.977013 -0.000193 0.977213 -0.001922\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 24: Detailed traces of signals depicted in row 6 of Fig. 5, corresponding to the time series\\nHourly: id H344.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.226804 0.256799 0.277159 0.346977 -0.090179 0.280489 -0.003329\\n1 0.175258 0.228913 0.234605 0.347615 -0.118701 0.241790 -0.007185\\n2 0.164948 0.209208 0.207347 0.348265 -0.139057 0.218575 -0.011228\\n3 0.164948 0.197360 0.193084 0.348928 -0.151568 0.208458 -0.015374\\n4 0.216495 0.190397 0.186586 0.349606 -0.159209 0.205701 -0.019115\\n5 0.195876 0.194204 0.189433 0.350297 -0.156094 0.214399 -0.024966\\n6 0.319588 0.221026 0.216221 0.351004 -0.129978 0.241574 -0.025353\\n7 0.226804 0.279857 0.276414 0.351726 -0.071869 0.293580 -0.017167\\n8 0.371134 0.357292 0.359372 0.352464 0.004828 0.364392 -0.005020\\n9 0.536082 0.438540 0.446126 0.353218 0.085322 0.442703 0.003423\\n10 0.711340 0.511441 0.519928 0.353989 0.157452 0.510142 0.009787'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='8 0.371134 0.357292 0.359372 0.352464 0.004828 0.364392 -0.005020\\n9 0.536082 0.438540 0.446126 0.353218 0.085322 0.442703 0.003423\\n10 0.711340 0.511441 0.519928 0.353989 0.157452 0.510142 0.009787\\n11 0.752577 0.571604 0.578186 0.354777 0.216827 0.571596 0.006590\\n12 0.783505 0.617085 0.618778 0.355584 0.261501 0.613425 0.005353\\n13 0.773196 0.651777 0.655123 0.356409 0.295368 0.649259 0.005864\\n14 0.618557 0.670202 0.676814 0.357253 0.312950 0.669555 0.007260\\n15 0.793814 0.679884 0.692592 0.358116 0.321768 0.684208 0.008384\\n16 0.793814 0.672488 0.696440 0.359000 0.313488 0.684764 0.011676\\n17 0.680412 0.648851 0.677696 0.359904 0.288947 0.662714 0.014983\\n18 0.525773 0.602496 0.630922 0.360828 0.241667 0.620368 0.010554\\n19 0.505155 0.537698 0.552296 0.361775 0.175923 0.552599 -0.000304\\n20 0.701031 0.463760 0.466442 0.362743 0.101016 0.477429 -0.010987\\n21 0.484536 0.395795 0.390958 0.363734 0.032061 0.408708 -0.017750\\n22 0.247423 0.337809 0.338500 0.364748 -0.026939 0.354028 -0.015528'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='21 0.484536 0.395795 0.390958 0.363734 0.032061 0.408708 -0.017750\\n22 0.247423 0.337809 0.338500 0.364748 -0.026939 0.354028 -0.015528\\n23 0.371134 0.292452 0.303902 0.365786 -0.073334 0.312588 -0.008686\\n24 0.216495 0.254359 0.258435 0.366848 -0.112489 0.270568 -0.012133\\n25 0.412371 0.227557 0.224291 0.367934 -0.140377 0.237846 -0.013555\\n26 0.237113 0.207962 0.201250 0.369046 -0.161084 0.219420 -0.018169\\n27 0.206186 0.196049 0.189439 0.370183 -0.174133 0.209743 -0.020304\\n28 0.206186 0.189030 0.182843 0.371346 -0.182316 0.207727 -0.024884\\n29 0.237113 0.194524 0.185734 0.372536 -0.178011 0.213194 -0.027460\\n30 0.206186 0.220227 0.215444 0.373753 -0.153526 0.242485 -0.027041\\n31 0.329897 0.279614 0.274624 0.374998 -0.095383 0.292834 -0.018210\\n32 0.371134 0.355078 0.358020 0.376270 -0.021193 0.365332 -0.007312\\n33 0.494845 0.437103 0.445832 0.377572 0.059531 0.441323 0.004510\\n34 0.690722 0.509515 0.520006 0.378903 0.130612 0.512064 0.007942'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='32 0.371134 0.355078 0.358020 0.376270 -0.021193 0.365332 -0.007312\\n33 0.494845 0.437103 0.445832 0.377572 0.059531 0.441323 0.004510\\n34 0.690722 0.509515 0.520006 0.378903 0.130612 0.512064 0.007942\\n35 0.989691 0.570761 0.579003 0.380263 0.190497 0.569851 0.009152\\n36 1.000000 0.615868 0.623981 0.381654 0.234214 0.617254 0.006728\\n37 0.845361 0.651487 0.656782 0.383076 0.268411 0.650336 0.006446\\n38 0.742268 0.670664 0.678412 0.384528 0.286136 0.673055 0.005357\\n39 0.721649 0.680534 0.691961 0.386013 0.294521 0.684347 0.007614\\n40 0.567010 0.671607 0.692853 0.387530 0.284078 0.683297 0.009555\\n41 0.546392 0.648851 0.672476 0.389079 0.259771 0.660613 0.011863\\n42 0.432990 0.599785 0.621940 0.390662 0.209123 0.615426 0.006514\\n43 0.391753 0.537520 0.544543 0.392279 0.145241 0.549961 -0.005417\\n44 0.443299 0.462772 0.457700 0.393930 0.068842 0.471080 -0.013380\\n45 0.422680 0.397098 0.380324 0.395616 0.001482 0.401229 -0.020905\\n46 0.381443 0.342213 0.325583 0.397337 -0.055124 0.347827 -0.022244'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='45 0.422680 0.397098 0.380324 0.395616 0.001482 0.401229 -0.020905\\n46 0.381443 0.342213 0.325583 0.397337 -0.055124 0.347827 -0.022244\\n47 0.257732 0.297711 0.287130 0.399094 -0.101384 0.304270 -0.017140\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware\\nSequential Autoencoder\\nTiexin Qin1 Shiqi Wang1 Haoliang Li1\\nAbstract\\nDomain generalization aims to improve the gen-\\neralization capability of machine learning sys-\\ntems to out-of-distribution (OOD) data. Exist-\\ning domain generalization techniques embark\\nupon stationary and discrete environments to\\ntackle the generalization issue caused by OOD\\ndata. However, many real-world tasks in non-\\nstationary environments ( e.g., self-driven car\\nsystem, sensor measures) involve more complex\\nand continuously evolving domain drift, which\\nraises new challenges for the problem of do-\\nmain generalization. In this paper, we formu-\\nlate the aforementioned setting as the problem of\\nevolving domain generalization. Speciﬁcally, we\\npropose to introduce a probabilistic framework\\ncalled Latent Structure-aware Sequential Autoen-\\ncoder (LSSAE) to tackle the problem of evolving\\ndomain generalization via exploring the under-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='propose to introduce a probabilistic framework\\ncalled Latent Structure-aware Sequential Autoen-\\ncoder (LSSAE) to tackle the problem of evolving\\ndomain generalization via exploring the under-\\nlying continuous structure in the latent space of\\ndeep neural networks, where we aim to identify\\ntwo major factors namely covariate shift and con-\\ncept shift accounting for distribution shift in non-\\nstationary environments. Experimental results on\\nboth synthetic and real-world datasets show that\\nLSSAE can lead to superior performances based\\non the evolving domain generalization setting.\\n1. Introduction\\nThe success of machine learning techniques typically lies on\\nthe assumption that training data and test data are sampled\\nindependently and identically from similar distributions.\\nHowever, this assumption does not hold when deploying the\\ntrained model in many real-world environments where the\\ndistribution of test data varies from training data. This dis-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='However, this assumption does not hold when deploying the\\ntrained model in many real-world environments where the\\ndistribution of test data varies from training data. This dis-\\ntribution discrepancy, so-called distribution shift, can lead\\n1City University of Hong Kong, Hong Kong. Correspondence\\nto: Haoliang Li <haoliang.li1991@gmail.com>.\\nProceedings of the 39 th International Conference on Machine\\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-\\nright 2022 by the author(s).\\nto the dramatic performance decrease of machine learning\\nmodels (Torralba & Efros, 2011). To mitigate this issue,\\ndomain generalization (DG) has been proposed to learn a\\nmore robust model which can be better generalized to OOD\\ndata (Muandet et al., 2013; Balaji et al., 2018; Li et al.,\\n2018b).\\nWhile some progress is being achieved so far, existing DG\\nmethods are limited to the setting of generalization among\\ndiscrete and stationary environments. This setting can be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='2018b).\\nWhile some progress is being achieved so far, existing DG\\nmethods are limited to the setting of generalization among\\ndiscrete and stationary environments. This setting can be\\nproblematic in some real-world applications where we re-\\nquire that model can be generalized among continuous do-\\nmains (Hoffman et al., 2014). For example, a self-driving\\ncar system, when deployed in the real world, struggles to\\nperform under an open environment where the accepted data\\nchanges naturally according to the geographic location, time\\nintervals, and other factors in a gradual manner (Hoffman\\net al., 2014). For another example, the measures of sen-\\nsors can also drift over time due to the outer environments\\nand inter factors, such as aging (Vergara et al., 2012). In\\nthese scenarios, treating each domain in a separate manner\\nis unlikely to yield the desired performance as it does not\\nconsider the property of continuous domain structure.\\nAnother limitation of most of the existing DG methods is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='is unlikely to yield the desired performance as it does not\\nconsider the property of continuous domain structure.\\nAnother limitation of most of the existing DG methods is\\nthat they did not take “concept shift” into consideration.\\nSuch concept shift can also lead to performance drop (Fed-\\nerici et al., 2021). A typical example of concept shift would\\nbe that the incidence rate of a particular disease in cer-\\ntain groups may change over time due to the development\\nof treatments and preventive measures. Therefore, exist-\\ning DG techniques may fail to be applied to some other\\ncomplex real-world applications in non-stationary environ-\\nments (Sugiyama et al., 2013; Tahmasbi et al., 2021).\\nIn this paper, we propose to focus on the problem of domain\\ngeneralization based on the non-stationary setting, where\\ndata can evolve gradually with both covariate shift and con-\\ncept shift. Particularly, we formulate this non-stationary\\nscenario as evolving domain generalization where we only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='data can evolve gradually with both covariate shift and con-\\ncept shift. Particularly, we formulate this non-stationary\\nscenario as evolving domain generalization where we only\\nhave access to adequate labeled examples from the sequen-\\ntial source domains. Our objective is to develop algorithms\\nthat can explore the underlying continuous structure of dis-\\ntribution shift and generalize well to evolving target domains\\nwhere the samples are unavailable during the training stage.\\narXiv:2205.07649v2  [cs.LG]  16 Jun 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nUnlike existing DG methods that only focus on covariate\\nshift based on the stationary environment, in this paper, we\\npropose a novel framework called Latent Structure-aware\\nSequential Autoencoder (LSSAE), a dynamic probabilistic\\nframework to model the underlying latent variables across\\ndomains. More speciﬁcally, we propose to use two latent\\nvariables to represent the sampling bias in data sample space\\n(i.e., covariate shift) and data category space (i.e., concept\\nshift), and propose a domain-related module and a category-\\nrelated module to infer their dynamic transition functions\\nbased on different time stamps. We conduct extensive ex-\\nperiments to verify that our framework can successfully\\ncapture the underlying covariate shift and interpret the con-\\ncept shift simultaneously. Last but not least, we show that\\nour proposed LSSAE has a promising generation capability'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='capture the underlying covariate shift and interpret the con-\\ncept shift simultaneously. Last but not least, we show that\\nour proposed LSSAE has a promising generation capability\\nto predict unseen target domains, which can be helpful to\\nthe problem related to sequential data generation. The main\\ncontributions of this paper are summarized as follows.\\n• We propose to focus on the problem of non-stationary\\nevolving domain generalization where both covariate\\nshift and concept shift may exist in the setting.\\n• We propose a novel probabilistic framework LSSAE\\nwhich incorporates variational inference to identify the\\ncontinuous latent structures of these two shifts sepa-\\nrately and simultaneously.\\n• We provide empirical results to show that the proposed\\napproach yield better results than other DG methods\\nacross scenarios. Besides, it presents a powerful gener-\\nation ability of predicting unseen evolving domains.\\n2. Related Work\\nDomain Generalization (DG). The goal of DG is to learn'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='across scenarios. Besides, it presents a powerful gener-\\nation ability of predicting unseen evolving domains.\\n2. Related Work\\nDomain Generalization (DG). The goal of DG is to learn\\nrobust models which can generalize well towards the out-\\nof-distribution samples from unseen domains. Existing DG\\nmethods commonly rely on multiple source domains to learn\\nrepresentative features that can be better generalized. Ac-\\ncording to various strategies used to learn these representa-\\ntions, we can roughly categorize them into three catogories.\\nThe ﬁrst type is feature-based methods, which aim to learn\\ndomain-invariant representation which can be better gener-\\nalized to target domains. Speciﬁcally, it can be achieved by\\naligning the distribution of representations from all source\\ndomains (Blanchard et al., 2011; Li et al., 2018b; Albu-\\nquerque et al., 2021) and feature disentanglement (Ilse et al.,\\n2020; Wang et al., 2021; Nguyen et al., 2021). The second'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='domains (Blanchard et al., 2011; Li et al., 2018b; Albu-\\nquerque et al., 2021) and feature disentanglement (Ilse et al.,\\n2020; Wang et al., 2021; Nguyen et al., 2021). The second\\ncategory is meta-learning based methods, which utilize the\\nmodel agnostic training procedure to stimulate the train/test\\nshift for acquiring generalized models (Li et al., 2018a; Bal-\\naji et al., 2018; Dou et al., 2019). Last but not least, data\\naugmentation-based techniques, which aim to manipulate\\nthe perturbation both in original images and features to stim-\\nulate the unseen target domains, can also beneﬁt the problem\\nof domain generalization (V olpi et al., 2018; Shankar et al.,\\n2018; Zhou et al., 2021).\\nContinuous Domain Adaptation (CDA).The problem of\\ncontinuous domain adaptation (i.e., evolving domain adap-\\ntation) has attracted increasing attention recently, where\\nthe CDA methods can be categorized into the intermediate-\\ndomains based methods (Kumar et al., 2020; Gong et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='tation) has attracted increasing attention recently, where\\nthe CDA methods can be categorized into the intermediate-\\ndomains based methods (Kumar et al., 2020; Gong et al.,\\n2019; Chen & Chao, 2021), domain manifold based meth-\\nods (Hoffman et al., 2014; Li et al., 2017), adversary-based\\napproaches (Wang et al., 2020; Wulfmeier et al., 2018), and\\nmeta-learning based methods (Liu et al., 2020; Lao et al.,\\n2020). More or less, they require some samples from tar-\\nget domains for adaptation. In Mancini et al. (2019), they\\npropose to substitute this reliance with some metadata from\\ntarget domains as additional supervision. Instead, our focus\\nis continuous domain generalization where no information\\nfrom target domains is accessible for model learning, which\\nis a more challenging but realistic task for real-world appli-\\ncations.\\nSequential Data Generation.Recent process in unsuper-\\nvised sequence generation (Yingzhen & Mandt, 2018; Han'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='is a more challenging but realistic task for real-world appli-\\ncations.\\nSequential Data Generation.Recent process in unsuper-\\nvised sequence generation (Yingzhen & Mandt, 2018; Han\\net al., 2021; Park et al., 2021) suggests the importance of\\ndecoupling time-invariant and time-variant information dur-\\ning the representation learning procedure. However, these\\napproaches only take sequence generation tasks into consid-\\neration and fail to consider the category-related information,\\nwhich is important for the problem of domain generalization.\\nUnlike these approaches, we propose jointly focusing on\\nthe dynamic modeling of time-variant information on both\\ndata sample space and category space across domains.\\n3. Methodology\\nIn this section, we ﬁrst formalize the problem of evolving\\ndomain generalization based on the non-stationary environ-\\nment and then describe our framework LSSAE for address-\\ning this problem. After that, we will provide theoretical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='domain generalization based on the non-stationary environ-\\nment and then describe our framework LSSAE for address-\\ning this problem. After that, we will provide theoretical\\nanalysis for the proposed framework in Sec. 3.3 and imple-\\nmentation in Sec. 3.4.\\n3.1. Problem Formulation\\nSuppose we are given T sequentially arriving source do-\\nmains S = {D1,D2,..., DT }, where each domain Dt =\\n{(xt,i,yt,i)}nt\\ni=1 is comprised of nt labeled samples for\\nt ∈ {1,2,...,T }. The goal of our problem setting is\\nto train a classiﬁcation model on S which can be gen-\\neralized to M following arriving target domains T =\\n{DT+1,DT+2,..., DT+M }, Dt = {(xt,i)}nt\\ni=1 (t ∈{T +\\n1,T + 2,...,T + M}), which are not available during\\ntraining stage. For simplicity, we omit the index iwhen-\\never xi refers to a single data point. To further quantify\\nthe continuously evolving nature of domains, we denote'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nX Y\\nW\\nX Y\\nW\\nV\\nXt- 1 Yt- 1\\nWt- 1\\nVt- 1\\nXt Yt\\nWt\\nVt /gid00473/gid00473/gid00473\\n(a)\\n(b)\\n(c)\\n(d)\\n/gid01141/gid01141/gid01141\\nXt- 1 Yt- 1\\nWt- 1\\nX t Yt\\nWt /gid00473/gid00473/gid00473\\n/gid00473/gid00473/gid00473\\n/gid00473/gid00473/gid00473 \\n/gid00473/gid00473/gid00473\\nFigure 1.Comparison of causality diagram for stationary and non-\\nstationary domain generalization scenarios. (a) represents the\\nstandard stationary DG settings which only contains covariate shift\\n(P(X) varies for source and target domains). (b) is an extension\\nversion of (a) which contains both covariate shift and concept shift\\n(P(Y |X) varies). (c) and (d) are corresponding non-stationary\\nDG settings where there exist evolving patterns among adjacent\\ndomains.\\n0 ≤D(Dt,Dt+1) ≤ϵ for two consecutive domains un-\\nder some distribution distance function D (e.g., Kullback-\\nLeibler distance). In other words, the discrepancy between'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='domains.\\n0 ≤D(Dt,Dt+1) ≤ϵ for two consecutive domains un-\\nder some distribution distance function D (e.g., Kullback-\\nLeibler distance). In other words, the discrepancy between\\ntwo consecutive domains is bounded.\\nConventional DG setting only assumes that P(X)\\nvaries (i.e., covariate shift) for different domains (See\\nFig. 1 (a)), which may not be ideal since both P(X) and\\nP(Y|X) can be non-stationary (i.e., P(X) and P(Y|X)\\nvary over time which lead to evolving covariate shift and\\nconcept shift, respectively). To tackle this problem, in this\\npaper, we aim to explore the evolving patterns of covariate\\nshift and concept shift across domains.\\n3.2. LSSAE: Latent Structure-aware Sequential\\nAutoencoder\\nTo model the dynamic in non-stationary systems, we con-\\nsider two independent factors W and V (i.e., W ⊥ ⊥V)\\nwhich account for the distribution drift in data sample space\\n(i.e., covariate shift) and data category space (i.e., con-\\ncept shift) respectively according to different time stamps'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='which account for the distribution drift in data sample space\\n(i.e., covariate shift) and data category space (i.e., con-\\ncept shift) respectively according to different time stamps\\n(See Fig. 1 (d)). For data (xt,yt) collected at time stamp t,\\nwe denote zw\\nt and zv\\nt as the latent variables of W and V at\\ntime stamp t. For completeness, we further consider time-\\ninvariant latent code zc to capture the static information of\\nxt. We thus can deﬁne a probabilistic generative model for\\nthe joint distribution of all source domains as\\np(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\n= p(x1:T ,zw\\n1:T ,zc)p(y1:T ,zv\\n1:T |zc). (1)\\nwhere the ﬁrst term p(x1:T ,zw\\n1:T ,zc) and second term\\np(y1:T ,zv\\n1:T |x1:T ) can be formulated by using Markov\\nchain model as\\np(x1:T ,zw\\n1:T ,zc) =p(zc)\\nT∏\\nt=1\\np(zw\\nt |zw\\n<t) p(xt|zc,zw\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\ncovariate shift\\n,\\n(2)\\np(y1:T ,zv\\n1:T |zc) =\\nT∏\\nt=1\\np(zv\\nt |zv\\n<t) p(yt|zc,zv\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\nconcept shift\\n, (3)\\nand p(xt|zc,zw\\nt ) and p(yt|zc,zv\\nt ) denote covariate shift and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='<t) p(xt|zc,zw\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\ncovariate shift\\n,\\n(2)\\np(y1:T ,zv\\n1:T |zc) =\\nT∏\\nt=1\\np(zv\\nt |zv\\n<t) p(yt|zc,zv\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\nconcept shift\\n, (3)\\nand p(xt|zc,zw\\nt ) and p(yt|zc,zv\\nt ) denote covariate shift and\\nconcept shift, respectively. Eq. 2 shows that the generation\\nprocess of domains data xt at time stamp tdepends on the\\ncorresponding dynamic latent code zw\\nt and static code zc,\\nand Eq. 3 shows that the inference process (i.e., classiﬁer to\\nproduce yt) rely on the corresponding zv\\nt and zc. Our ob-\\njective is to learn the classiﬁer p(yt|zc,zv\\nt ) which disposes\\nof covariate shift through zc and concept shift with dynamic\\nzv\\nt for the problem of evolving domain generalization.\\nDomain-related module for covariate shift.To model\\np(x1:T ,zw\\n1:T ,zc) where covariate shift involved, we set\\nthe prior distribution as p(zc) = N(0,I), p(zw\\nt |zw\\n<t) =\\nN(µ(zw\\nt ),σ2(zw\\nt )) which can be parameterized by some re-\\ncurrent neural networks (e.g., LSTM (Hochreiter & Schmid-\\nhuber, 1997)) by setting zw'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='t |zw\\n<t) =\\nN(µ(zw\\nt ),σ2(zw\\nt )) which can be parameterized by some re-\\ncurrent neural networks (e.g., LSTM (Hochreiter & Schmid-\\nhuber, 1997)) by setting zw\\n0 = 0 for initial state when t= 0,\\nand p(xt|zc,zw\\nt ) as a conditional decoder for the reconstruc-\\ntion of input data xt. To approximate the prior distributions\\np(zw\\nt |zw\\n<t), we propose to use variational inference to learn\\nan approximate posterior distribution qover latent variables\\ngiven data which can be formulated as\\nq(zw\\n1:T ,zc|x1:T ) =q(zc|x1:T )\\nT∏\\nt=1\\nq(zw\\nt |zw\\n<t,xt), (4)\\nwhere q(zc|x1:T ) and q(zw\\nt |zw\\n<t,xt) can be also parameter-\\nized by neural networks. The objective function for latent\\nfeature representation learning can be derived based on the\\nevidence lower bound (ELBO) form (Kingma & Welling,\\n2014) given as\\nLd =\\nT∑\\nt=1\\nEq(zc|xt)q(zw\\nt |zw\\n<t,xt)\\n[\\nlog p(xt|zc,zw\\nt )\\n]\\n−λ1DKL(q(zc|x1:T ),p(zc))\\n−λ2\\nT∑\\nt=1\\nDKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t)),\\n(5)\\nwhere the ﬁrst term denotes the reconstruction term for input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='T∑\\nt=1\\nEq(zc|xt)q(zw\\nt |zw\\n<t,xt)\\n[\\nlog p(xt|zc,zw\\nt )\\n]\\n−λ1DKL(q(zc|x1:T ),p(zc))\\n−λ2\\nT∑\\nt=1\\nDKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t)),\\n(5)\\nwhere the ﬁrst term denotes the reconstruction term for input\\ndata xt, the second and third term denote KL divergence\\nwhich are to align the posterior distributions zc and zw\\nt with\\nthe corresponding prior distributions.\\nCategory-related module for concept shift. To model\\np(y1:T ,zv\\n1:T |zc) for classiﬁcation purpose where concept'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nshift is involved, we propose to introduce another module\\nwhich can be easily integrated into our uniﬁed framework\\nunder domain generalization scenarios. Speciﬁcally, we\\npropose to model p(y1:T ,zv\\n1:T |zc) with a dynamic distri-\\nbution p(yt,zv\\nt |zc) where the zv\\nt is varied and encode the\\nshift information in the data category space ( e.g., the pro-\\nportion of each category). The module can be optimized\\nvia maximizing the distribution p(y1:T ,zv\\n1:T |zc) given a se-\\nquence of domains. In practise, we represent p(zv\\nt |zv\\n<t) as\\np(zv\\nt |zv\\n<t) =Cat(π(zv\\n<t)) which is a learnable categorical\\ndistribution. In a similar vein with zw\\nt , we utilize a distribu-\\ntion qto model the posterior distribution and approximate\\nthe prior distribution of zv\\nt by adopting variational inference\\ngiven as\\nq(zv\\n1:T |y1:T ) =\\nT∏\\nt=1\\nq(zv\\nt |zv\\n<t,yt), (6)\\nwhere q(zv\\nt |zv\\n<t,yt) can be parameterized by a recurrent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='the prior distribution of zv\\nt by adopting variational inference\\ngiven as\\nq(zv\\n1:T |y1:T ) =\\nT∏\\nt=1\\nq(zv\\nt |zv\\n<t,yt), (6)\\nwhere q(zv\\nt |zv\\n<t,yt) can be parameterized by a recurrent\\nneural network with categorical distribution as output. We\\nset zv\\n0 = 0 for initial state when t = 0. The proposed\\nmodule can be jointly trained with the inference process in\\nEq. 6 as well as classiﬁcation loss by maximizing\\nLc =\\nT∑\\nt=1\\nEq(zv\\nt |zv\\n<t,yt)\\n[\\nlog p(yt|zc,zv\\nt )\\n]\\n−λ3\\nT∑\\nt=1\\nDKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t)).\\n(7)\\nHere, the ﬁrst term denotes the classiﬁcation loss (i.e., max-\\nimizing log likelihood) and the second term denotes KL\\ndivergence which aims to align the posterior distribution of\\nzv\\nt with its prior distribution.\\nTemporal smooth constraint for better stability. We\\nempirically ﬁnd that the estimation of conditional den-\\nsity (i.e., p(zw\\nt |zw\\n<t) and p(zv\\nt |zv\\n<t)) which is to model\\nthe complex dynamics over temporal transition may not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='empirically ﬁnd that the estimation of conditional den-\\nsity (i.e., p(zw\\nt |zw\\n<t) and p(zv\\nt |zv\\n<t)) which is to model\\nthe complex dynamics over temporal transition may not\\nbe stable based on our formulation above. We conjecture\\nthe reason that some of the static information can be dis-\\ntorted by the dynamic inference module q(zw\\nt |zw\\n<t,xt) and\\nq(zv\\nt |zv\\n<t,yt) for better reconstruction quality, which fur-\\nther yields sub-optimal results for recognition tasks. Intu-\\nitively, one can tackle this limitation by reducing the dimen-\\nsion of latent codes zw\\nt and zv\\nt or decreasing the learning\\nrate of corresponding inference modules and prior modules\\nmanually to achieve better decoupling effect. However, we\\nﬁnd that such strategy may not lead to desired performance.\\nIn our work, we propose to employ Lipschitz constrain over\\nthe temporal domain to stabilize the learning of the dynamic\\ninference modules as follows\\n|q(zw\\nt |zw\\n<t,xt) −q(zw\\nt−1|zw\\n<t−1,xt−1)|≤ α,\\n|q(zv\\nt |zv\\n<t,yt) −q(zv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='the temporal domain to stabilize the learning of the dynamic\\ninference modules as follows\\n|q(zw\\nt |zw\\n<t,xt) −q(zw\\nt−1|zw\\n<t−1,xt−1)|≤ α,\\n|q(zv\\nt |zv\\n<t,yt) −q(zv\\nt−1|zv\\n<t−1,yt−1)|≤ α, (8)\\nwhere αis referred to as a Lipschitz constant. The above\\nregularization term is denoted as TS constraint for simplicity.\\nWe expect it can help with reducing the potential for volatile\\ntraining.\\nObjective function.Given training data S, our proposed\\nframework can be optimized through the objective function\\nLLSSAE = Ld + Lc with the temporal smooth constrains\\nin Eq. 8, where the ﬁrst term Ld and second term Lc aim\\nto tackle the problem of covariate shift and concept shift,\\nrespectively.\\nDiscussion. It is worth noting that there exists some works\\nusing probabilistic graph model for future video frame gen-\\neration task (Yingzhen & Mandt, 2018; Han et al., 2021),\\nwhich are similar to our proposed method at a high level.\\nNevertheless, our method is different since 1) our proposed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='eration task (Yingzhen & Mandt, 2018; Han et al., 2021),\\nwhich are similar to our proposed method at a high level.\\nNevertheless, our method is different since 1) our proposed\\nframework can be applied not only to generation task (as\\nwe show in the ablation study of experimental section) but\\nalso to evolving domain generalization task (which is the\\nmain focus in our paper); 2) in order to ﬁt our framework\\nto the non-stationary recognition task, we introduce a novel\\ncategory-related module to capture the concept shift, as\\nsuch, better generalization performance can be achieved.\\n3.3. Theoretical Analysis\\nIn this section, we aim to give a theoretical insight on our\\nproposed method by extending variational inference from\\nstationary environments to non-stationary environments.\\nProbabilistic model for stationary environments. We\\nﬁrst elaborate our proposed framework based on the sta-\\ntionary condition, where zc, zw and zv are introduced to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Probabilistic model for stationary environments. We\\nﬁrst elaborate our proposed framework based on the sta-\\ntionary condition, where zc, zw and zv are introduced to\\ncapture the domain-invariant category information, domain-\\nspeciﬁc and category information, respectively. We thus\\nhave the following theorem.\\nTheorem 3.1. For the data log likelihood log p(x,y) in a\\nstationary environment, we have the evidence lower bound\\nmax\\np,q\\nEzc,zw,zv\\n[\\nlog p(x|zc,zw)p(y|zc,zv)\\n]\\n−DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw))\\n−DKL(q(zv|y),p(zv)),\\n(9)\\nwhere zc ∼q(zc|x),zw ∼q(zw|x),zv ∼q(zv|y).\\nProof. We consider the data generation procedure\\np(x,y,zc,zw,zv). We have\\nlog p(x,y) =DKL(q(zc,zw,zv|x,y),p(zc,zw,zv|x,y))\\n+ Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y) ,\\n(10)\\nwhere the ﬁrst term is the KL divergence of the approximate\\nfrom the true posterior. Since this term is non-negative,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nthe second term is called the evidence lower bound on the\\nmarginal likelihood of data (x,y). This can be written as\\nlog p(x,y) ≥Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y)\\n= Ezc,zw,zv\\n[\\nlog p(x|zc,zw)p(y|zc,zv)\\n]\\n−DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw))\\n−DKL(q(zv|y),p(zv)).\\n(11)\\nThis completes the proof.\\nTheorem 3.1 shows that for a speciﬁed domain, the latent\\nspace can be decoupled into a domain invariant subspace, a\\ndomain related subspace and a category related subspace.\\nProbabilistic model for non-stationary environ-\\nments. We now extend the aforementioned analysis to\\nthe non-stationary environment. Speciﬁcally, for the\\nnon-stationary environment, we can cast the dynamic\\nvariational inference framework via modeling the sequence\\nof latent variables zw and zv as two parallel Markov\\nchains (i.e., p(zw) = p(zw\\nt |zw\\n<t) and p(zv) = p(zv\\nt |zv\\n<t)).\\nWe thus have the following theorem.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='of latent variables zw and zv as two parallel Markov\\nchains (i.e., p(zw) = p(zw\\nt |zw\\n<t) and p(zv) = p(zv\\nt |zv\\n<t)).\\nWe thus have the following theorem.\\nTheorem 3.2. By denoting p(zw) = p(zw\\nt |zw\\n<t) and\\np(zv) = p(zv\\nt |zv\\n<t)), LLSSAE is equivalent to the ELBO\\nof the data log likelihood log p(x1:T ,y1:T ) based on the\\nnon-stationary environment setting.\\nProof. We can reformulate the lower bound in Eq. 9 for a\\nnon-stationary environment as\\nlog p(x1:T ,y1:T ) ≥Eq log p(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\nq(zc,zw\\n1:T ,zv\\n1:T |x1:T ,y1:T ) ,\\n(12)\\nwhich can be written as\\nlog p(x1:T ,y1:T )\\n≥\\nT∑\\nt=1\\nEzc,zw\\nt ,zv\\nt\\n[\\nlog p(xt|zc,zw\\nt )p(y|xc,zv\\nt )\\n]\\n−DKL(q(zc|x1:T ),p(zc))\\n−\\nT∑\\nt=1\\nDKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t))\\n−\\nT∑\\nt=1\\nDKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t)),\\n(13)\\nwhere zc ∼ q(zc|x1:T ),zw\\nt ∼ q(zw\\nt |zw\\n<t,xt) and zv\\nt ∼\\nq(zv\\nt |zv\\n<t,yt). The detailed derivation procedure is pro-\\nvided in App. A. We can see that the reconstruction part for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='t |zv\\n<t)),\\n(13)\\nwhere zc ∼ q(zc|x1:T ),zw\\nt ∼ q(zw\\nt |zw\\n<t,xt) and zv\\nt ∼\\nq(zv\\nt |zv\\n<t,yt). The detailed derivation procedure is pro-\\nvided in App. A. We can see that the reconstruction part for\\nxt in the ﬁrst term together with the second and third terms\\ncan form our objective Ld for domain-related module (i.e.,\\ncovariate shift), and the combination of the reconstruction\\npart for yt in the ﬁrst term and the last term can form Lc for\\ncategory-related module (i.e., concept shift). As a result, the\\nformulation above is equivalent to our objective LLSSAE .\\nE\\nE\\n...\\n...\\n...\\n...\\nD\\nC\\nE ... ...\\nx\\ny/gid00466/gid00507/gid00021 \\n/gid00466/gid00507/gid00021 \\ny/gid00466/gid00507/gid00021 \\nx/gid00466/gid00507/gid00021 \\n/gid00031/gid00052/gid00041/gid00028/gid00040/gid00036/gid00030/gid00001 /gid00043/gid00535 z/gid00001/gid00001/gid00536\\n/gid00031/gid00052/gid00041/gid00028/gid00040/gid00036/gid00030/gid00001 /gid00043/gid00535 z/gid00001/gid00001/gid00001/gid00536'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='/gid00031/gid00052/gid00041/gid00028/gid00040/gid00036/gid00030/gid00001 /gid00043/gid00535 z/gid00001/gid00001/gid00001/gid00536\\n/gid00046/gid00047/gid00028/gid00047/gid00036/gid00030/gid00001 /gid00043/gid00535 z /gid00536\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00466\\n~\\n~\\nTS constraint \\nTS constraint \\n/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032 \\n/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032 \\n/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032 \\n/gid00030\\n/gid00049\\n/gid00050\\n/gid00050\\n/gid00049\\n/gid00030\\nz/gid00050\\n/gid00466/gid00507/gid00021 \\nz /gid00049\\n/gid00466/gid00507/gid00021 \\nz/gid00030\\n/gid00049\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00467\\n/gid00049\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00021\\n/gid00049\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00466\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00467\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00021\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 \\nF\\n/gid00030'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00467\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00021\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 \\nF\\n/gid00030\\n/gid00050\\nF\\n/gid00049\\n/gid00043/gid00042/gid00046/gid00047/gid00032/gid00045/gid00036/gid00042/gid00045 \\n/gid00043/gid00042/gid00046/gid00047/gid00032/gid00045/gid00036/gid00042/gid00045 \\n/gid00043/gid00042/gid00046/gid00047/gid00032/gid00045/gid00036/gid00042/gid00045 \\nFigure 2.An overview of network architecture for LSSAE. Our\\nframework consists of the static variational encoding network Ec,\\ndynamic variational encoding networksEw and Ev, dynamic prior\\nnetworks Fw and Fv, a decoder D and a classiﬁer C. It is worth\\nnoting that we do not requireEv (i.e., only data from target domain\\nTavailable) during inference stage.\\n3.4. Implementation\\nThe implementation of network architecture for LSSAE is\\ndepicted in Fig. 2. It is composed of two parts: (1) the\\ndomain-related module (middle and bottom region); (2) the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='3.4. Implementation\\nThe implementation of network architecture for LSSAE is\\ndepicted in Fig. 2. It is composed of two parts: (1) the\\ndomain-related module (middle and bottom region); (2) the\\ncategory-related module (top region).\\nThe domain-related module consists of a static variational\\nencoding network Ec for q(zc|x1:T ), a dynamic variational\\nencoding network Ew associated with q(zw\\nt |zw\\n<t,xt), a dy-\\nnamic prior network Fw working for p(zw\\nt |zw\\n<t) and a de-\\ncoder D corresponding to p(xt|zc,zw\\nt ). Similar to V AE\\n(Kingma & Welling, 2014), we can apply the reparameteri-\\nzation trick (Kingma & Welling, 2014) to optimize param-\\neters of Ec, Ew and Fw. Speciﬁcally, we implement Ec\\nby a feature extractor which will be also utilized to extract\\nfeatures during test time. Ew can be implemented by a\\nfeature extractor with same architecture of Ec but not shar-\\ning network parameters, and followed by a LSTM network.\\nFw is implemented as a one-layer LSTM network. For the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='feature extractor with same architecture of Ec but not shar-\\ning network parameters, and followed by a LSTM network.\\nFw is implemented as a one-layer LSTM network. For the\\ncategory-related module, we design a dynamic inference\\nnetwork Ew which takes the one-hot code of label yt as\\nthe input and with the output of the categorical distribution\\nq(zv\\nt |zv\\n<t,yt), and a classiﬁer C which takes zc and zv\\nt as\\nthe input for p(yt|zc,zv\\nt ). Similarly, the prior network Fv\\nfor p(zv\\nt |zv\\n<t) is a LSTM network with a categorical distri-\\nbution as the output. After that, we use Gumbel-Softmax\\nreparameterization trick (Jang et al., 2017; Maddison et al.,\\n2017) to sample zv\\nt for optimization. Regarding the clas-\\nsiﬁer C, we utilize a linear layer by following Gulrajani\\n& Lopez-Paz (2021). The details of our architecture and\\nhyperparameters for objective function can be found in the\\nsupplementary material.\\nOptimization. We ﬁrst initialize zw and zv as zw\\n0 = 0 and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nzv\\n0 = 0, respectively. To train our framework, we generate\\nthe dynamic prior distributions p(zw\\nt |zw\\n<t) and p(zv\\nt |zv\\n<t)\\nthrough Fw and Fv respectively based onT source domains\\nS. For time stamp t, we sample a batch of data xt and take\\nit as input for Ec and Ew to obtain the parameters of their\\nposterior distributions. After that, the latent features zc\\nand zw\\nt are resampled separately through reparameterization\\ntrick and then concatenated together. Finally the decoder\\nD outputs the reconstruction data of xt. Meanwhile, Ev\\ntakes the corresponding labels yt of xt in one-hot format\\nas input and outputs the latent features, and then the latent\\nfeatures are also resampled through reparameterization trick\\nto obtain zv\\nt . The classiﬁer C takes zc and zv\\nt as the input\\nto predict the labels of xt. During training, we sample a\\nmini-batch from each single domain with the same number'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='to obtain zv\\nt . The classiﬁer C takes zc and zv\\nt as the input\\nto predict the labels of xt. During training, we sample a\\nmini-batch from each single domain with the same number\\nof data sample to form a large batch in order to suit our\\nframework to the temporal smooth constraint for stable\\ntraining. This optimization procedure of LSSAE is depicted\\nin Algorithm 1.\\nInference. To predict the label of xt sampled from one\\nof the target domains in Dt in T, we adopt Fv to infer the\\nlatent code zv\\nt and Ec to extract the latent features zc, and\\nthen apply the classiﬁer Cfor the prediction purpose. It is\\nworth noting that we do not require Ev (i.e., only data from\\ntarget domain Tavailable) during inference stage. We show\\nthis procedure in Algorithm 2.\\n4. Experiments\\nIn this section, we present experimental results to validate\\nthe effectiveness of our proposed LSSAE based on the set-\\nting of evolving domain generalization.\\n4.1. Experimental Setup'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='4. Experiments\\nIn this section, we present experimental results to validate\\nthe effectiveness of our proposed LSSAE based on the set-\\nting of evolving domain generalization.\\n4.1. Experimental Setup\\nWe compare the proposed LSSAE with other DG methods\\non two synthetic datesets (Circle and Sine) and four real-\\nworld datasets (Rotated MNIST, Portraits, Caltran, Power-\\nSupply). We also evaluate the results on two variants named\\nCircle-C and Sine-C derived from Circle and Sine via syn-\\nthesizing concept shift manually. We split the domains into\\nsource domains, intermediate domains and target domains\\nwith the ratio of {1/2 : 1/6 : 1/3}. The intermediate do-\\nmains are utilized as validation set. More details of dataset\\nconstruction can be found in the supplementary material.\\n(1) Circle/-C (Pesaranghader & Viktor, 2016). This dataset\\ncontains evolving 30 domains where the instance are sam-\\npled from 30 2D Gaussian distributions. For Circle-C, con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='(1) Circle/-C (Pesaranghader & Viktor, 2016). This dataset\\ncontains evolving 30 domains where the instance are sam-\\npled from 30 2D Gaussian distributions. For Circle-C, con-\\ncept shift is introduced via changing the center and radius\\nof decision boundary in a gradual manner over time.\\n(2) Sine/-C (Pesaranghader & Viktor, 2016). We rearrange\\nthis dataset by extending it to 24 evolving domains. To\\nsimulate concept drift for Sine-C, labels are reversed (i.e.,\\nAlgorithm 1Optimization procedure for LSSAE\\nInput: sequential source labeled datasetsS; static feature\\nextractor Ec; dynamic inference networks Ew, Ev and\\ntheir corresponding prior networks Fw, Fv; decoder D\\nand classiﬁer C.\\nRandomly initialize Ec,Ew,Ev,Fw,Fv,D,C\\nAssign zw\\n0 ,zv\\n0 ←0\\nfor t= 1,2,...,K do\\nGenerate prior distribution p(zw\\nt |zw\\n<t) via Fw\\nGenerate prior distribution p(zv\\nt |zv\\n<t) via Fv\\nfor i= 1,2,... do\\nSample a batch of data (xt,yt) from St\\n⊿Calculate Ld by Eq. 5 for Ec, Ew, Dand Fw'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generate prior distribution p(zw\\nt |zw\\n<t) via Fw\\nGenerate prior distribution p(zv\\nt |zv\\n<t) via Fv\\nfor i= 1,2,... do\\nSample a batch of data (xt,yt) from St\\n⊿Calculate Ld by Eq. 5 for Ec, Ew, Dand Fw\\n⊿Calculate Lc by Eq. 7 for Ec, Ev, Cand Fv\\n⊿Calculate temporal smooth constriction by Eq. 8\\nUpdate all modules by the summary of these loss\\nend for\\nend for\\nAlgorithm 2Inference procedure for LSSAE\\nInput: sequential target datasets T; static feature extrac-\\ntor Ec; dynamic prior network Fv and classiﬁer C.\\nAssign zv\\n0 ←0\\nfor t= 1,2,... do\\nSample zv\\nt ∼p(zv\\nt |zv\\n<t) via Fv\\nfor i= 1,2,... do\\n⊿Extract the feature for data xt via Ec\\n⊿Generate the prediction ˜ yt via C\\nend for\\nend for\\nfrom 0 to 1 or from 1 to 0) from the 6-th domain to the last\\none.\\n(3) Rotated MNIST (RMNIST)(Ghifary et al., 2015). Ro-\\ntated MNIST (RMNIST) is composed of MNIST digits\\nof various rotations. We extend it to 19 evolving do-\\nmains via applying the rotations with degree of R =\\n{0◦,15◦,30◦,..., 180◦}in order.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='tated MNIST (RMNIST) is composed of MNIST digits\\nof various rotations. We extend it to 19 evolving do-\\nmains via applying the rotations with degree of R =\\n{0◦,15◦,30◦,..., 180◦}in order.\\n(4) Portraits (Ginosar et al., 2015). This dataset comprises\\nphotos of high-school seniors from the 1905s to the 2005s\\nfor gender classiﬁcation. We split the dataset into 34 do-\\nmains by a ﬁxed internal over time.\\n(5) Caltran (Hoffman et al., 2014). Caltran is a real-world\\nsurveillance dataset comprising images captured from a\\nﬁxed trafﬁc camera deployed in an intersection. The task is\\nto predict the type of scene based on continuously evolving\\ndata. We divide it into 34 domains based on different times.\\n(6) PowerSupply (Dau et al., 2019). PowerSupply is con-\\nstructed for the time-section prediction of current power\\nsupply based on the hourly records of an Italy electricity\\ncompany. The concept shift may raise from the change in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 1.The comparison of accuracy (%) between LSSAE and other DG baselines on various datasets. As DG baselines do not take\\nconcept shift into consideration, we report the results on datasets without concept shift and with concept shift separately.\\nAlgorithm Circle Sine RMNIST Portraits Caltran Avg Circle-C Sine-C PowerSupply Avg\\nERM 49.9 63.0 43.6 87.8 66.3 62.1 34.0 61.5 71.0 55.5\\nMixup 48.4 62.9 44.9 87.8 66.0 62.0 33.9 60.9 70.8 55.2\\nMMD 50.7 55.8 44.8 87.3 57.1 59.1 33.7 52.7 70.9 52.4\\nMLDG 50.8 63.2 43.1 88.5 66.2 62.3 34.6 62.0 70.8 55.8\\nIRM 51.3 63.2 39.0 85.4 64.1 60.6 38.5 61.2 70.8 56.8\\nRSC 48.0 61.5 41.7 87.3 67.0 61.1 33.7 61.5 70.9 55.4\\nMTL 51.2 62.9 41.7 89.0 68.2 62.6 33.9 61.4 70.7 55.3\\nFish 48.8 62.3 44.2 88.8 68.6 62.5 34.3 62.7 70.8 55.9\\nCORAL 53.9 51.6 44.5 87.4 65.7 60.6 34.1 59.0 71.0 54.7\\nAndMask 47.9 69.3 42.8 70.3 56.9 57.4 37.7 52.7 70.7 53.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Fish 48.8 62.3 44.2 88.8 68.6 62.5 34.3 62.7 70.8 55.9\\nCORAL 53.9 51.6 44.5 87.4 65.7 60.6 34.1 59.0 71.0 54.7\\nAndMask 47.9 69.3 42.8 70.3 56.9 57.4 37.7 52.7 70.7 53.7\\nDIV A 67.9 52.9 42.7 88.2 69.2 64.2 33.9 52.9 70.8 55.1\\nLSSAE (Ours) 73.8 71.4 46.4 89.1 70.6 70.3 44.8 60.8 71.1 58.9\\nseason, weather or price. We form 30 domains according to\\ndays.\\nThe methods for comparison include: (1) ERM (Vapnik,\\n1998); (2) Mixup (Yan et al., 2020); (3) MMD (Li et al.,\\n2018b); (4) MLDG (Li et al., 2018a); (5) IRM (Rosen-\\nfeld et al., 2021); (6) RSC (Huang et al., 2020);\\n(7) MTL (Blanchard et al., 2021); (8) Fish (Shi et al., 2021);\\n(9) CORAL (Sun & Saenko, 2016); (10) AndMask (Paras-\\ncandolo et al., 2021); (11) DIV A (Ilse et al., 2020). All of our\\nexperiments are implemented in the PyTorch platform based\\non DomainBed package (Gulrajani & Lopez-Paz, 2021). For\\na fair comparison, we keep the neural network architecture\\nof encoding part and classiﬁcation part to be same for all'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='on DomainBed package (Gulrajani & Lopez-Paz, 2021). For\\na fair comparison, we keep the neural network architecture\\nof encoding part and classiﬁcation part to be same for all\\nbaselines for different benchmarks.\\n4.2. Quantitative Results\\nThe results of of our proposed LSSAE and baselines are\\npresented in Table 1. As conventional DG methods focus\\nupon covariate shift only, we separate the datasets according\\nto with or without concept shift into two parts for fairness.\\nWe can see that LSSAE consistently outperforms other base-\\nlines over all datasets, it achieves 70.3% accuracy when\\nthere exists covariate shift only (Circle, Sine, RMNIST, Por-\\ntraits and Caltran), and achieves58.9% accuracy when there\\nexist concept shift (Circle-C, Sine-C, PowerSupply). The\\nresults are signiﬁcantly better than the compared DG ap-\\nproaches, which are reasonable since existing DG methods\\ncannot deal with distribution shift well in non-stationary\\nenvironments but our proposed LSSAE can properly cap-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='proaches, which are reasonable since existing DG methods\\ncannot deal with distribution shift well in non-stationary\\nenvironments but our proposed LSSAE can properly cap-\\nture the evolving patterns to gain better performance. More\\nresults can be found in the supplementary materials.\\nTo better understand the rationality of our method, we vi-\\nsualize the decision boundaries of our method and ERM\\nbaseline on two synthetic datasets: Circle and Sine by com-\\nparing our proposed method with ERM baseline. The vi-\\nsualization results are depicted in Fig. 3 and Fig. 4. As\\n(a) Data\\nEvolving P(X)\\n(b) Ground Truth (c) ERM (d) LSSAE\\nStatic P(Y|X)\\nEvolving P(Y|X)\\nCircle Circle-C \\nFigure 3.Decision boundary visualization of Circle and Circle-C\\ndatasets each with 30 domains. (a) presents the original data in\\ndifferent domains by color, where the right half part are source\\ndomains. (b) shows the positive and negative labels in red and\\nblue dots. (c) and (d) are decision boundaries learned by ERM and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='different domains by color, where the right half part are source\\ndomains. (b) shows the positive and negative labels in red and\\nblue dots. (c) and (d) are decision boundaries learned by ERM and\\nLSSAE, respectively.\\nEvolving P(X)\\n(a) Data (b) Ground Truth (c) ERM (d) LSSAE\\nStatic P(Y|X)\\nEvolving P(Y|X)\\nSine Sine-C \\nFigure 4.Decision boundary visualization of Sine and Sine-C\\ndatasets. (a) presents 24 domains indexed by different colors,\\nwhere the left half part are source domains. (b) shows the positive\\nand negative labels in red and blue dots. (c) and (d) are decision\\nboundaries learned by ERM and LSSAE, respectively.\\nshown in Fig. 3, both of ERM and LSSAE can ﬁt the source\\ndomains well. However, different from ERM which only ﬁt\\nthe source domains, LSSAE shows a desired generalization\\nability to unseen target domains. This validates that our\\nLSSAE can capture the underlying evolving patterns across\\ndomains to achieve better results. As for Circle-C where we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='ability to unseen target domains. This validates that our\\nLSSAE can capture the underlying evolving patterns across\\ndomains to achieve better results. As for Circle-C where we\\nintroduce evolving concept shift via modifying the center\\nand radius of decision boundary over a period of time, we\\nobserve that our proposed LSSAE can still produce a more\\naccurate decision boundary compared with ERM. Similar\\nobservation can be found in Fig. 4, where LSSAE can be\\nbetter generalized to evolving domains compared with ERM.\\nHowever, we ﬁnd that our proposed LSSAE may not be able'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\n(a) random data sequences \\n(b) reconstructions\\n(c) generated sequences with fixed\\n(d) generated sequences with fixed\\nzc\\nzwt\\n/gid00083\\nFigure 5.Visualisation of generated and reconstructed data se-\\nquences on RMNIST dataset.\\nto obtain a desired boundary based on the setting of Sine-C,\\nwhere we introduce concept shift by reversing the labels.\\nWe conjecture the reason that our proposed LSSAE may\\nnot be able to well handle the abrupt concept shift where no\\ncontinuous evolving pattern exists.\\n4.3. Ablation Study\\nAnalysis on domain-related module.To better evaluate\\nwhether our proposed LSSAE can capture domain spe-\\nciﬁc information, we conduct experiments by evaluating\\nthe reconstruction and generation capability of LSSAE on\\nRMNIST dataset. The reconstruction and generation re-\\nsults are shown in Fig. 5. Each subﬁgure shows two se-\\nquences which follow the domain order (i.e., rotation degree'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='RMNIST dataset. The reconstruction and generation re-\\nsults are shown in Fig. 5. Each subﬁgure shows two se-\\nquences which follow the domain order (i.e., rotation degree\\ngradually evolves) from the left to the right. Subﬁgure (a)\\nshows the original data sequence sampled from source do-\\nmains (green bounding box), intermediate domains (orange\\nbounding box) and target domains (pink bounding box) in\\norder. Noted that we are only interested in degree of rota-\\ntion thus the category of images from different domains may\\nnot be consistent. Subﬁgure (b) shows the reconstructions\\nfor each sample in Subﬁgure (a). Based on the results, we\\nﬁnd that we can achieve a desired reconstruction quality,\\nwhich suggests that the domain-related information can be\\ncaptured even we random the category related information.\\nTo further show that our proposed method is capable for\\ngeneration task, in subﬁgure (c), we visualize the randomly\\ngenerated samples using zw\\nt ∼p(zw\\nt |zw\\n<t) while keeping zc'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='To further show that our proposed method is capable for\\ngeneration task, in subﬁgure (c), we visualize the randomly\\ngenerated samples using zw\\nt ∼p(zw\\nt |zw\\n<t) while keeping zc\\nthe same for all domains. We can see that the category infor-\\nmation (i.e., digit information) remains unchanged when we\\nﬁx zc, and can generated samples for unseen future domains\\nwhere the rotation degree of digit is gradually changed with\\nevolving prior p(zw\\nt |zw\\n<t). This observation suggests that\\nour proposed method can be used for data augmentation\\nwhen adapting to the unseen target domains. However, we\\nalso observe that there exists image quality distortion when\\ngenerating new domains. We conjecture the reason that the\\nTable 2.Comparison of different prior distributions for category-\\nrelated module on PowerSupply dataset.\\nPrior Type Accuracy (%)\\nWithout zv 70.7\\nGaussian 70.1\\nUniform 71.0\\nCategorical (Ours) 71.1\\nrange of degrees for training may not be sufﬁciently diverse,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='related module on PowerSupply dataset.\\nPrior Type Accuracy (%)\\nWithout zv 70.7\\nGaussian 70.1\\nUniform 71.0\\nCategorical (Ours) 71.1\\nrange of degrees for training may not be sufﬁciently diverse,\\nwhich limits the performance of generation to unseen digit\\nrotation degrees. Subﬁgure (d) shows randomly generated\\ndata via sampling zc ∼p(zc) while keeping zw\\nt the same for\\nall domains at different time stamp t(i.e., zw\\nt = zw\\n1 ), where\\nwe ﬁnd that the generated digit images belong to different\\ncategories but with the same rotation degree, which further\\nindicates that our proposed method can successfully extract\\ndomain related information.\\nAnalysis on category-related module.We then evaluate\\nthe effectiveness of our proposed category-related module\\non PowerSupply dataset. To this end, we consider three\\ndifferent ablation studies by 1) removing category-related\\nmodule, 2) replacing the learnable categorical prior with a\\nlearnable Gaussian prior, 3) replacing the learnable categor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='different ablation studies by 1) removing category-related\\nmodule, 2) replacing the learnable categorical prior with a\\nlearnable Gaussian prior, 3) replacing the learnable categor-\\nical prior with a ﬁx Uniform prior. The results are shown\\nin Table 2. As we can see, our proposed LSSAE based on\\ncategorical prior can achieve the best performance among\\nother baselines, which shows the effectiveness of our pro-\\nposed method. However, we observe that the performance\\ndrops to some extent by replacing the categorical prior with\\nGaussian prior, which we conjecture the reason that cate-\\ngorical distribution can better capture the category related\\ninformation which is discrete. We also observe that better\\nperformance can be achieved by comparing with the results\\nusing Uniform distribution as prior, which is reasonable\\nsince Uniform distribution does not change over time.\\nAnalysis on temporal smooth constraint. The smooth\\nconstraint is mainly designed for stabilizing the training'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='since Uniform distribution does not change over time.\\nAnalysis on temporal smooth constraint. The smooth\\nconstraint is mainly designed for stabilizing the training\\nprocedure. To verify this, we conduct experiments by con-\\nsidering “with & without this constraint” on RMNIST and\\nCalTran datasets while keeping other hyperparameters the\\nsame. The results are reported as the variance of the test\\naccuracy in the last ﬁve epochs (See Table 3). For RMNIST,\\nwhen training with constraint, the variance is 0.4; without\\nconstraint, the variance is 3.6. For CalTran, when training\\nwith constraint, the variance is 4.7, and without constraint,\\nthe variance is 10.0. We can see that there exists an obvious\\ndecrease in terms of variance when incorporating with our\\nproposed TS constraint. Besides, as our smooth constraint\\nhelps stabilize the training process, better performance can\\nbe expected. For RMNIST, training with our smooth con-\\nstraint can achieve 0.9% performance gain; For CalTran,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 3.Ablation study on temporal smooth constraint.\\nTS Constraint RMNIST CalTran\\nVar ↓ Acc (%)↑ Var ↓ Acc (%)↑\\n× 3.6 45.5 10.0 69.9\\n✓ 0.4 46.4 4.7 70.6\\nthe performance gain is 0.7%.\\n5. Conclusion\\nIn this paper, we propose to focus on the problem of evolv-\\ning domain generalization, where the covariate shift and\\nconcept shift vary over time. To tackle this problem, we\\npropose a novel framework LSSAE to model the dynamics\\nof distribution shift (i.e., covariate shift and concept shift).\\nWe also provide theoretical analysis, which shows that our\\nproposed method is equivalent to maximizing the ELBO\\nbased on the non-stationary environment setting, and justi-\\nﬁes the rationality of our proposed method for the problem\\nof evolving domain generalization. Experimental results\\non both toy data and real-world datasets across multiple\\ndomains further indicate the signiﬁcance of our proposed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='of evolving domain generalization. Experimental results\\non both toy data and real-world datasets across multiple\\ndomains further indicate the signiﬁcance of our proposed\\nmethod based on this setting.\\nAcknowledgements\\nThis work was supported in part by CityU New Research\\nInitiatives/Infrastructure Support from Central (APRC\\n9610528), CityU Applied Research Grant (ARG 9667244)\\nand Hong Kong Innovation and Technology Commission\\n(InnoHK Project CIMDA). Besides, this work was also sup-\\nported in part by the National Natural Science Foundation of\\nChina under 62022002, in part by the Hong Kong Research\\nGrants Council General Research Fund (GRF) under Grant\\n11203220.\\nReferences\\nAlbuquerque, I., Monteiro, J., Darvishi, M., Falk, T. H.,\\nand Mitliagkas, I. Generalizing to unseen domains via\\ndistribution matching. arXiv preprint arXiv:1911.00804,\\n2021.\\nBalaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg:\\nTowards domain generalization using meta-regularization.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='distribution matching. arXiv preprint arXiv:1911.00804,\\n2021.\\nBalaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg:\\nTowards domain generalization using meta-regularization.\\nAdvances in Neural Information Processing Systems, 31:\\n998–1008, 2018.\\nBlanchard, G., Lee, G., and Scott, C. Generalizing from sev-\\neral related classiﬁcation tasks to a new unlabeled sample.\\nAdvances in Neural Information Processing Systems, 24:\\n2178–2186, 2011.\\nBlanchard, G., Deshmukh, A. A., Dogan, ¨U., Lee, G., and\\nScott, C. Domain generalization by marginal transfer\\nlearning. J. Mach. Learn. Res., 22:2–1, 2021.\\nChen, H.-Y . and Chao, W.-L. Gradual domain adaptation\\nwithout indexed intermediate domains. In Thirty-Fifth\\nConference on Neural Information Processing Systems,\\n2021.\\nDau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu,\\nY ., Gharghabi, S., Ratanamahatana, C. A., and Keogh,\\nE. The ucr time series archive. IEEE/CAA Journal of\\nAutomatica Sinica, 6(6):1293–1305, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Y ., Gharghabi, S., Ratanamahatana, C. A., and Keogh,\\nE. The ucr time series archive. IEEE/CAA Journal of\\nAutomatica Sinica, 6(6):1293–1305, 2019.\\nDou, Q., Coelho de Castro, D., Kamnitsas, K., and Glocker,\\nB. Domain generalization via model-agnostic learning\\nof semantic features. Advances in Neural Information\\nProcessing Systems, 32:6450–6461, 2019.\\nFederici, M., Tomioka, R., and Forr ´e, P. An information-\\ntheoretic approach to distribution shifts. In Thirty-Fifth\\nConference on Neural Information Processing Systems,\\n2021.\\nGhifary, M., Kleijn, W. B., Zhang, M., and Balduzzi, D. Do-\\nmain generalization for object recognition with multi-task\\nautoencoders. In Proceedings of the IEEE International\\nConference on Computer Vision, pp. 2551–2559, 2015.\\nGinosar, S., Rakelly, K., Sachs, S., Yin, B., and Efros, A. A.\\nA century of portraits: A visual historical record of amer-\\nican high school yearbooks. In 2015 IEEE International\\nConference on Computer Vision Workshop (ICCVW), pp.\\n652–658, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='A century of portraits: A visual historical record of amer-\\nican high school yearbooks. In 2015 IEEE International\\nConference on Computer Vision Workshop (ICCVW), pp.\\n652–658, 2015.\\nGong, R., Li, W., Chen, Y ., and Gool, L. V . Dlow: Domain\\nﬂow for adaptation and generalization. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pp. 2477–2486, 2019.\\nGulrajani, I. and Lopez-Paz, D. In search of lost domain\\ngeneralization. In International Conference on Learning\\nRepresentations, 2021.\\nHan, J., Min, M. R., Han, L., Li, L. E., and Zhang, X.\\nDisentangled recurrent wasserstein autoencoder. In Inter-\\nnational Conference on Learning Representations, 2021.\\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\\nNeural Computation, 9(8):1735–1780, 1997.\\nHoffman, J., Darrell, T., and Saenko, K. Continuous man-\\nifold based adaptation for evolving visual domains. In\\nProceedings of the IEEE Conference on Computer Vision'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Hoffman, J., Darrell, T., and Saenko, K. Continuous man-\\nifold based adaptation for evolving visual domains. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 867–874, 2014.\\nHuang, Z., Wang, H., Xing, E. P., and Huang, D. Self-\\nchallenging improves cross-domain generalization. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nComputer Vision–ECCV 2020: 16th European Confer-\\nence, Glasgow, UK, August 23–28, 2020, Proceedings,\\nPart II 16, pp. 124–140, 2020.\\nIlse, M., Tomczak, J. M., Louizos, C., and Welling, M. Diva:\\nDomain invariant variational autoencoders. In Medical\\nImaging with Deep Learning, pp. 322–348. PMLR, 2020.\\nJang, E., Gu, S., and Poole, B. Categorical reparameteriza-\\ntion with gumbel-softmax. International Conference on\\nLearning Representations, 2017.\\nKingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization. In International Conference on Learning\\nRepresentations, 2015.\\nKingma, D. P. and Welling, M. Auto-encoding variational\\nbayes. International Conference on Learning Represen-\\ntations, 2014.\\nKumar, A., Ma, T., and Liang, P. Understanding self-training\\nfor gradual domain adaptation. In International Confer-\\nence on Machine Learning, pp. 5468–5479. PMLR, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='tations, 2014.\\nKumar, A., Ma, T., and Liang, P. Understanding self-training\\nfor gradual domain adaptation. In International Confer-\\nence on Machine Learning, pp. 5468–5479. PMLR, 2020.\\nLao, Q., Jiang, X., Havaei, M., and Bengio, Y . Continu-\\nous domain adaptation with variational domain-agnostic\\nfeature replay. arXiv preprint arXiv:2003.04382, 2020.\\nLi, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Learn-\\ning to generalize: Meta-learning for domain generaliza-\\ntion. In Thirty-Second AAAI Conference on Artiﬁcial\\nIntelligence, 2018a.\\nLi, H., Pan, S. J., Wang, S., and Kot, A. C. Domain general-\\nization with adversarial feature learning. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp. 5400–5409, 2018b.\\nLi, W., Xu, Z., Xu, D., Dai, D., and Van Gool, L. Domain\\ngeneralization and adaptation using low rank exemplar\\nsvms. IEEE Transactions on Pattern Analysis and Ma-\\nchine Intelligence, 40(5):1114–1127, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='generalization and adaptation using low rank exemplar\\nsvms. IEEE Transactions on Pattern Analysis and Ma-\\nchine Intelligence, 40(5):1114–1127, 2017.\\nLiu, H., Long, M., Wang, J., and Wang, Y . Learning to adapt\\nto evolving domains. In Advances in Neural Information\\nProcessing Systems, volume 33, pp. 22338–22348, 2020.\\nMaddison, C. J., Mnih, A., and Teh, Y . W. The concrete\\ndistribution: A continuous relaxation of discrete random\\nvariables. International Conference on Learning Repre-\\nsentations, 2017.\\nMancini, M., Bulo, S., Caputo, B., and Ricci, E. Adagraph:\\nUnifying predictive and continuous domain adaptation\\nthrough graphs. In 2019 IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pp. 6561–\\n6570, jun 2019.\\nMuandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain\\ngeneralization via invariant feature representation. In\\nInternational Conference on Machine Learning, pp. 10–\\n18, 2013.\\nNguyen, A. T., Tran, T., Gal, Y ., and Baydin, A. G. Do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='generalization via invariant feature representation. In\\nInternational Conference on Machine Learning, pp. 10–\\n18, 2013.\\nNguyen, A. T., Tran, T., Gal, Y ., and Baydin, A. G. Do-\\nmain invariant representation learning with domain den-\\nsity transformations. arXiv preprint arXiv:2102.05082,\\n2021.\\nParascandolo, G., Neitz, A., ORVIETO, A., Gresele, L., and\\nSch¨olkopf, B. Learning explanations that are hard to vary.\\nIn International Conference on Learning Representations,\\n2021.\\nPark, S. W., Shu, D. W., and Kwon, J. Generative adversarial\\nnetworks for markovian temporal dynamics: Stochastic\\ncontinuous data generation. In Proceedings of the 38th\\nInternational Conference on Machine Learning, volume\\n139, pp. 8413–8421, 18–24 Jul 2021.\\nPesaranghader, A. and Viktor, H. L. Fast hoeffding\\ndrift detection method for evolving data streams. In\\nECML/PKDD, 2016.\\nRosenfeld, E., Ravikumar, P. K., and Risteski, A. The\\nrisks of invariant risk minimization. In International'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='drift detection method for evolving data streams. In\\nECML/PKDD, 2016.\\nRosenfeld, E., Ravikumar, P. K., and Risteski, A. The\\nrisks of invariant risk minimization. In International\\nConference on Learning Representations, 2021.\\nShankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S.,\\nJyothi, P., and Sarawagi, S. Generalizing across domains\\nvia cross-gradient training. In International Conference\\non Learning Representations, 2018.\\nShi, Y ., Seely, J., Torr, P. H., Siddharth, N., Hannun, A.,\\nUsunier, N., and Synnaeve, G. Gradient matching for\\ndomain generalization. arXiv preprint arXiv:2104.09937,\\n2021.\\nSugiyama, M., Yamada, M., and du Plessis, M. C. Learning\\nunder nonstationarity: covariate shift and class-balance\\nchange. Wiley Interdisciplinary Reviews: Computational\\nStatistics, 5(6):465–477, 2013.\\nSun, B. and Saenko, K. Deep coral: Correlation alignment\\nfor deep domain adaptation. In Hua, G. and J ´egou, H.\\n(eds.), Computer Vision – ECCV 2016 Workshops , pp.\\n443–450, Cham, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Sun, B. and Saenko, K. Deep coral: Correlation alignment\\nfor deep domain adaptation. In Hua, G. and J ´egou, H.\\n(eds.), Computer Vision – ECCV 2016 Workshops , pp.\\n443–450, Cham, 2016.\\nTahmasbi, A., Jothimurugesan, E., Tirthapura, S., and Gib-\\nbons, P. B. Driftsurf: Stable-state/reactive-state learning\\nunder concept drift. In International Conference on Ma-\\nchine Learning, pp. 10054–10064. PMLR, 2021.\\nTorralba, A. and Efros, A. A. Unbiased look at dataset bias.\\nIn CVPR 2011, pp. 1521–1528. IEEE, 2011.\\nVapnik, V . Statistical learning theory wiley.New York, 1998.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nVergara, A., Vembu, S., Ayhan, T., Ryan, M. A., Homer,\\nM. L., and Huerta, R. Chemical gas sensor drift compen-\\nsation using classiﬁer ensembles. Sensors and Actuators\\nB: Chemical, 166:320–329, 2012.\\nV olpi, R., Namkoong, H., Sener, O., Duchi, J. C., Murino,\\nV ., and Savarese, S. Generalizing to unseen domains via\\nadversarial data augmentation. In Advances in Neural\\nInformation Processing Systems, volume 31, 2018.\\nWang, H., He, H., and Katabi, D. Continuously indexed\\ndomain adaptation. In III, H. D. and Singh, A. (eds.),\\nProceedings of the 37th International Conference on Ma-\\nchine Learning, volume 119, pp. 9898–9907, 13–18 Jul\\n2020.\\nWang, Y ., Li, H., Chau, L.-P., and Kot, A. C. Variational dis-\\nentanglement for domain generalization. arXiv preprint\\narXiv:2109.05826, 2021.\\nWulfmeier, M., Bewley, A., and Posner, I. Incremental\\nadversarial domain adaptation for continually changing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='entanglement for domain generalization. arXiv preprint\\narXiv:2109.05826, 2021.\\nWulfmeier, M., Bewley, A., and Posner, I. Incremental\\nadversarial domain adaptation for continually changing\\nenvironments. In 2018 IEEE International Conference on\\nRobotics and Automation (ICRA), pp. 4489–4495. IEEE,\\n2018.\\nYan, S., Song, H., Li, N., Zou, L., and Ren, L. Improve un-\\nsupervised domain adaptation with mixup training. arXiv\\npreprint arXiv:2001.00677, 2020.\\nYingzhen, L. and Mandt, S. Disentangled sequential autoen-\\ncoder. In International Conference on Machine Learning,\\npp. 5670–5679. PMLR, 2018.\\nZhou, K., Yang, Y ., Qiao, Y ., and Xiang, T. Domain general-\\nization with mixstyle. arXiv preprint arXiv:2104.02008,\\n2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nA. Proofs\\nA.1. Derivation of ELBO for Stationary Environments\\nAs we introduce two latent variables to account for the two types of distribution shift, the data generating procedure for one\\ndomain can be expressed as:\\np(x,y,zc,zw,zv) =p(zc)p(zw)p(zv)p(x|zc,zw)p(y|zc,zv)\\n= p(zw)p(zv)\\nN∏\\ni=1\\np(zc\\ni )p(xi|zc\\ni ,zw)p(yi|zc\\ni ,zv)\\n(14)\\nWe let (zc,zw) be the latent variables for x, and zv be latent variable for y, thus the distribution of these three latent\\nvariables can be inferred from the observable datapoints as p(zc|x),p(zw|x), p(zv|y), respectively. The joint distribution of\\nlatent variables is:\\np(zc,zw,zv|x,y) =p(zc|x)p(zw|x)p(zv|y) (15)\\nFor our approximating distribution in Eq 15, we can choose the form q(zc,zw,zv|x,y) =q(zc|x)q(zw|x)q(zv|y). Thus,\\nwe can get:\\nDKL(q,p) =Eq log q(zc,zw,zv|x,y)\\np(zc,zw,zv|x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv)p(x,y)\\n= Eq log q(zc,zw,zv|x,y)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='we can get:\\nDKL(q,p) =Eq log q(zc,zw,zv|x,y)\\np(zc,zw,zv|x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv)p(x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv) + Eq log p(x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv) + logp(x,y)\\n(16)\\nWe can get:\\nlog p(x,y) =DKL(q,p) +Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y) (17)\\nAs DKL(q,p) ≥0, the variational lower bound for log p(x,y) is:\\nL= Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y) . (18)\\nThis formulation can be reorganized as:\\nL= Eq log p(x,y|zc,zw,zv)p(zc)p(zw)p(zv)\\nq(zc|x)q(zw|x)q(zv|y)\\n= Eq log p(x,y|zc,zw,zv) +Eq log p(zc)\\nq(zc|x) + Eq log p(zw)\\nq(zw|x) + Eq log p(zv)\\nq(zv|y)\\n= Eq log p(x,y|zc,zw,zv) −DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw)) −DKL(q(zv|y),p(zv)).\\n(19)\\nAs p(x,y|zc,zw,zv) =p(x|zc,zw)p(y|zc,zv), the above formulation can be rewrote as:\\nL≥ Eq log p(x|zc,zw)p(y|zc,zv) −DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw)) −DKL(q(zv|y),p(zv)). (20)\\nA.2. Derivation of ELBO for Non-stationary Environments'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='L≥ Eq log p(x|zc,zw)p(y|zc,zv) −DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw)) −DKL(q(zv|y),p(zv)). (20)\\nA.2. Derivation of ELBO for Non-stationary Environments\\nWe assume the prior distribution of latent variableszw and zv satisfy Markov property. This means each of them relies on\\nthe value of their previous states:\\np(zw) =p(zw\\nt |zw\\n<t),p(zv) =p(zv\\nt |zv\\n<t), (21)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nThe joint distribution of data and latent variables is:\\np(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\n=\\nT∏\\nt=1\\np(xt,yt|zc,zw\\nt ,zv\\nt )p(zc)p(zw\\nt |zw\\n<t)p(zv\\nt |zv\\n<t)\\n=\\nT∏\\nt=1\\np(zw\\nt |zw\\n<t)p(zv\\nt |zv\\n<t)\\nNt∏\\ni=1\\np(zc\\ni )p(xi|zc\\ni ,zw\\nt )p(yi|zc\\ni ,zv\\nt )\\n(22)\\nwhere p(zw\\n1 ) =p(zw\\n1 |zw\\n0 ) and p(zv\\n1) =p(zv\\n1|zv\\n0).\\nFor a non-stationary environment, we assume that (zc,zw\\nt ) are the latent variables for xt and zv\\nt is the latent variable for yt\\nat t-th time stamp. Thus the distribution of the latent variables for t-th time stamp can be express as p(zc|xt), p(zw\\nt |xt) and\\np(zv\\nt |yt), respectively. We employq(zc|xt), q(zw\\nt |zw\\n<t,xt) and q(zv\\nt |zv\\n<t,yt) to approximate the prior distributions here.\\nSimilar to Eq 18, we can draw the variational lower bound for log p(x1:T ,y1:T ) as:\\nL= Eq log p(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\nq(zc,zw\\n1:T ,zv\\n1:T |x1:T ,y1:T ) . (23)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Similar to Eq 18, we can draw the variational lower bound for log p(x1:T ,y1:T ) as:\\nL= Eq log p(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\nq(zc,zw\\n1:T ,zv\\n1:T |x1:T ,y1:T ) . (23)\\nWhen incorporating with Eq 22, this can be reorganized as:\\nL= Eq log\\n∏T\\nt=1 p(xt, yt|zc, zw\\nt , zv\\nt )p(zc)p(zw\\nt |zw\\n<t)p(zv\\nt |zv\\n<t)∏T\\nt=1 q(zc|xt)q(zw\\nt |zw\\n<t, xt)q(zv\\nt |zv\\n<t, yt)\\n= Eq\\n[\\nlog\\n∏T\\nt=1 p(zw\\nt |zw\\n<t)∏T\\nt=1 q(zw\\nt |zw\\n<t, xt)\\n+ log\\n∏T\\nt=1 p(zv\\nt |zv\\n<t)∏T\\nt=1 q(zv\\nt |zv\\n<t, yt)\\n+ log\\n∏T\\nt=1 p(zc)∏T\\nt=1 q(zc|xt)\\n+ log\\nT∏\\nt=1\\np(xt|zc\\nt, zw\\nt )p(yt|zc\\nt, zv\\nt )\\n]\\n= Eq\\n[\\n−\\nT∑\\nt=1\\nlog q(zw\\nt |zw\\n<t, xt)\\np(zw\\nt |zw\\n<t) −\\nT∑\\nt=1\\nlog q(zv\\nt |zv\\n<t, yt)\\np(zv\\nt |zv\\n<t) −\\nT∑\\nt=1\\nlog q(zc\\nt|xt)\\np(zc\\nt)\\n+\\nT∑\\nt=1\\nlog p(xt|zc, zw\\nt )p(yt|zc, zv\\nt )\\n]\\n(24)\\nBy Jensen’s inequality, the above formulation can be rewrote as:\\nL≥ Eq\\n[ T∑\\nt=1\\nlog p(xt|zc,zw\\nt )p(yt|zc,zv\\nt ) −DKL(q(zc|xt),p(zc)) −DKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t))\\n−DKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t))\\n] (25)\\nThis complements the proof.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='L≥ Eq\\n[ T∑\\nt=1\\nlog p(xt|zc,zw\\nt )p(yt|zc,zv\\nt ) −DKL(q(zc|xt),p(zc)) −DKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t))\\n−DKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t))\\n] (25)\\nThis complements the proof.\\nB. Probabilistic Generative Model of LSSAE\\nIn Fig. 6, we present the complete probabilistic generative graph of our LSSAE. The left part is the DAG of LSSAE presented\\nin Fig. 1(d) where we did not include zc in this ﬁgure as the main focus of Fig. 1 is the dynamic factors (i.e., W and V) for\\npresenting our main idea of evolving dynamics at a high level instead of DG. In the right part, we illustrate the relationship\\nbetween our DAG and probabilistic generative model (with zc). Here, the dependence of Xt on Yt (the dotted blue line) is\\nsubstituted with zc (the solid red line) which mainly captures the static category information in data sample space.\\nC. Additional Details on the Experimental Setup\\nC.1. Datasets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='substituted with zc (the solid red line) which mainly captures the static category information in data sample space.\\nC. Additional Details on the Experimental Setup\\nC.1. Datasets\\nOur experiments are conducted on 2 synthetic and 4 real-world datasets presented in Table 4. More detailed description are\\ngiven below.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nVt-1 Vt ...\\n(d)\\nXt-1 Yt-1 \\nWt-1 \\nX t Yt\\nWt ......\\n... zv\\nt-1 zv\\nt ...\\nProbabilisitic Generative Model of (d)\\nXt-1 Yt-1 \\nzw\\nt-1 \\nX t Yt\\nzw\\nt ......\\n...\\nzc zc\\nFigure 6.The probabilistic generative graph of LSSAE with zc presented. Here, the dependence of Xt on Yt in Fig. 1(d) is substituted\\nwith zc which mainly captures the static category information in data sample space.\\nTable 4.Brief description of employed benchmarks in this work.\\nDataset Type Number Source Domains Intermediate Domains Target Domains Total Domains\\nCircle/-C Digital 3,000 15 5 10 30\\nSine/-C Digital 2,280 12 4 8 24\\nRMNIST Image 70,000 10 3 6 19\\nPortraits Image 37,921 19 5 10 34\\nCalTran Image 5,450 19 5 10 34\\nPowerSupply Digital 29,928 15 5 10 30\\n• Circle (Pesaranghader & Viktor, 2016): Each data in this dataset owns two attributes (x,y),x,y ∈[0,1]. The label is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='CalTran Image 5,450 19 5 10 34\\nPowerSupply Digital 29,928 15 5 10 30\\n• Circle (Pesaranghader & Viktor, 2016): Each data in this dataset owns two attributes (x,y),x,y ∈[0,1]. The label is\\nassigned using a circle curve as the decision boundary following (x−x0)2 + (y−y0)2 ≤r2, where (x0,y0) are the\\nlocation of the center and ris the radius of this circle. To generate Circle-C, we inject gradual shift via modifying the\\nvalue of x0 continuously throughout the domains.\\n• Sine (Pesaranghader & Viktor, 2016): Each data in this dataset owns two attributes(x,y),x,y ∈[0,1]. The label is\\nassigned using a sine curve as the decision boundary following y≤sin(x). To simulate abrupt concept drift for Sine-C,\\nlabels are reversed (i.e., from 0 to 1 or from 1 to 0) from the 6-th domain to the last one.\\n• RMNIST (Ghifary et al., 2015): This dataset is composed of MNIST digits of various rotations. We generate 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='• RMNIST (Ghifary et al., 2015): This dataset is composed of MNIST digits of various rotations. We generate 19\\ndomains via applying the rotations with degree of R= {0◦,15◦,30◦,..., 180◦}on each domain. Note that each image\\nis seen at exactly one angle, so the training procedure cannot track a single image across different angles.\\n• Portraits (Ginosar et al., 2015): This is a real-world dataset of photos collected in American high school seniors. The\\nportraits are taken over 108 years (1905-2013) across 26 states. The goal is to classify the gender for each photo. We\\nsplit the dataset into 34 domains by a ﬁxed internal along time.\\n• CalTran (Hoffman et al., 2014): This dataset contains real-world images captured by a ﬁxed trafﬁc camera deployed in\\nan intersection over time. Frames were updated at 3 minute intervals each with a resolution 320 ×320. We divide\\nit into 34 domains by time. This is a scene classiﬁcation task to determine whether one or more cars are present in,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='it into 34 domains by time. This is a scene classiﬁcation task to determine whether one or more cars are present in,\\nor approaching the intersection. The challenge mainly raise from the continually evolving domain shift as changes\\ninclude time, illumination, weather, etc.\\n• PowerSupply (Dau et al., 2019): This dataset is comprised of records of hourly power supply collected by an Italy\\nelectricity company. We form 30 domains according to days. Each data is assigned by a binary class label which\\nrepresents which time of day the current power supply belongs to (i.e., am or pm). The concept shift may results from\\nthe change in season, weather, price or the differences between working days and weekend.\\nC.2. Model Architecture & Hyperparameters\\nNeural network architectures used for each dataset:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 5.Model architectures for different datasets.\\nDataset Encoder Decoder\\nCircle/-C\\nNon-linear Encoder Non-linear Decoder Sine/-C\\nPowerSupply\\nRMNIST MNIST ConvNet MNIST ConvTranNet\\nPortraits ResNet-18 ConvTranNetCalTran\\nNeural network architecture for digital experiments (Circle/-C, Sine/-C, PowerSupply):\\nTable 6.Implementation of Non-linear Encoder.\\n# Layer\\n1 Linear(in= d, output=512)\\n2 ReLU\\n3 Linear(in=512, output=512)\\n4 ReLU\\n5 Linear(in=512, output=512)\\n6 ReLU\\n7 Linear(in=512, output=512)\\nTable 7.Implementation of Non-linear Decoder.\\n# Layer\\n1 Linear(in= d, output=16)\\n2 BatchNorm\\n3 LeakyReLU(0.2)\\n4 Linear(in=16, output=64)\\n5 BatchNorm\\n6 LeakyReLU(0.2)\\n7 Linear(in=64, output=128)\\n8. BatchNorm\\n9 LeakyReLU(0.2)\\n10 Linear(in=128, output= d)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nNeural network architecture for RMNIST experiments:\\nTable 8.Implementation of MNIST ConvNet.\\n# Layer\\n1 Conv2D(in= d, output=64)\\n2 ReLU\\n3 GroupNorm(groupds=8)\\n4 Conv2D(in=64, output=128, stride=2)\\n5 ReLU\\n6 GroupNorm(groupds=8)\\n7 Conv2D(in=128, output=128)\\n8 ReLU\\n9 GroupNorm(groupds=8)\\n10 Conv2D(in=128, output=128)\\n11 ReLU\\n12 GroupNorm(groupds=8)\\n13 Global average-pooling\\nTable 9.Implementation of MNIST ConvTranNet.\\n# Layer\\n1 Linear(in= d, output=1024)\\n2 BatchNorm\\n3 ReLU\\n4 Upsample(8)\\n5 ConvTransposed2D(in=64, output=128, kernel=5)\\n6 BatchNorm\\n7 ReLU\\n8 Upsample(24)\\n9 ConvTransposed2D(in=128, output=256, kernel=5)\\n10 BatchNorm\\n11 ReLU\\n12 Conv2D(in=256, output=1, kernel=1)\\n13 Sigmoid'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nNeural network architecture for Portraits and CalTran experiments:\\nTable 10.Implementation of ConvTranNet.\\n# Layer\\n1 Linear(in= d, output=1024)\\n2 BatchNorm\\n3 ReLU\\n4 Upsample(16)\\n5 ConvTransposed2D(in=64, output=128, kernel=5)\\n6 BatchNorm\\n7 ReLU\\n8 Upsample(40)\\n9 ConvTransposed2D(in=128, output=256, kernel=5)\\n10 BatchNorm\\n11 ReLU\\n12 Upsample(80)\\n13 ConvTransposed2D(in=256, output=3, kernel=5)\\n14 BatchNorm\\n15 ReLU\\n16 Sigmoid\\nThe model architecture of the encoder is ResNet-18, we replace the ﬁnal softmax layer of the ofﬁcial version following Gul-\\nrajani & Lopez-Paz (2021). Besides, a dropout layer before the ﬁnal linear layer is inserted. This network is initialized by\\nrandom rather than loading the pretrained parameters on ImageNet.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nWe list the values of hyperparameters for different datasets below. All models are optimized by Adam (Kingma & Ba, 2015).\\nIn our experiments, we found that keeping the balance of the three KL divergence terms for zc, zw and zv via adjusting the\\nvalue of λ1, λ2 and λ3 is beneﬁcial for the ﬁnal results.\\nTable 11.Hyperparametes and their default values.\\nDataset Parameters Value\\nCircle\\nlearning rate for Ec,D,C 5e-5\\nlearning rate for Ew,Fw,Ev,Fv 5e-6\\nbatch size 24\\nλ1,λ2,λ3 1.0, 1.0, 1.0\\nα 0.05\\nCircle-C\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 1.0, 2.0, 1.0\\nα 0.05\\nSine\\nlearning rate for Ec,D,C 5e-5\\nlearning rate for Ew,Fw,Ev,Fv 5e-6\\nbatch size 24\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nSine-C\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nRMNIST\\nlearning rate for Ec,D,C 1e-3\\nlearning rate for Ew,Fw,Ev,Fv 1e-4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='α 0.05\\nSine-C\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nRMNIST\\nlearning rate for Ec,D,C 1e-3\\nlearning rate for Ew,Fw,Ev,Fv 1e-4\\nbatch size 48\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nPortraits\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 0.5, 1.0, 1.0\\nα 0.05\\nCalTran\\nlearning rate for Ec,D,C 5e-5\\nlearning rate for Ew,Fw,Ev,Fv 5e-6\\nbatch size 24\\nλ1,λ2,λ3 1.0, 1.0, 1.0\\nα 0.1\\nPowerSupply\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 48\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nD. Additional Experimental Results\\nIn this section, we provide more experimental results for our proposed evolving domain generalization task. As we can see,\\nin most of the cases, we can achieve the state-of-the-art performance compared with other domain generalization baselines.\\nHowever, in some cases, our proposed method cannot achieve desired results. For example, in Sine-C, ERM and some\\ndomain generalization baselines can achieve a desired classiﬁcation performance especially for domain index 17,18 and 19,\\nwhich we conjecture the reason that their decision boundaries overﬁt to the abrupt concept shift, which further leads to their\\npoor performance on the following domains after domain 19. However, our proposed method aims to learn the evolving\\npattern starting from t= 0, which may not be optimal to suit to this abrupt concept shift case. One possible future direction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='pattern starting from t= 0, which may not be optimal to suit to this abrupt concept shift case. One possible future direction\\nis only to learn the evolving pattern in a certain time duration (i.e., time duration in [t−T0,t], where tis the current time\\nstamp). Nevertheless, our proposed method shows its effectiveness in dynamic modeling for evolving domain generalization\\ntask. We will leave the discussion how to ﬁnd a suitable time duration (i.e., a suitable T0) in our future work.\\nTable 12.Circle. We show the results on each target domain by domain index.\\nAlgorithm 21 22 23 24 25 26 27 28 29 30 Avg\\nERM 53.9 ±3.5 55.8 ±4.8 53.9 ±5.2 44.7 ±6.3 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 49.9\\nMixup 48.6 ±3.8 51.7 ±4.0 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 48.4\\nMMD 50.0 ±3.9 53.6 ±4.4 55.0 ±4.3 51.9 ±6.0 60.8 ±3.9 49.7 ±7.2 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 50.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='MMD 50.0 ±3.9 53.6 ±4.4 55.0 ±4.3 51.9 ±6.0 60.8 ±3.9 49.7 ±7.2 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 50.7\\nMLDG 57.8 ±3.6 57.7 ±5.0 55.3 ±4.9 46.4 ±6.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 50.8\\nIRM 57.8 ±3.9 59.4 ±5.4 56.9 ±4.9 48.1 ±7.4 57.5 ±4.3 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 51.3\\nRSC 45.3 ±3.6 51.4 ±3.8 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 48.0\\nMTL 61.4 ±2.2 57.2 ±6.4 53.3 ±5.1 48.3 ±6.2 56.9 ±4.8 49.2 ±3.7 43.3 ±5.0 45.8 ±2.8 54.2 ±5.7 42.2 ±4.9 51.2\\nFish 51.7 ±3.7 53.1 ±3.7 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 48.8\\nCORAL 65.3 ±3.2 63.9 ±4.4 60.0 ±4.8 56.4 ±6.0 60.2 ±4.3 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 53.9\\nAndMask 42.8 ±3.4 50.6 ±4.1 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 49.7 ±5.4 47.9\\nDIV A 81.3 ±3.5 76.3 ±4.2 74.7 ±4.6 56.7 ±5.1 67.0 ±6.1 62.3 ±5.1 62.0 ±5.6 66.3 ±4.1 70.3 ±5.6 62.0 ±4.2 67.9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='DIV A 81.3 ±3.5 76.3 ±4.2 74.7 ±4.6 56.7 ±5.1 67.0 ±6.1 62.3 ±5.1 62.0 ±5.6 66.3 ±4.1 70.3 ±5.6 62.0 ±4.2 67.9\\nLSSAE (Ours) 95.8 ±1.9 95.6 ±2.1 93.5 ±2.9 96.3 ±1.8 83.8 ±5.2 74.3 ±3.6 51.9 ±5.6 52.3 ±8.1 46.5 ±9.2 48.4 ±5.3 73.8\\nTable 13.Sine. We show the results on each target domain denoted by domain index.\\nAlgorithm 17 18 19 20 21 22 23 24 Avg\\nERM 71.4 ±6.1 91.0 ±1.5 81.6 ±2.4 53.4 ±2.9 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 63.0\\nMixup 63.1 ±5.9 93.5 ±1.7 80.6 ±3.8 52.8 ±2.9 60.3 ±7.2 54.2 ±2.7 49.5 ±4.4 49.3 ±8.0 62.9\\nMMD 57.0 ±4.2 57.1 ±4.1 47.6 ±5.4 50.0 ±1.8 55.1 ±6.7 54.4 ±4.7 49.5 ±4.8 51.7 ±5.0 55.8\\nMLDG 69.2 ±4.2 67.7 ±4.1 52.1 ±5.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 63.2\\nIRM 66.9 ±6.2 81.1 ±3.2 88.5 ±3.0 56.6 ±6.0 57.2 ±5.8 53.7 ±5.1 49.5 ±2.2 51.7 ±5.4 63.2\\nRSC 61.3 ±6.6 83.5 ±1.9 84.5 ±2.6 52.8 ±2.8 55.1 ±6.7 54.4 ±4.7 49.5 ±4.8 51.7 ±5.0 61.5\\nMTL 70.6 ±6.6 91.6 ±1.2 79.9 ±3.4 51.0 ±4.7 60.3 ±7.6 53.6 ±5.2 49.5 ±5.3 46.9 ±5.9 62.9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='RSC 61.3 ±6.6 83.5 ±1.9 84.5 ±2.6 52.8 ±2.8 55.1 ±6.7 54.4 ±4.7 49.5 ±4.8 51.7 ±5.0 61.5\\nMTL 70.6 ±6.6 91.6 ±1.2 79.9 ±3.4 51.0 ±4.7 60.3 ±7.6 53.6 ±5.2 49.5 ±5.3 46.9 ±5.9 62.9\\nFish 66.1 ±6.9 82.0 ±2.7 87.5 ±2.4 55.2 ±3.0 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 62.3\\nCORAL 60.0 ±5.3 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 49.7 ±6.2 48.6 ±4.6 46.3 ±5.0 51.7 ±5.0 51.6\\nAndMask 44.2 ±5.1 42.9 ±4.2 54.2 ±7.0 71.9 ±1.9 86.4 ±3.2 90.4 ±2.9 88.1 ±3.4 76.4 ±3.7 69.3\\nDIV A 79.0 ±6.6 60.8 ±1.9 47.6 ±2.6 50.0 ±2.8 55.1 ±6.7 51.9 ±4.7 38.6 ±4.8 40.4 ±5.0 52.9\\nLSSAE (Ours) 93.0 ±1.7 86.9 ±0.7 69.2 ±1.5 63.8 ±3.8 68.8 ±2.5 76.8 ±4.8 63.9 ±1.3 49.0 ±3.1 71.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 14.RMNIST. We show the results on each target domain denoted by rotation angle.\\nAlgorithm 130◦ 140◦ 150◦ 160◦ 170◦ 180◦ Avg\\nERM 56.8 ±0.9 44.2 ±0.8 37.8 ±0.6 38.3 ±0.8 40.9 ±0.8 43.6 ±0.8 43.6\\nMixup 61.3 ±0.7 47.4 ±0.8 39.1 ±0.7 38.3 ±0.7 40.5 ±0.8 42.8 ±0.9 44.9\\nMMD 59.2 ±0.9 46.0 ±0.8 39.0 ±0.7 39.3 ±0.8 41.6 ±0.7 43.7 ±0.8 44.8\\nMLDG 57.4 ±0.7 44.5 ±0.9 37.5 ±0.8 37.5 ±0.8 39.9 ±0.8 42.0 ±0.9 43.1\\nIRM 47.7 ±0.9 38.5 ±0.7 34.1 ±0.7 35.7 ±0.8 37.8 ±0.8 40.3 ±0.8 39.0\\nRSC 54.1 ±0.9 41.9 ±0.8 35.8 ±0.7 37.0 ±0.8 39.8 ±0.8 41.6 ±0.8 41.7\\nMTL 54.8 ±0.9 43.1 ±0.8 36.4 ±0.8 36.1 ±0.8 39.1 ±0.9 40.9 ±0.8 41.7\\nFish 60.8 ±0.8 47.8 ±0.8 39.2 ±0.8 37.6 ±0.7 39.0 ±0.8 40.7 ±0.7 44.2\\nCORAL 58.8 ±0.9 46.2 ±0.8 38.9 ±0.7 38.5 ±0.8 41.3 ±0.8 43.5 ±0.8 44.5\\nAndMask 53.5 ±0.9 42.9 ±0.8 37.8 ±0.7 38.6 ±0.8 40.8 ±0.8 43.2 ±0.8 42.8\\nDIV A 58.3 ±0.8 45.0 ±0.8 37.6 ±0.8 36.9 ±0.7 38.1 ±0.8 40.1 ±0.8 42.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='AndMask 53.5 ±0.9 42.9 ±0.8 37.8 ±0.7 38.6 ±0.8 40.8 ±0.8 43.2 ±0.8 42.8\\nDIV A 58.3 ±0.8 45.0 ±0.8 37.6 ±0.8 36.9 ±0.7 38.1 ±0.8 40.1 ±0.8 42.7\\nLSSAE (Ours) 64.1 ±0.8 51.6 ±0.8 43.4 ±0.8 38.6 ±0.7 40.3 ±0.8 40.4 ±0.8 46.4\\nTable 15.Portraits. We show the results on each target domain denoted by domain index.\\nAlgorithm 25 26 27 28 29 30 31 32 33 34 Avg\\nERM 75.5 ±0.9 83.8 ±0.9 88.5 ±0.8 93.3 ±0.7 93.4 ±0.6 92.1 ±0.7 90.6 ±0.8 84.3 ±0.9 88.5 ±0.9 87.9 ±1.4 87.8\\nMixup 75.5 ±0.9 83.8 ±0.9 88.5 ±0.8 93.3 ±0.7 93.4 ±0.6 92.1 ±0.7 90.6 ±0.8 84.3 ±0.9 88.5 ±0.9 87.9 ±1.4 87.8\\nMMD 74.0 ±1.0 83.8 ±0.8 87.2 ±0.8 93.0 ±0.7 93.0 ±0.6 91.9 ±0.7 90.9 ±0.7 84.7 ±1.4 88.3 ±0.9 85.8 ±1.8 87.3\\nMLDG 76.4 ±0.8 85.5 ±0.9 90.1 ±0.7 94.3 ±0.6 93.5 ±0.6 92.0 ±0.7 90.8 ±0.8 85.6 ±1.1 89.3 ±0.8 87.6 ±1.6 88.5\\nIRM 74.2 ±0.9 83.5 ±0.9 88.5 ±0.8 91.0 ±0.8 90.4 ±0.7 87.3 ±0.8 87.0 ±0.9 80.4 ±1.5 86.7 ±0.9 85.1 ±1.8 85.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='IRM 74.2 ±0.9 83.5 ±0.9 88.5 ±0.8 91.0 ±0.8 90.4 ±0.7 87.3 ±0.8 87.0 ±0.9 80.4 ±1.5 86.7 ±0.9 85.1 ±1.8 85.4\\nRSC 75.2 ±0.9 84.7 ±0.8 87.9 ±0.7 93.3 ±0.7 92.5 ±0.7 91.0 ±0.7 90.0 ±0.7 84.6 ±1.2 88.2 ±0.8 85.8 ±1.9 87.3\\nMTL 78.2 ±0.9 86.5 ±0.8 90.9 ±0.8 94.2 ±0.7 93.8 ±0.6 92.0 ±0.7 91.2 ±0.7 86.0 ±1.2 89.3 ±0.8 87.4 ±1.4 89.0\\nFish 78.6 ±0.9 86.9 ±0.8 89.5 ±0.8 93.5 ±0.7 93.3 ±0.6 92.1 ±0.6 91.1 ±0.7 86.2 ±1.3 88.7 ±0.9 87.7 ±1.6 88.8\\nCORAL 74.6 ±0.9 84.6 ±0.8 87.9 ±0.8 93.3 ±0.6 92.7 ±0.7 91.5 ±0.7 90.7 ±0.7 84.6 ±1.5 88.1 ±0.9 85.9 ±1.9 87.4\\nAndMask 62.0 ±1.1 70.8 ±1.1 67.0 ±1.2 70.2 ±1.1 75.2 ±1.1 74.1 ±1.0 72.7 ±1.1 64.7 ±1.6 77.3 ±1.1 74.9 ±2.1 70.9\\nDIV A 76.2 ±1.0 86.6 ±0.8 88.8 ±0.8 93.5 ±0.7 93.1 ±0.6 91.6 ±0.6 91.1 ±0.7 84.7 ±1.3 89.1 ±0.8 87.0 ±1.5 88.2\\nLSSAE (Ours) 77.7 ±0.9 87.1 ±0.8 90.8 ±0.7 94.3 ±0.6 94.3 ±0.6 92.2 ±0.6 91.2 ±0.7 86.7 ±1.1 89.6 ±0.8 86.9 ±1.4 89.1\\nTable 16.CalTran. We show the results on each target domain denoted by domain index.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Table 16.CalTran. We show the results on each target domain denoted by domain index.\\nAlgorithm 25 26 27 28 29 30 31 32 33 34 Avg\\nERM 29.9 ±3.5 88.4 ±2.1 61.1 ±3.5 56.3 ±3.2 90.0 ±1.6 60.1 ±2.5 55.5 ±3.5 88.8 ±2.4 57.1 ±3.5 50.5 ±5.2 66.3\\nMixup 53.6 ±3.9 89.0 ±2.0 61.8 ±2.4 55.7 ±2.9 88.2 ±2.1 58.6 ±3.0 52.3 ±3.7 88.6 ±2.7 57.1 ±3.0 55.1 ±4.3 66.0\\nMMD 30.2 ±2.1 92.7 ±1.7 56.4 ±3.7 39.1 ±3.2 93.6 ±1.7 52.1 ±3.2 42.8 ±3.0 92.1 ±2.2 42.1 ±3.8 29.4 ±3.8 57.1\\nMLDG 54.8 ±4.1 88.6 ±2.6 62.2 ±3.6 55.1 ±4.1 88.3 ±1.7 60.9 ±4.3 51.7 ±2.6 89.0 ±1.9 56.5 ±3.4 55.3 ±4.8 66.2\\nIRM 46.4 ±3.7 90.8 ±1.7 60.8 ±3.4 52.9 ±3.1 91.8 ±1.7 56.6 ±3.1 52.1 ±2.9 90.9 ±2.6 55.6 ±3.9 43.1 ±5.5 64.1\\nRSC 57.2 ±3.0 88.4 ±2.6 62.6 ±3.0 56.5 ±3.7 88.0 ±2.4 59.4 ±3.0 51.9 ±2.9 90.0 ±2.0 59.4 ±2.9 56.0 ±3.1 67.0\\nMTL 64.2 ±3.0 87.2 ±2.5 64.9 ±3.9 60.0 ±4.8 84.5 ±2.2 60.6 ±3.5 52.6 ±3.7 83.9 ±2.9 58.2 ±4.1 65.7 ±5.6 68.2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='MTL 64.2 ±3.0 87.2 ±2.5 64.9 ±3.9 60.0 ±4.8 84.5 ±2.2 60.6 ±3.5 52.6 ±3.7 83.9 ±2.9 58.2 ±4.1 65.7 ±5.6 68.2\\nFish 61.1 ±3.5 88.2 ±1.5 64.7 ±4.0 57.9 ±3.1 88.3 ±2.2 59.9 ±3.0 57.5 ±2.7 87.4 ±2.8 57.7 ±3.7 63.0 ±6.1 68.6\\nCORAL 50.4 ±3.0 90.8 ±2.0 61.2 ±3.8 55.0 ±2.5 92.0 ±1.7 56.8 ±3.8 52.0 ±3.8 90.9 ±1.6 56.8 ±2.4 50.9 ±5.6 65.7\\nAndMask 30.0 ±2.2 92.7 ±1.7 56.2 ±3.8 39.1 ±3.2 93.6 ±1.7 51.6 ±3.2 42.6 ±2.9 92.1 ±2.2 41.2 ±3.7 29.9 ±3.6 56.9\\nDIV A 60.6 ±2.9 90.1 ±1.7 67.5 ±3.1 58.9 ±3.5 88.4 ±2.8 58.7 ±3.3 53.8 ±3.6 89.8 ±1.7 61.8 ±4.8 62.0 ±3.4 69.2\\nLSSV AE 63.4 ±3.4 92.1 ±2.0 62.6 ±4.7 58.8 ±4.4 92.9 ±1.6 62.0 ±3.9 54.3 ±3.0 92.1 ±2.2 60.5 ±3.8 67.4 ±3.6 70.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 17.Circle-C. We show the results on each target domain denoted by domain index.\\nAlgorithm 21 22 23 24 25 26 27 28 29 30 Avg\\nERM 40.6 ±3.8 43.1 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.9\\nMixup 40.6 ±3.8 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.8\\nMMD 38.6 ±3.7 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.6\\nMLDG 44.4 ±4.4 43.9 ±5.0 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 35.3\\nIRM 63.6 ±4.2 56.7 ±7.0 48.1 ±4.3 37.2 ±4.8 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 39.7\\nRSC 38.6 ±3.7 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.6\\nMTL 41.7 ±5.0 45.6 ±7.1 36.9 ±6.3 36.4 ±6.2 44.7 ±6.0 31.4 ±3.2 27.8 ±4.2 28.3 ±2.8 31.9 ±4.6 26.1 ±4.1 35.1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='MTL 41.7 ±5.0 45.6 ±7.1 36.9 ±6.3 36.4 ±6.2 44.7 ±6.0 31.4 ±3.2 27.8 ±4.2 28.3 ±2.8 31.9 ±4.6 26.1 ±4.1 35.1\\nFish 42.5 ±3.8 43.3 ±4.8 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 35.1\\nCORAL 40.8 ±3.7 43.3 ±4.8 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.9\\nAndMask 53.6 ±4.2 54.7 ±5.4 51.7 ±4.7 33.6 ±5.1 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 35.0 ±6.7 38.9\\nDIV A 40.6 ±3.8 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.8\\nLSSAE (Ours) 74.5 ±1.8 65.5 ±3.9 55.5 ±3.2 36.0 ±3.5 45.0 ±0.7 40.5 ±4.6 35.0 ±1.4 33.0 ±1.4 34.0 ±6.4 29.0 ±2.8 44.8\\nTable 18.Sine-C. We show the results on each target domain denoted by domain index.\\nAlgorithm 17 18 19 20 21 22 23 24 Avg\\nERM 64.2 ±6.8 84.9 ±3.2 83.7 ±3.1 54.9 ±2.1 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.8\\nMixup 60.3 ±7.8 77.4 ±3.3 87.8 ±1.8 57.3 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='ERM 64.2 ±6.8 84.9 ±3.2 83.7 ±3.1 54.9 ±2.1 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.8\\nMixup 60.3 ±7.8 77.4 ±3.3 87.8 ±1.8 57.3 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.2\\nMMD 55.8 ±5.1 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 52.4\\nMLDG 65.6 ±6.4 88.7 ±2.3 84.4 ±2.8 52.4 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 62.2\\nIRM 61.4 ±7.0 82.8 ±3.2 86.5 ±3.0 54.9 ±1.8 55.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.5\\nRSC 65.0 ±6.7 83.5 ±3.1 85.4 ±3.1 53.8 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.8\\nMTL 61.9 ±7.3 82.0 ±2.3 83.7 ±4.2 54.2 ±5.4 60.3 ±7.6 53.6 ±5.2 49.5 ±5.3 46.9 ±5.9 61.5\\nFish 69.4 ±6.2 94.5 ±1.5 79.5 ±3.1 52.4 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 62.8\\nCORAL 70.3 ±5.0 77.2 ±3.6 67.0 ±4.2 52.1 ±2.5 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 59.2\\nAndMask 55.8 ±5.1 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 52.3\\nDIV A 76.9 ±6.1 61.1 ±5.9 47.6 ±3.4 48.6 ±4.2 51.1 ±7.0 52.9 ±6.1 38.5 ±5.0 36.5 ±6.8 51.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='AndMask 55.8 ±5.1 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 52.3\\nDIV A 76.9 ±6.1 61.1 ±5.9 47.6 ±3.4 48.6 ±4.2 51.1 ±7.0 52.9 ±6.1 38.5 ±5.0 36.5 ±6.8 51.7\\nLSSAE (Ours) 62.3 ±3.1 63.2 ±6.7 57.6 ±2.6 66.4 ±2.6 63.5 ±3.9 59.5 ±4.0 52.6 ±2.2 61.3 ±1.9 60.8\\nTable 19.PowerSupply. We show the results on each target domain denoted by domain index.\\nAlgorithm 21 22 23 24 25 26 27 28 29 30 Avg\\nERM 69.8 ±1.4 70.0 ±1.4 69.2 ±1.3 64.4 ±1.5 85.8 ±1.0 76.0 ±1.3 70.1 ±1.5 69.8 ±1.5 69.0 ±1.3 65.5 ±1.5 71.0\\nMixup 69.6 ±1.4 69.5 ±1.5 68.3 ±1.5 64.3 ±1.5 87.1 ±1.0 76.6 ±1.3 70.1 ±1.4 69.2 ±1.3 68.1 ±1.5 65.0 ±1.6 70.8\\nMMD 70.0 ±1.3 69.7 ±1.4 68.7 ±1.4 64.8 ±1.5 85.6 ±1.0 76.1 ±1.3 70.0 ±1.5 69.5 ±1.4 68.7 ±1.3 65.6 ±1.5 70.9\\nMLDG 69.7 ±1.4 69.7 ±1.5 68.6 ±1.5 64.6 ±1.5 86.4 ±1.1 76.3 ±1.4 70.1 ±1.4 69.4 ±1.3 68.4 ±1.5 65.6 ±1.5 70.8\\nIRM 69.8 ±1.4 69.5 ±1.4 68.3 ±1.4 64.1 ±1.4 87.2 ±0.9 76.5 ±1.3 70.0 ±1.5 69.1 ±1.5 68.2 ±1.3 65.0 ±1.4 70.8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='IRM 69.8 ±1.4 69.5 ±1.4 68.3 ±1.4 64.1 ±1.4 87.2 ±0.9 76.5 ±1.3 70.0 ±1.5 69.1 ±1.5 68.2 ±1.3 65.0 ±1.4 70.8\\nRSC 69.9 ±1.4 69.6 ±1.4 68.6 ±1.4 64.4 ±1.5 86.6 ±1.0 76.3 ±1.3 70.0 ±1.5 69.4 ±1.4 68.4 ±1.3 65.4 ±1.5 70.9\\nMTL 69.6 ±1.4 69.4 ±1.5 68.2 ±1.6 64.2 ±1.5 87.4 ±1.2 76.6 ±1.3 69.9 ±1.5 69.1 ±1.5 68.2 ±1.5 64.6 ±1.4 70.7\\nFish 69.7 ±1.4 69.4 ±1.4 68.2 ±1.4 64.2 ±1.4 87.3 ±1.0 76.6 ±1.3 69.9 ±1.5 69.2 ±1.5 68.2 ±1.3 65.2 ±1.5 70.8\\nCORAL 69.9 ±1.4 69.7 ±1.4 68.9 ±1.4 64.6 ±1.4 86.1 ±1.0 76.3 ±1.3 70.0 ±1.5 69.5 ±1.5 68.8 ±1.3 65.7 ±1.5 71.0\\nAndMask 69.9 ±1.4 69.4 ±1.4 68.2 ±1.3 64.0 ±1.4 87.4 ±0.9 76.7 ±1.3 70.0 ±1.5 69.1 ±1.5 68.0 ±1.3 64.7 ±1.5 70.7\\nDIV A 69.7 ±1.4 69.5 ±1.3 68.2 ±1.4 63.9 ±1.5 87.5 ±1.0 76.5 ±1.3 69.9 ±1.5 69.1 ±1.5 68.1 ±1.3 64.7 ±1.5 70.7\\nLSSAE (Ours) 70.0 ±1.4 69.8 ±1.4 69.0 ±1.5 65.4 ±1.4 85.1 ±1.1 76.0 ±1.4 70.1 ±1.7 69.9 ±1.3 69.0 ±1.6 66.3 ±1.4 71.1')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31665d4c",
   "metadata": {},
   "source": [
    "### Embedding and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db335bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb, uuid\n",
    "from chromadb.config import Settings\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51590f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbbeddingManager at 0x1fdd38c14c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    # def get_embedding_dimension(self) -> int:\n",
    "    #     if not self.model:\n",
    "    #         raise ValueError(\"Model not loaded\")\n",
    "    #     return self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "## initialise the embedding manager\n",
    "embedding_manager = EmbbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453e2af",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217c21e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing document in collection: 383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1fdd3a45040>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path = self.persist_directory)\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata={\n",
    "                \"description\": \"PDF document embedding for RAG\",\n",
    "                \"hnsw:space\": \"cosine\"  # cosine distance keeps scores bounded\n",
    "            }\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing document in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents:List[Any], embeddings:np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match the number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        document_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            document_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "\n",
    "                metadatas=metadatas,\n",
    "                documents=document_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents toi vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vecctor store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47693cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nYaroslav Ganin GANIN @SKOLTECH .RU\\nVictor Lempitsky LEMPITSKY @SKOLTECH .RU\\nSkolkovo Institute of Science and Technology (Skoltech)\\nAbstract\\nTop-performing deep architectures are trained on\\nmassive amounts of labeled data. In the absence\\nof labeled data for a certain task, domain adap-\\ntation often provides an attractive option given\\nthat labeled data of similar nature but from a dif-\\nferent domain (e.g. synthetic images) are avail-\\nable. Here, we propose a new approach to do-\\nmain adaptation in deep architectures that can\\nbe trained on large amount of labeled data from\\nthe source domain and large amount of unlabeled\\ndata from the target domain (no labeled target-\\ndomain data is necessary).\\nAs the training progresses, the approach pro-\\nmotes the emergence of “deep” features that are\\n(i) discriminative for the main learning task on\\nthe source domain and (ii) invariant with respect\\nto the shift between the domains. We show that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='motes the emergence of “deep” features that are\\n(i) discriminative for the main learning task on\\nthe source domain and (ii) invariant with respect\\nto the shift between the domains. We show that\\nthis adaptation behaviour can be achieved in al-\\nmost any feed-forward model by augmenting it\\nwith few standard layers and a simple new gra-\\ndient reversal layer. The resulting augmented\\narchitecture can be trained using standard back-\\npropagation.\\nOverall, the approach can be implemented with\\nlittle effort using any of the deep-learning pack-\\nages. The method performs very well in a se-\\nries of image classiﬁcation experiments, achiev-\\ning adaptation effect in the presence of big do-\\nmain shifts and outperforming previous state-of-\\nthe-art on Ofﬁce datasets.\\n1. Introduction\\nDeep feed-forward architectures have brought impressive\\nadvances to the state-of-the-art across a wide variety of\\nmachine-learning tasks and applications. At the moment,\\nhowever, these leaps in performance come only when a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='advances to the state-of-the-art across a wide variety of\\nmachine-learning tasks and applications. At the moment,\\nhowever, these leaps in performance come only when a\\nlarge amount of labeled training data is available. At the\\nsame time, for problems lacking labeled data, it may be\\nstill possible to obtain training sets that are big enough for\\ntraining large-scale deep models, but that suffer from the\\nshift in data distribution from the actual data encountered\\nat “test time”. One particularly important example is syn-\\nthetic or semi-synthetic training data, which may come in\\nabundance and be fully labeled, but which inevitably have\\na distribution that is different from real data (Liebelt &\\nSchmid, 2010; Stark et al., 2010; V´azquez et al., 2014; Sun\\n& Saenko, 2014).\\nLearning a discriminative classiﬁer or other predictor in\\nthe presence of a shift between training and test distribu-\\ntions is known as domain adaptation (DA). A number of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='& Saenko, 2014).\\nLearning a discriminative classiﬁer or other predictor in\\nthe presence of a shift between training and test distribu-\\ntions is known as domain adaptation (DA). A number of\\napproaches to domain adaptation has been suggested in the\\ncontext of shallow learning, e.g. in the situation when data\\nrepresentation/features are given and ﬁxed. The proposed\\napproaches then build the mappings between the source\\n(training-time) and the target (test-time) domains, so that\\nthe classiﬁer learned for the source domain can also be ap-\\nplied to the target domain, when composed with the learned\\nmapping between domains. The appeal of the domain\\nadaptation approaches is the ability to learn a mapping be-\\ntween domains in the situation when the target domain data\\nare either fully unlabeled ( unsupervised domain annota-\\ntion) or have few labeled samples (semi-supervised domain\\nadaptation). Below, we focus on the harder unsupervised\\ncase, although the proposed approach can be generalized to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='tion) or have few labeled samples (semi-supervised domain\\nadaptation). Below, we focus on the harder unsupervised\\ncase, although the proposed approach can be generalized to\\nthe semi-supervised case rather straightforwardly.\\nUnlike most previous papers on domain adaptation that\\nworked with ﬁxed feature representations, we focus on\\ncombining domain adaptation and deep feature learning\\nwithin one training process (deep domain adaptation). Our\\ngoal is to embed domain adaptation into the process of\\nlearning representation, so that the ﬁnal classiﬁcation de-\\ncisions are made based on features that are both discrim-\\ninative and invariant to the change of domains, i.e. have\\nthe same or very similar distributions in the source and the\\ntarget domains. In this way, the obtained feed-forward net-\\nwork can be applicable to the target domain without being\\nhindered by the shift between the two domains.\\nWe thus focus on learning features that combine (i)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='work can be applicable to the target domain without being\\nhindered by the shift between the two domains.\\nWe thus focus on learning features that combine (i)\\ndiscriminativeness and (ii) domain-invariance. This is\\nachieved by jointly optimizing the underlying features as\\nwell as two discriminative classiﬁers operating on these\\nfeatures: (i) the label predictor that predicts class labels\\nand is used both during training and at test time and (ii) the\\narXiv:1409.7495v2  [stat.ML]  27 Feb 2015'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\ndomain classiﬁer that discriminates between the source and\\nthe target domains during training. While the parameters of\\nthe classiﬁers are optimized in order to minimize their error\\non the training set, the parameters of the underlying deep\\nfeature mapping are optimized in order tominimize the loss\\nof the label classiﬁer and tomaximize the loss of the domain\\nclassiﬁer. The latter encourages domain-invariant features\\nto emerge in the course of the optimization.\\nCrucially, we show that all three training processes can\\nbe embedded into an appropriately composed deep feed-\\nforward network (Figure 1) that uses standard layers and\\nloss functions, and can be trained using standard backprop-\\nagation algorithms based on stochastic gradient descent or\\nits modiﬁcations (e.g. SGD with momentum). Our ap-\\nproach is generic as it can be used to add domain adaptation\\nto any existing feed-forward architecture that is trainable by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='its modiﬁcations (e.g. SGD with momentum). Our ap-\\nproach is generic as it can be used to add domain adaptation\\nto any existing feed-forward architecture that is trainable by\\nbackpropagation. In practice, the only non-standard com-\\nponent of the proposed architecture is a rather trivial gra-\\ndient reversal layer that leaves the input unchanged during\\nforward propagation and reverses the gradient by multiply-\\ning it by a negative scalar during the backpropagation.\\nBelow, we detail the proposed approach to domain adap-\\ntation in deep architectures, and present results on tradi-\\ntional deep learning image datasets (such as MNIST (Le-\\nCun et al., 1998) and SVHN (Netzer et al., 2011)) as well\\nas on O FFICE benchmarks (Saenko et al., 2010), where\\nthe proposed method considerably improves over previous\\nstate-of-the-art accuracy.\\n2. Related work\\nA large number of domain adaptation methods have been\\nproposed over the recent years, and here we focus on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='state-of-the-art accuracy.\\n2. Related work\\nA large number of domain adaptation methods have been\\nproposed over the recent years, and here we focus on the\\nmost related ones. Multiple methods perform unsuper-\\nvised domain adaptation by matching the feature distri-\\nbutions in the source and the target domains. Some ap-\\nproaches perform this by reweighing or selecting samples\\nfrom the source domain (Borgwardt et al., 2006; Huang\\net al., 2006; Gong et al., 2013), while others seek an ex-\\nplicit feature space transformation that would map source\\ndistribution into the target ones (Pan et al., 2011; Gopalan\\net al., 2011; Baktashmotlagh et al., 2013). An important\\naspect of the distribution matching approach is the way the\\n(dis)similarity between distributions is measured. Here,\\none popular choice is matching the distribution means in\\nthe kernel-reproducing Hilbert space (Borgwardt et al.,\\n2006; Huang et al., 2006), whereas (Gong et al., 2012; Fer-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='one popular choice is matching the distribution means in\\nthe kernel-reproducing Hilbert space (Borgwardt et al.,\\n2006; Huang et al., 2006), whereas (Gong et al., 2012; Fer-\\nnando et al., 2013) map the principal axes associated with\\neach of the distributions. Our approach also attempts to\\nmatch feature space distributions, however this is accom-\\nplished by modifying the feature representation itself rather\\nthan by reweighing or geometric transformation. Also, our\\nmethod uses (implicitly) a rather different way to measure\\nthe disparity between distributions based on their separa-\\nbility by a deep discriminatively-trained classiﬁer.\\nSeveral approaches perform gradual transition from the\\nsource to the target domain (Gopalan et al., 2011; Gong\\net al., 2012) by a gradual change of the training distribu-\\ntion. Among these methods, (S. Chopra & Gopalan, 2013)\\ndoes this in a “deep” way by the layerwise training of a\\nsequence of deep autoencoders, while gradually replacing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='tion. Among these methods, (S. Chopra & Gopalan, 2013)\\ndoes this in a “deep” way by the layerwise training of a\\nsequence of deep autoencoders, while gradually replacing\\nsource-domain samples with target-domain samples. This\\nimproves over a similar approach of (Glorot et al., 2011)\\nthat simply trains a single deep autoencoder for both do-\\nmains. In both approaches, the actual classiﬁer/predictor\\nis learned in a separate step using the feature representa-\\ntion learned by autoencoder(s). In contrast to (Glorot et al.,\\n2011; S. Chopra & Gopalan, 2013), our approach performs\\nfeature learning, domain adaptation and classiﬁer learning\\njointly, in a uniﬁed architecture, and using a single learning\\nalgorithm (backpropagation). We therefore argue that our\\napproach is simpler (both conceptually and in terms of its\\nimplementation). Our method also achieves considerably\\nbetter results on the popular OFFICE benchmark.\\nWhile the above approaches perform unsupervised domain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='implementation). Our method also achieves considerably\\nbetter results on the popular OFFICE benchmark.\\nWhile the above approaches perform unsupervised domain\\nadaptation, there are approaches that perform supervised\\ndomain adaptation by exploiting labeled data from the tar-\\nget domain. In the context of deep feed-forward archi-\\ntectures, such data can be used to “ﬁne-tune” the net-\\nwork trained on the source domain (Zeiler & Fergus, 2013;\\nOquab et al., 2014; Babenko et al., 2014). Our approach\\ndoes not require labeled target-domain data. At the same\\ntime, it can easily incorporate such data when it is avail-\\nable.\\nAn idea related to ours is described in (Goodfellow et al.,\\n2014). While their goal is quite different (building gener-\\native deep networks that can synthesize samples), the way\\nthey measure and minimize the discrepancy between the\\ndistribution of the training data and the distribution of the\\nsynthesized data is very similar to the way our architecture'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='they measure and minimize the discrepancy between the\\ndistribution of the training data and the distribution of the\\nsynthesized data is very similar to the way our architecture\\nmeasures and minimizes the discrepancy between feature\\ndistributions for the two domains.\\nFinally, a recent and concurrent report by (Tzeng et al.,\\n2014) also focuses on domain adaptation in feed-forward\\nnetworks. Their set of techniques measures and minimizes\\nthe distance of the data means across domains. This ap-\\nproach may be regarded as a “ﬁrst-order” approximation\\nto our approach, which seeks a tighter alignment between\\ndistributions.\\n3. Deep Domain Adaptation\\n3.1. The model\\nWe now detail the proposed model for the domain adap-\\ntation. We assume that the model works with input sam-\\nples x ∈ X, where X is some input space and cer-\\ntain labels (output) y from the label space Y. Below,\\nwe assume classiﬁcation problems where Y is a ﬁnite set\\n(Y = {1,2,...L }), however our approach is generic and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='tain labels (output) y from the label space Y. Below,\\nwe assume classiﬁcation problems where Y is a ﬁnite set\\n(Y = {1,2,...L }), however our approach is generic and\\ncan handle any output label space that other deep feed-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nFigure 1.The proposed architecture includes a deep feature extractor (green) and a deep label predictor (blue), which together form\\na standard feed-forward architecture. Unsupervised domain adaptation is achieved by adding a domain classiﬁer (red) connected to the\\nfeature extractor via a gradient reversal layer that multiplies the gradient by a certain negative constant during the backpropagation-\\nbased training. Otherwise, the training proceeds in a standard way and minimizes the label prediction loss (for source examples) and\\nthe domain classiﬁcation loss (for all samples). Gradient reversal ensures that the feature distributions over the two domains are made\\nsimilar (as indistinguishable as possible for the domain classiﬁer), thus resulting in the domain-invariant features.\\nforward models can handle. We further assume that there\\nexist two distributions S(x,y) and T(x,y) on X ⊗Y,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='forward models can handle. We further assume that there\\nexist two distributions S(x,y) and T(x,y) on X ⊗Y,\\nwhich will be referred to as the source distribution and\\nthe target distribution (or the source domain and the tar-\\nget domain). Both distributions are assumed complex and\\nunknown, and furthermore similar but different (in other\\nwords, Sis “shifted” from T by some domain shift).\\nOur ultimate goal is to be able to predict labels y given\\nthe input x for the target distribution. At training time,\\nwe have an access to a large set of training samples\\n{x1,x2,..., xN}from both the source and the target do-\\nmains distributed according to the marginal distributions\\nS(x) and T(x). We denote with di the binary variable (do-\\nmain label) for the i-th example, which indicates whether\\nxi come from the source distribution (xi∼S(x) if di=0) or\\nfrom the target distribution (xi∼T(x) if di=1). For the ex-\\namples from the source distribution (di=0) the correspond-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='xi come from the source distribution (xi∼S(x) if di=0) or\\nfrom the target distribution (xi∼T(x) if di=1). For the ex-\\namples from the source distribution (di=0) the correspond-\\ning labels yi ∈Y are known at training time. For the ex-\\namples from the target domains, we do not know the labels\\nat training time, and we want to predict such labels at test\\ntime.\\nWe now deﬁne a deep feed-forward architecture that for\\neach input x predicts its label y ∈Y and its domain label\\nd ∈{0,1}. We decompose such mapping into three parts.\\nWe assume that the input x is ﬁrst mapped by a mapping\\nGf (a feature extractor) to a D-dimensional feature vector\\nf ∈RD. The feature mapping may also include several\\nfeed-forward layers and we denote the vector of parame-\\nters of all layers in this mapping as θf, i.e. f = Gf(x; θf).\\nThen, the feature vector f is mapped by a mapping Gy (la-\\nbel predictor) to the label y, and we denote the parameters\\nof this mapping with θy. Finally, the same feature vector f'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Then, the feature vector f is mapped by a mapping Gy (la-\\nbel predictor) to the label y, and we denote the parameters\\nof this mapping with θy. Finally, the same feature vector f\\nis mapped to the domain label dby a mapping Gd (domain\\nclassiﬁer) with the parameters θd (Figure 1).\\nDuring the learning stage, we aim to minimize the label\\nprediction loss on the annotated part (i.e. the source part)\\nof the training set, and the parameters of both the feature\\nextractor and the label predictor are thus optimized in or-\\nder to minimize the empirical loss for the source domain\\nsamples. This ensures the discriminativeness of the fea-\\ntures f and the overall good prediction performance of the\\ncombination of the feature extractor and the label predictor\\non the source domain.\\nAt the same time, we want to make the features f\\ndomain-invariant. That is, we want to make the dis-\\ntributions S(f) = {Gf(x; θf) |x∼S(x)}and T(f) =\\n{Gf(x; θf) |x∼T(x)}to be similar. Under the covariate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='domain-invariant. That is, we want to make the dis-\\ntributions S(f) = {Gf(x; θf) |x∼S(x)}and T(f) =\\n{Gf(x; θf) |x∼T(x)}to be similar. Under the covariate\\nshift assumption, this would make the label prediction ac-\\ncuracy on the target domain to be the same as on the source\\ndomain (Shimodaira, 2000). Measuring the dissimilarity\\nof the distributions S(f) and T(f) is however non-trivial,\\ngiven that f is high-dimensional, and that the distributions\\nthemselves are constantly changing as learning progresses.\\nOne way to estimate the dissimilarity is to look at the loss\\nof the domain classiﬁer Gd, provided that the parameters\\nθd of the domain classiﬁer have been trained to discrim-\\ninate between the two feature distributions in an optimal\\nway.\\nThis observation leads to our idea. At training time, in or-\\nder to obtain domain-invariant features, we seek the param-\\neters θf of the feature mapping that maximize the loss of\\nthe domain classiﬁer (by making the two feature distribu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='der to obtain domain-invariant features, we seek the param-\\neters θf of the feature mapping that maximize the loss of\\nthe domain classiﬁer (by making the two feature distribu-\\ntions as similar as possible), while simultaneously seeking\\nthe parameters θd of the domain classiﬁer that minimize the\\nloss of the domain classiﬁer. In addition, we seek to mini-\\nmize the loss of the label predictor.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nMore formally, we consider the functional:\\nE(θf,θy,θd) =\\n∑\\ni=1..N\\ndi=0\\nLy\\n(\\nGy(Gf(xi; θf); θy),yi\\n)\\n−\\nλ\\n∑\\ni=1..N\\nLd\\n(\\nGd(Gf(xi; θf); θd),yi\\n)\\n=\\n=\\n∑\\ni=1..N\\ndi=0\\nLi\\ny(θf,θy) −λ\\n∑\\ni=1..N\\nLi\\nd(θf,θd) (1)\\nHere, Ly(·,·) is the loss for label prediction (e.g. multino-\\nmial), Ld(·,·) is the loss for the domain classiﬁcation (e.g.\\nlogistic), while Li\\ny and Li\\nd denote the corresponding loss\\nfunctions evaluated at the i-th training example.\\nBased on our idea, we are seeking the parametersˆθf,ˆθy,ˆθd\\nthat deliver a saddle point of the functional (1):\\n(ˆθf,ˆθy) = arg min\\nθf ,θy\\nE(θf,θy,ˆθd) (2)\\nˆθd = arg max\\nθd\\nE(ˆθf,ˆθy,θd) . (3)\\nAt the saddle point, the parametersθd of the domain classi-\\nﬁer θd minimize the domain classiﬁcation loss (since it en-\\nters into (1) with the minus sign) while the parametersθy of\\nthe label predictor minimize the label prediction loss. The\\nfeature mapping parameters θf minimize the label predic-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='ters into (1) with the minus sign) while the parametersθy of\\nthe label predictor minimize the label prediction loss. The\\nfeature mapping parameters θf minimize the label predic-\\ntion loss (i.e. the features are discriminative), while maxi-\\nmizing the domain classiﬁcation loss (i.e. the features are\\ndomain-invariant). The parameter λcontrols the trade-off\\nbetween the two objectives that shape the features during\\nlearning.\\nBelow, we demonstrate that standard stochastic gradient\\nsolvers (SGD) can be adapted for the search of the saddle\\npoint (2)-(3).\\n3.2. Optimization with backpropagation\\nA saddle point (2)-(3) can be found as a stationary point of\\nthe following stochastic updates:\\nθf ←− θf −µ\\n(\\n∂Li\\ny\\n∂θf\\n−λ∂Li\\nd\\n∂θf\\n)\\n(4)\\nθy ←− θy −µ∂Li\\ny\\n∂θy\\n(5)\\nθd ←− θd −µ∂Li\\nd\\n∂θd\\n(6)\\nwhere µis the learning rate (which can vary over time).\\nThe updates (4)-(6) are very similar to stochastic gradient\\ndescent (SGD) updates for a feed-forward deep model that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='θd ←− θd −µ∂Li\\nd\\n∂θd\\n(6)\\nwhere µis the learning rate (which can vary over time).\\nThe updates (4)-(6) are very similar to stochastic gradient\\ndescent (SGD) updates for a feed-forward deep model that\\ncomprises feature extractor fed into the label predictor and\\ninto the domain classiﬁer. The difference is the −λfactor\\nin (4) (the difference is important, as without such factor,\\nstochastic gradient descent would try to make features dis-\\nsimilar across domains in order to minimize the domain\\nclassiﬁcation loss). Although direct implementation of (4)-\\n(6) as SGD is not possible, it is highly desirable to reduce\\nthe updates (4)-(6) to some form of SGD, since SGD (and\\nits variants) is the main learning algorithm implemented in\\nmost packages for deep learning.\\nFortunately, such reduction can be accomplished by intro-\\nducing a special gradient reversal layer (GRL) deﬁned as\\nfollows. The gradient reversal layer has no parameters as-\\nsociated with it (apart from the meta-parameter λ, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='ducing a special gradient reversal layer (GRL) deﬁned as\\nfollows. The gradient reversal layer has no parameters as-\\nsociated with it (apart from the meta-parameter λ, which\\nis not updated by backpropagation). During the forward\\npropagation, GRL acts as an identity transform. During\\nthe backpropagation though, GRL takes the gradient from\\nthe subsequent level, multiplies it by −λ and passes it to\\nthe preceding layer. Implementing such layer using exist-\\ning object-oriented packages for deep learning is simple, as\\ndeﬁning procedures for forwardprop (identity transform),\\nbackprop (multiplying by a constant), and parameter up-\\ndate (nothing) is trivial.\\nThe GRL as deﬁned above is inserted between the feature\\nextractor and the domain classiﬁer, resulting in the archi-\\ntecture depicted in Figure 1. As the backpropagation pro-\\ncess passes through the GRL, the partial derivatives of the\\nloss that is downstream the GRL (i.e. Ld) w.r.t. the layer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='tecture depicted in Figure 1. As the backpropagation pro-\\ncess passes through the GRL, the partial derivatives of the\\nloss that is downstream the GRL (i.e. Ld) w.r.t. the layer\\nparameters that are upstream the GRL (i.e. θf) get multi-\\nplied by −λ, i.e. ∂Ld\\n∂θf\\nis effectively replaced with −λ∂Ld\\n∂θf\\n.\\nTherefore, running SGD in the resulting model implements\\nthe updates (4)-(6) and converges to a saddle point of (1).\\nMathematically, we can formally treat the gradient reversal\\nlayer as a “pseudo-function”Rλ(x) deﬁned by two (incom-\\npatible) equations describing its forward- and backpropa-\\ngation behaviour:\\nRλ(x) = x (7)\\ndRλ\\ndx = −λI (8)\\nwhere I is an identity matrix. We can then deﬁne the\\nobjective “pseudo-function” of (θf,θy,θd) that is being\\noptimized by the stochastic gradient descent within our\\nmethod:\\n˜E(θf,θy,θd) =\\n∑\\ni=1..N\\ndi=0\\nLy\\n(\\nGy(Gf(xi; θf); θy),yi\\n)\\n+\\n∑\\ni=1..N\\nLd\\n(\\nGd(Rλ(Gf(xi; θf)); θd),yi\\n)\\n(9)\\nRunning updates (4)-(6) can then be implemented as do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='method:\\n˜E(θf,θy,θd) =\\n∑\\ni=1..N\\ndi=0\\nLy\\n(\\nGy(Gf(xi; θf); θy),yi\\n)\\n+\\n∑\\ni=1..N\\nLd\\n(\\nGd(Rλ(Gf(xi; θf)); θd),yi\\n)\\n(9)\\nRunning updates (4)-(6) can then be implemented as do-\\ning SGD for (9) and leads to the emergence of features\\nthat are domain-invariant and discriminative at the same\\ntime. After the learning, the label predictor y(x) =\\nGy(Gf(x; θf); θy) can be used to predict labels for sam-\\nples from the target domain (as well as from the source\\ndomain).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nThe simple learning procedure outlined above can be re-\\nderived/generalized along the lines suggested in (Goodfel-\\nlow et al., 2014) (see Appendix A).\\n3.3. Relation to H∆H-distance\\nIn this section we give a brief analysis of our method in\\nterms of H∆H-distance (Ben-David et al., 2010; Cortes &\\nMohri, 2011) which is widely used in the theory of non-\\nconservative domain adaptation. Formally,\\ndH∆H(S,T) = 2 sup\\nh1,h2∈H\\n|Pf ∼S[h1(f) ̸= h2(f)]−\\n−Pf ∼T[h1(f) ̸= h2(f)]| (10)\\ndeﬁnes a discrepancy distance between two distributions S\\nand T w.r.t. a hypothesis set H. Using this notion one can\\nobtain a probabilistic bound (Ben-David et al., 2010) on the\\nperformance εT(h) of some classiﬁer hfrom T evaluated\\non the target domain given its performance εS(h) on the\\nsource domain:\\nεT(h) ≤εS(h) + 1\\n2dH∆H(S,T) + C, (11)\\nwhere Sand T are source and target distributions respec-\\ntively, and Cdoes not depend on particular h.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='source domain:\\nεT(h) ≤εS(h) + 1\\n2dH∆H(S,T) + C, (11)\\nwhere Sand T are source and target distributions respec-\\ntively, and Cdoes not depend on particular h.\\nConsider ﬁxed Sand T over the representation space pro-\\nduced by the feature extractor Gf and a family of label\\npredictors Hp. We assume that the family of domain classi-\\nﬁers Hd is rich enough to contain the symmetric difference\\nhypothesis set of Hp:\\nHp∆Hp = {h|h= h1 ⊕h2 , h1,h2 ∈Hp}. (12)\\nIt is not an unrealistic assumption as we have a freedom to\\npick Hd whichever we want. For example, we can set the\\narchitecture of the domain discriminator to be the layer-\\nby-layer concatenation of two replicas of the label predic-\\ntor followed by a two layer non-linear perceptron aimed to\\nlearn the XOR-function. Given the assumption holds, one\\ncan easily show that training the Gd is closely related to\\nthe estimation of dHp∆Hp (S,T). Indeed,\\ndHp∆Hp (S,T) =\\n= 2 sup\\nh∈Hp∆Hp\\n|Pf ∼S[h(f) = 1] −Pf ∼T[h(f) = 1]|≤\\n≤2 sup\\nh∈Hd'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='can easily show that training the Gd is closely related to\\nthe estimation of dHp∆Hp (S,T). Indeed,\\ndHp∆Hp (S,T) =\\n= 2 sup\\nh∈Hp∆Hp\\n|Pf ∼S[h(f) = 1] −Pf ∼T[h(f) = 1]|≤\\n≤2 sup\\nh∈Hd\\n|Pf ∼S[h(f) = 1] −Pf ∼T[h(f) = 1]|=\\n= 2 sup\\nh∈Hd\\n|1 −α(h)|= 2 sup\\nh∈Hd\\n[α(h) −1]\\n(13)\\nwhere α(h) = Pf ∼S[h(f) = 0] + Pf ∼T[h(f) = 1] is max-\\nimized by the optimal Gd.\\nThus, optimal discriminator gives the upper bound for\\ndHp∆Hp (S,T). At the same time, backpropagation of\\nthe reversed gradient changes the representation space\\nso that α(Gd) becomes smaller effectively reducing\\ndHp∆Hp (S,T) and leading to the better approximation of\\nεT(Gy) by εS(Gy).\\n4. Experiments\\nWe perform extensive evaluation of the proposed approach\\non a number of popular image datasets and their modiﬁ-\\ncations. These include large-scale datasets of small im-\\nages popular with deep learning methods, and the O FFICE\\ndatasets (Saenko et al., 2010), which are ade facto standard\\nfor domain adaptation in computer vision, but have much'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='ages popular with deep learning methods, and the O FFICE\\ndatasets (Saenko et al., 2010), which are ade facto standard\\nfor domain adaptation in computer vision, but have much\\nfewer images.\\nBaselines. For the bulk of experiments the following base-\\nlines are evaluated. The source-only model is trained with-\\nout consideration for target-domain data (no domain clas-\\nsiﬁer branch included into the network). The train-on-\\ntarget model is trained on the target domain with class\\nlabels revealed. This model serves as an upper bound on\\nDA methods, assuming that target data are abundant and\\nthe shift between the domains is considerable.\\nIn addition, we compare our approach against the recently\\nproposed unsupervised DA method based on subspace\\nalignment (SA) (Fernando et al., 2013), which is simple\\nto setup and test on new datasets, but has also been shown\\nto perform very well in experimental comparisons with\\nother “shallow” DA methods. To boost the performance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='to setup and test on new datasets, but has also been shown\\nto perform very well in experimental comparisons with\\nother “shallow” DA methods. To boost the performance\\nof this baseline, we pick its most important free parame-\\nter (the number of principal components) from the range\\n{2,..., 60}, so that the test performance on the target do-\\nmain is maximized. To apply SA in our setting, we train\\na source-only model and then consider the activations of\\nthe last hidden layer in the label predictor (before the ﬁnal\\nlinear classiﬁer) as descriptors/features, and learn the map-\\nping between the source and the target domains (Fernando\\net al., 2013).\\nSince the SA baseline requires to train a new classiﬁer after\\nadapting the features, and in order to put all the compared\\nsettings on an equal footing, we retrain the last layer of\\nthe label predictor using a standard linear SVM (Fan et al.,\\n2008) for all four considered methods (including ours; the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='settings on an equal footing, we retrain the last layer of\\nthe label predictor using a standard linear SVM (Fan et al.,\\n2008) for all four considered methods (including ours; the\\nperformance on the target domain remains approximately\\nthe same after the retraining).\\nFor the O FFICE dataset (Saenko et al., 2010), we directly\\ncompare the performance of our full network (feature ex-\\ntractor and label predictor) against recent DA approaches\\nusing previously published results.\\nCNN architectures. In general, we compose feature ex-\\ntractor from two or three convolutional layers, picking their\\nexact conﬁgurations from previous works. We give the ex-\\nact architectures in Appendix B.\\nFor the domain adaptator we stick to the three fully con-\\nnected layers ( x → 1024 → 1024 → 2), except for\\nMNIST where we used a simpler ( x → 100 → 2) ar-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nMNIST S YN NUMBERS SVHN S YN SIGNS\\nSOURCE\\nTARGET\\nMNIST-M SVHN MNIST GTSRB\\nFigure 2.Examples of domain pairs used in the experiments. See Section 4.1 for details.\\nMETHOD\\nSOURCE MNIST S YN NUMBERS SVHN S YN SIGNS\\nTARGET MNIST-M SVHN MNIST GTSRB\\nSOURCE ONLY .5749 .8665 .5919 .7400\\nSA (F ERNANDO ET AL ., 2013) .6078 (7.9%) .8672 (1.3%) .6157 (5.9%) .7635 (9.1%)\\nPROPOSED APPROACH .8149 (57.9%) .9048 (66.1%) .7107 (29.3%) .8866 (56.7%)\\nTRAIN ON TARGET .9891 .9244 .9951 .9987\\nTable 1.Classiﬁcation accuracies for digit image classiﬁcations for different source and target domains. MNIST-M corresponds to\\ndifference-blended digits over non-uniform background. The ﬁrst row corresponds to the lower performance bound (i.e. if no adaptation\\nis performed). The last row corresponds to training on the target domain data with known class labels (upper bound on the DA perfor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='is performed). The last row corresponds to training on the target domain data with known class labels (upper bound on the DA perfor-\\nmance). For each of the two DA methods (ours and (Fernando et al., 2013)) we show how much of the gap between the lower and the\\nupper bounds was covered (in brackets). For all ﬁve cases, our approach outperforms (Fernando et al., 2013) considerably, and covers a\\nbig portion of the gap.\\nchitecture to speed up the experiments.\\nFor loss functions, we set Ly and Ld to be the logistic re-\\ngression loss and the binomial cross-entropy respectively.\\nCNN training procedure. The model is trained on 128-\\nsized batches. Images are preprocessed by the mean sub-\\ntraction. A half of each batch is populated by the sam-\\nples from the source domain (with known labels), the rest\\nis comprised of the target domain (with unknown labels).\\nIn order to suppress noisy signal from the domain classiﬁer\\nat the early stages of the training procedure instead of ﬁxing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='is comprised of the target domain (with unknown labels).\\nIn order to suppress noisy signal from the domain classiﬁer\\nat the early stages of the training procedure instead of ﬁxing\\nthe adaptation factor λ, we gradually change it from 0 to 1\\nusing the following schedule:\\nλp = 2\\n1 + exp(−γ·p) −1, (14)\\nwhere γwas set to 10 in all experiments (the schedule was\\nnot optimized/tweaked). Further details on the CNN train-\\ning can be found in Appendix C.\\nVisualizations. We use t-SNE (van der Maaten, 2013) pro-\\njection to visualize feature distributions at different points\\nof the network, while color-coding the domains (Figure 3).\\nWe observe strong correspondence between the success of\\nthe adaptation in terms of the classiﬁcation accuracy for the\\ntarget domain, and the overlap between the domain distri-\\nbutions in such visualizations.\\nChoosing meta-parameters. In general, good unsu-\\npervised DA methods should provide ways to set meta-\\nparameters (such as λ, the learning rate, the momentum'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='butions in such visualizations.\\nChoosing meta-parameters. In general, good unsu-\\npervised DA methods should provide ways to set meta-\\nparameters (such as λ, the learning rate, the momentum\\nrate, the network architecture for our method) in an unsu-\\npervised way, i.e. without referring to labeled data in the\\ntarget domain. In our method, one can assess the per-\\nformance of the whole system (and the effect of chang-\\ning hyper-parameters) by observing the test error on the\\nsource domain and the domain classiﬁer error. In general,\\nwe observed a good correspondence between the success of\\nadaptation and these errors (adaptation is more successful\\nwhen the source domain test error is low, while the domain\\nclassiﬁer error is high). In addition, the layer, where the\\nthe domain adaptator is attached can be picked by comput-\\ning difference between means as suggested in (Tzeng et al.,\\n2014).\\n4.1. Results\\nWe now discuss the experimental settings and the results.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='the domain adaptator is attached can be picked by comput-\\ning difference between means as suggested in (Tzeng et al.,\\n2014).\\n4.1. Results\\nWe now discuss the experimental settings and the results.\\nIn each case, we train on the source dataset and test on a\\ndifferent target domain dataset, with considerable shifts be-\\ntween domains (see Figure 2). The results are summarized\\nin Table 1 and Table 2.\\nMNIST →MNIST-M. Our ﬁrst experiment deals with\\nthe MNIST dataset (LeCun et al., 1998) (source). In or-\\nder to obtain the target domain (MNIST-M) we blend dig-\\nits from the original set over patches randomly extracted\\nfrom color photos from BSDS500 (Arbelaez et al., 2011).\\nThis operation is formally deﬁned for two images I1,I2 as\\nIout\\nijk = |I1\\nijk −I2\\nijk|, where i,j are the coordinates of a\\npixel and k is a channel index. In other words, an output\\nsample is produced by taking a patch from a photo and in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nMETHOD\\nSOURCE AMAZON DSLR W EBCAM\\nTARGET WEBCAM WEBCAM DSLR\\nGFK(PLS, PCA) (G ONG ET AL ., 2012) .464 ±.005 .613 ±.004 .663 ±.004\\nSA (F ERNANDO ET AL ., 2013) .450 .648 .699\\nDA-NBNN (T OMMASI & CAPUTO , 2013) .528 ±.037 .766 ±.017 .762 ±.025\\nDLID (S. C HOPRA & GOPALAN , 2013) .519 .782 .899\\nDECAF6 SOURCE ONLY (DONAHUE ET AL ., 2014) .522 ±.017 .915 ±.015 –\\nDANN (G HIFARY ET AL ., 2014) .536 ±.002 .712 ±.000 .835 ±.000\\nDDC (T ZENG ET AL ., 2014) .594 ±.008 .925 ±.003 .917 ±.008\\nPROPOSED APPROACH .673 ±.017 .940 ±.008 .937 ±.010\\nTable 2.Accuracy evaluation of different DA approaches on the standard O FFICE (Saenko et al., 2010) dataset. Our method (last row)\\noutperforms competitors setting the new state-of-the-art.\\nMNIST →MNIST-M: top feature extractor layer\\n(a) Non-adapted\\n (b) Adapted\\nSYN NUMBERS →SVHN: last hidden layer of the label predictor\\n(a) Non-adapted\\n (b) Adapted'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='MNIST →MNIST-M: top feature extractor layer\\n(a) Non-adapted\\n (b) Adapted\\nSYN NUMBERS →SVHN: last hidden layer of the label predictor\\n(a) Non-adapted\\n (b) Adapted\\nFigure 3.The effect of adaptation on the distribution of the extracted features (best viewed in color). The ﬁgure shows t-SNE (van der\\nMaaten, 2013) visualizations of the CNN’s activations(a) in case when no adaptation was performed and(b) in case when our adaptation\\nprocedure was incorporated into training. Blue points correspond to the source domain examples, whilered ones correspond to the target\\ndomain. In all cases, the adaptation in our method makes the two distributions of features much closer.\\nverting its pixels at positions corresponding to the pixels of\\na digit. For a human the classiﬁcation task becomes only\\nslightly harder compared to the original dataset (the digits\\nare still clearly distinguishable) whereas for a CNN trained\\non MNIST this domain is quite distinct, as the background'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='slightly harder compared to the original dataset (the digits\\nare still clearly distinguishable) whereas for a CNN trained\\non MNIST this domain is quite distinct, as the background\\nand the strokes are no longer constant. Consequently, the\\nsource-only model performs poorly. Our approach suc-\\nceeded at aligning feature distributions (Figure 3), which\\nled to successful adaptation results (considering that the\\nadaptation is unsupervised). At the same time, the im-\\nprovement over source-only model achieved by subspace\\nalignment (SA) (Fernando et al., 2013) is quite modest,\\nthus highlighting the difﬁculty of the adaptation task.\\nSynthetic numbers →SVHN. To address a common sce-\\nnario of training on synthetic data and testing on real data,\\nwe use Street-View House Number dataset SVHN (Netzer\\net al., 2011) as the target domain and synthetic digits as the\\nsource. The latter (S YN NUMBERS ) consists of 500,000\\nimages generated by ourselves from Windows fonts by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='et al., 2011) as the target domain and synthetic digits as the\\nsource. The latter (S YN NUMBERS ) consists of 500,000\\nimages generated by ourselves from Windows fonts by\\nvarying the text (that includes different one-, two-, and\\nthree-digit numbers), positioning, orientation, background\\nand stroke colors, and the amount of blur. The degrees of\\nvariation were chosen manually to simulate SVHN, how-\\never the two datasets are still rather distinct, the biggest\\ndifference being the structured clutter in the background of\\nSVHN images.\\nThe proposed backpropagation-based technique works well\\ncovering two thirds of the gap between training with source\\ndata only and training on target domain data with known\\ntarget labels. In contrast, SA (Fernando et al., 2013) does\\nnot result in any signiﬁcant improvement in the classiﬁca-\\ntion accuracy, thus highlighting that the adaptation task is\\neven more challenging than in the case of the MNIST ex-\\nperiment.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='not result in any signiﬁcant improvement in the classiﬁca-\\ntion accuracy, thus highlighting that the adaptation task is\\neven more challenging than in the case of the MNIST ex-\\nperiment.\\nMNIST ↔SVHN. In this experiment, we further increase\\nthe gap between distributions, and test on MNIST and\\nSVHN, which are signiﬁcantly different in appearance.\\nTraining on SVHN even without adaptation is challeng-\\ning — classiﬁcation error stays high during the ﬁrst 150\\nepochs. In order to avoid ending up in a poor local min-\\nimum we, therefore, do not use learning rate annealing\\nhere. Obviously, the two directions (MNIST →SVHN\\nand SVHN → MNIST) are not equally difﬁcult. As\\nSVHN is more diverse, a model trained on SVHN is ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\n1 2 3 4 5\\n·104\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nBatches seen\\nValidation error\\nReal data only\\nSynthetic data only\\nBoth\\nFigure 4.Semi-supervised domain adaptation for the trafﬁc signs.\\nAs labeled target domain data are shown to the method, it achieves\\nsigniﬁcantly lower error than the model trained on target domain\\ndata only or on source domain data only.\\npected to be more generic and to perform reasonably on\\nthe MNIST dataset. This, indeed, turns out to be the case\\nand is supported by the appearance of the feature distribu-\\ntions. We observe a quite strong separation between the\\ndomains when we feed them into the CNN trained solely\\non MNIST, whereas for the SVHN-trained network the\\nfeatures are much more intermixed. This difference prob-\\nably explains why our method succeeded in improving the\\nperformance by adaptation in the SVHN →MNIST sce-\\nnario (see Table 1) but not in the opposite direction (SA is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='ably explains why our method succeeded in improving the\\nperformance by adaptation in the SVHN →MNIST sce-\\nnario (see Table 1) but not in the opposite direction (SA is\\nnot able to perform adaptation in this case either). Unsu-\\npervised adaptation from MNIST to SVHN gives a failure\\nexample for our approach (we are unaware of any unsuper-\\nvised DA methods capable of performing such adaptation).\\nSynthetic Signs →GTSRB. Overall, this setting is sim-\\nilar to the S YN NUMBERS →SVHN experiment, except\\nthe distribution of the features is more complex due to the\\nsigniﬁcantly larger number of classes (43 instead of 10).\\nFor the source domain we obtained 100,000 synthetic im-\\nages (which we call S YN SIGNS ) simulating various pho-\\ntoshooting conditions. Once again, our method achieves\\na sensible increase in performance once again proving its\\nsuitability for the synthetic-to-real data adaptation.\\nAs an additional experiment, we also evaluate the pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='a sensible increase in performance once again proving its\\nsuitability for the synthetic-to-real data adaptation.\\nAs an additional experiment, we also evaluate the pro-\\nposed algorithm for semi-supervised domain adaptation,\\ni.e. when one is additionally provided with a small amount\\nof labeled target data. For that purpose we split GTSRB\\ninto the train set (1280 random samples with labels) and\\nthe validation set (the rest of the dataset). The validation\\npart is used solely for the evaluation and does not partic-\\nipate in the adaptation. The training procedure changes\\nslightly as the label predictor is now exposed to the tar-\\nget data. Figure 4 shows the change of the validation error\\nthroughout the training. While the graph clearly suggests\\nthat our method can be used in the semi-supervised setting,\\nthorough veriﬁcation of semi-supervised setting is left for\\nfuture work.\\nOfﬁce dataset. We ﬁnally evaluate our method on O F-\\nFICE dataset, which is a collection of three distinct do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='thorough veriﬁcation of semi-supervised setting is left for\\nfuture work.\\nOfﬁce dataset. We ﬁnally evaluate our method on O F-\\nFICE dataset, which is a collection of three distinct do-\\nmains: A MAZON , DSLR, and W EBCAM . Unlike previ-\\nously discussed datasets, OFFICE is rather small-scale with\\nonly 2817 labeled images spread across 31 different cat-\\negories in the largest domain. The amount of available\\ndata is crucial for a successful training of a deep model,\\nhence we opted for the ﬁne-tuning of the CNN pre-trained\\non the ImageNet (Jia et al., 2014) as it is done in some re-\\ncent DA works (Donahue et al., 2014; Tzeng et al., 2014;\\nHoffman et al., 2013). We make our approach more com-\\nparable with (Tzeng et al., 2014) by using exactly the same\\nnetwork architecture replacing domain mean-based regu-\\nlarization with the domain classiﬁer.\\nFollowing most previous works, we evaluate our method\\nusing 5 random splits for each of the 3 transfer tasks com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='larization with the domain classiﬁer.\\nFollowing most previous works, we evaluate our method\\nusing 5 random splits for each of the 3 transfer tasks com-\\nmonly used for evaluation. Our training protocol is close to\\n(Tzeng et al., 2014; Saenko et al., 2010; Gong et al., 2012)\\nas we use the same number of labeled source-domain im-\\nages per category. Unlike those works and similarly to e.g.\\nDLID (S. Chopra & Gopalan, 2013) we use the whole un-\\nlabeled target domain (as the premise of our method is the\\nabundance of unlabeled data in the target domain). Un-\\nder this transductive setting, our method is able to improve\\npreviously-reported state-of-the-art accuracy for unsuper-\\nvised adaptation very considerably (Table 2), especially in\\nthe most challenging AMAZON →WEBCAM scenario (the\\ntwo domains with the largest domain shift).\\n5. Discussion\\nWe have proposed a new approach to unsupervised do-\\nmain adaptation of deep feed-forward architectures, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='two domains with the largest domain shift).\\n5. Discussion\\nWe have proposed a new approach to unsupervised do-\\nmain adaptation of deep feed-forward architectures, which\\nallows large-scale training based on large amount of an-\\nnotated data in the source domain and large amount of\\nunannotated data in the target domain. Similarly to many\\nprevious shallow and deep DA techniques, the adaptation\\nis achieved through aligning the distributions of features\\nacross the two domains. However, unlike previous ap-\\nproaches, the alignment is accomplished through standard\\nbackpropagation training. The approach is therefore rather\\nscalable, and can be implemented using any deep learning\\npackage. To this end we plan to release the source code for\\nthe Gradient Reversal layer along with the usage examples\\nas an extension to Caffe (Jia et al., 2014).\\nFurther evaluation on larger-scale tasks and in semi-\\nsupervised settings constitutes future work. It is also in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='as an extension to Caffe (Jia et al., 2014).\\nFurther evaluation on larger-scale tasks and in semi-\\nsupervised settings constitutes future work. It is also in-\\nteresting whether the approach can beneﬁt from a good ini-\\ntialization of the feature extractor. For this, a natural choice\\nwould be to use deep autoencoder/deconvolution network\\ntrained on both domains (or on the target domain) in the\\nsame vein as (Glorot et al., 2011; S. Chopra & Gopalan,\\n2013), effectively using (Glorot et al., 2011; S. Chopra &\\nGopalan, 2013) as an initialization to our method.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nAppendix A. An alternative optimization\\napproach\\nThere exists an alternative construction (inspired by (Good-\\nfellow et al., 2014)) that leads to the same updates (4)-(6).\\nRather than using the gradient reversal layer, the construc-\\ntion introduces two different loss functions for the domain\\nclassiﬁer. Minimization of the ﬁrst domain loss ( Ld+)\\nshould lead to a better domain discrimination, while the\\nsecond domain loss (Ld−) is minimized when the domains\\nare distinct. Stochastic updates for θf and θd are then de-\\nﬁned as:\\nθf ←− θf −µ\\n(\\n∂Li\\ny\\n∂θf\\n+ ∂Li\\nd−\\n∂θf\\n)\\nθd ←− θd −µ∂Li\\nd+\\n∂θd\\n,\\nThus, different parameters participate in the optimization\\nof different losses\\nIn this framework, the gradient reversal layer constitutes\\na special case, corresponding to the pair of domain losses\\n(Ld,−λLd). However, other pairs of loss functions can be\\nused. One example would be the binomial cross-entropy\\n(Goodfellow et al., 2014):\\nLd+(q,d) =\\n∑\\ni=1..N'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='(Ld,−λLd). However, other pairs of loss functions can be\\nused. One example would be the binomial cross-entropy\\n(Goodfellow et al., 2014):\\nLd+(q,d) =\\n∑\\ni=1..N\\ndilog(qi) + (1−di) log(1−qi) ,\\nwhere dindicates domain indices and qis an output of the\\npredictor. In that case “adversarial” loss is easily obtained\\nby swapping domain labels, i.e.Ld−(q,d) = Ld+(q,1−d).\\nThis particular pair has a potential advantage of produc-\\ning stronger gradients at early learning stages if the do-\\nmains are quite dissimilar. In our experiments, however,\\nwe did not observe any signiﬁcant improvement resulting\\nfrom this choice of losses.\\nAppendix B. CNN architectures\\nFour different architectures were used in our experiments\\n(ﬁrst three are shown in Figure 5):\\n• A smaller one (a) if the source domain is MNIST. This\\narchitecture was inspired by the classical LeNet-5 (Le-\\nCun et al., 1998).\\n• (b) for the experiments involving SVHN dataset. This\\none is adopted from (Srivastava et al., 2014).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='architecture was inspired by the classical LeNet-5 (Le-\\nCun et al., 1998).\\n• (b) for the experiments involving SVHN dataset. This\\none is adopted from (Srivastava et al., 2014).\\n• (c) in the S YN SINGS →GTSRB setting. We used\\nthe single-CNN baseline from (Cires ¸an et al., 2012)\\nas our starting point.\\n• Finally, we use pre-trained AlexNet from the\\nCaffe-package (Jia et al., 2014) for the O FFICE do-\\nmains. Adaptation architecture is identical to (Tzeng\\net al., 2014): 2-layer domain classiﬁer ( x→1024 →\\n1024 →2) is attached to the 256-dimensional bottle-\\nneck of fc7.\\nThe domain classiﬁer branch in all cases is somewhat ar-\\nbitrary (better adaptation performance might be attained if\\nthis part of the architecture is tuned).\\nAppendix C. Training procedure\\nWe use stochastic gradient descent with 0.9 momentum and\\nthe learning rate annealing described by the following for-\\nmula:\\nµp = µ0\\n(1 + α·p)β ,\\nwhere p is the training progress linearly changing from 0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='the learning rate annealing described by the following for-\\nmula:\\nµp = µ0\\n(1 + α·p)β ,\\nwhere p is the training progress linearly changing from 0\\nto 1, µ0 = 0 .01, α = 10 and β = 0 .75 (the schedule\\nwas optimized to promote convergence and low error on\\nthe source domain).\\nFollowing (Srivastava et al., 2014) we also use dropout and\\nℓ2-norm restriction when we train the SVHN architecture.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nconv 5x532 mapsReLU\\nmax-pool 2x22x2 stride\\nconv 5x548 mapsReLU\\nmax-pool 2x22x2 stride\\nfully-conn100 unitsReLU\\nfully-conn100 unitsReLU\\nfully-conn10 unitsSoft-max\\nGRL\\n fully-conn100 unitsReLU\\nfully-conn1 unitLogistic\\n(a) MNIST architecture\\nconv 5x564 mapsReLU\\nmax-pool 3x32x2 stride\\nconv 5x564 mapsReLU\\nmax-pool 3x32x2 stride\\nconv 5x5128 mapsReLU\\nfully-conn3072 unitsReLU\\nfully-conn2048 unitsReLU\\nfully-conn10 unitsSoft-max\\nGRL\\n fully-conn1024 unitsReLU\\nfully-conn1024 unitsReLU\\nfully-conn1 unitLogistic\\n(b) SVHN architecture\\nconv 5x596 mapsReLU\\nmax-pool 2x22x2 stride\\nconv 3x3144 mapsReLU\\nmax-pool 2x22x2 stride\\nconv 5x5256 mapsReLU\\nmax-pool 2x22x2 stride\\nfully-conn512 unitsReLU\\nfully-conn10 unitsSoft-max\\nGRL\\n fully-conn1024 unitsReLU\\nfully-conn1024 unitsReLU\\nfully-conn1 unitLogistic\\n(c) GTSRB architecture\\nFigure 5.CNN architectures used in the experiments. Boxes correspond to transformations applied to the data. Color-coding is the same'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='fully-conn1 unitLogistic\\n(c) GTSRB architecture\\nFigure 5.CNN architectures used in the experiments. Boxes correspond to transformations applied to the data. Color-coding is the same\\nas in Figure 1.\\nReferences\\nArbelaez, Pablo, Maire, Michael, Fowlkes, Charless, and\\nMalik, Jitendra. Contour detection and hierarchical im-\\nage segmentation. PAMI, 33, 2011.\\nBabenko, Artem, Slesarev, Anton, Chigorin, Alexander,\\nand Lempitsky, Victor S. Neural codes for image re-\\ntrieval. In ECCV, pp. 584–599, 2014.\\nBaktashmotlagh, Mahsa, Harandi, Mehrtash Tafazzoli,\\nLovell, Brian C., and Salzmann, Mathieu. Unsupervised\\ndomain adaptation by domain invariant projection. In\\nICCV, pp. 769–776, 2013.\\nBen-David, Shai, Blitzer, John, Crammer, Koby, Kulesza,\\nAlex, Pereira, Fernando, and Vaughan, Jennifer Wort-\\nman. A theory of learning from different domains.\\nJMLR, 79, 2010.\\nBorgwardt, Karsten M., Gretton, Arthur, Rasch, Malte J.,\\nKriegel, Hans-Peter, Sch ¨olkopf, Bernhard, and Smola,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='man. A theory of learning from different domains.\\nJMLR, 79, 2010.\\nBorgwardt, Karsten M., Gretton, Arthur, Rasch, Malte J.,\\nKriegel, Hans-Peter, Sch ¨olkopf, Bernhard, and Smola,\\nAlexander J. Integrating structured biological data by\\nkernel maximum mean discrepancy. In ISMB, pp. 49–\\n57, 2006.\\nCires ¸an, Dan, Meier, Ueli, Masci, Jonathan, and Schmid-\\nhuber, J ¨urgen. Multi-column deep neural network for\\ntrafﬁc sign classiﬁcation. Neural Networks, (32):333–\\n338, 2012.\\nCortes, Corinna and Mohri, Mehryar. Domain adaptation\\nin regression. In Algorithmic Learning Theory, 2011.\\nDonahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman,\\nJudy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. De-\\ncaf: A deep convolutional activation feature for generic\\nvisual recognition, 2014.\\nFan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,\\nXiang-Rui, and Lin, Chih-Jen. LIBLINEAR: A library\\nfor large linear classiﬁcation. Journal of Machine Learn-\\ning Research, 9:1871–1874, 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,\\nXiang-Rui, and Lin, Chih-Jen. LIBLINEAR: A library\\nfor large linear classiﬁcation. Journal of Machine Learn-\\ning Research, 9:1871–1874, 2008.\\nFernando, Basura, Habrard, Amaury, Sebban, Marc, and\\nTuytelaars, Tinne. Unsupervised visual domain adapta-\\ntion using subspace alignment. In ICCV, 2013.\\nGhifary, Muhammad, Kleijn, W Bastiaan, and Zhang,\\nMengjie. Domain adaptive neural networks for object\\nrecognition. In PRICAI 2014: Trends in Artiﬁcial Intel-\\nligence. 2014.\\nGlorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Do-\\nmain adaptation for large-scale sentiment classiﬁcation:\\nA deep learning approach. In ICML, pp. 513–520, 2011.\\nGong, Boqing, Shi, Yuan, Sha, Fei, and Grauman, Kristen.\\nGeodesic ﬂow kernel for unsupervised domain adapta-\\ntion. In CVPR, pp. 2066–2073, 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Unsupervised Domain Adaptation by Backpropagation\\nGong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\\nnecting the dots with landmarks: Discriminatively learn-\\ning domain-invariant features for unsupervised domain\\nadaptation. In ICML, pp. 222–230, 2013.\\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\\nBing, Warde-Farley, David, Ozair, Sherjil, Courville,\\nAaron, and Bengio, Yoshua. Generative adversarial nets.\\nIn NIPS, 2014.\\nGopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\\nDomain adaptation for object recognition: An unsuper-\\nvised approach. In ICCV, pp. 999–1006, 2011.\\nHoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\\nSaenko, Kate, and Darrell, Trevor. One-shot adapta-\\ntion of supervised deep convolutional models. CoRR,\\nabs/1312.6204, 2013.\\nHuang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\\nBorgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\\nrecting sample selection bias by unlabeled data. InNIPS,\\npp. 601–608, 2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Huang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\\nBorgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\\nrecting sample selection bias by unlabeled data. InNIPS,\\npp. 601–608, 2006.\\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev,\\nSergey, Long, Jonathan, Girshick, Ross, Guadar-\\nrama, Sergio, and Darrell, Trevor. Caffe: Convolu-\\ntional architecture for fast feature embedding. CoRR,\\nabs/1408.5093, 2014.\\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-\\nbased learning applied to document recognition. Pro-\\nceedings of the IEEE , 86(11):2278–2324, November\\n1998.\\nLiebelt, Joerg and Schmid, Cordelia. Multi-view object\\nclass detection with a 3d geometric model. In CVPR,\\n2010.\\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,\\nAlessandro, Wu, Bo, and Ng, Andrew Y . Reading dig-\\nits in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning 2011, 2011.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='its in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning 2011, 2011.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\\nand transferring mid-level image representations using\\nconvolutional neural networks. In CVPR, 2014.\\nPan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\\nYang, Qiang. Domain adaptation via transfer component\\nanalysis. IEEE Transactions on Neural Networks, 22(2):\\n199–210, 2011.\\nS. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\\nlearning for domain adaptation by interpolating between\\ndomains. In ICML Workshop on Challenges in Repre-\\nsentation Learning, 2013.\\nSaenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\\nTrevor. Adapting visual category models to new do-\\nmains. In ECCV, pp. 213–226. 2010.\\nShimodaira, Hidetoshi. Improving predictive inference un-\\nder covariate shift by weighting the log-likelihood func-\\ntion. Journal of Statistical Planning and Inference , 90'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Shimodaira, Hidetoshi. Improving predictive inference un-\\nder covariate shift by weighting the log-likelihood func-\\ntion. Journal of Statistical Planning and Inference , 90\\n(2):227–244, October 2000.\\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\\nA simple way to prevent neural networks from overﬁt-\\nting. The Journal of Machine Learning Research, 15(1):\\n1929–1958, 2014.\\nStark, Michael, Goesele, Michael, and Schiele, Bernt. Back\\nto the future: Learning shape models from 3d CAD data.\\nIn BMVC, pp. 1–11, 2010.\\nSun, Baochen and Saenko, Kate. From virtual to reality:\\nFast adaptation of virtual object detectors to real do-\\nmains. In BMVC, 2014.\\nTommasi, Tatiana and Caputo, Barbara. Frustratingly easy\\nnbnn domain adaptation. In ICCV, 2013.\\nTzeng, Eric, Hoffman, Judy, Zhang, Ning, Saenko, Kate,\\nand Darrell, Trevor. Deep domain confusion: Maximiz-\\ning for domain invariance. CoRR, abs/1412.3474, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-03-02T01:33:21+00:00', 'author': 'Yaroslav Ganin, Victor Lempitsky', 'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning', 'moddate': '2015-03-02T01:33:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': 'Unsupervised Domain Adaptation by Backpropagation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': '1409.7495v2.pdf', 'file_type': 'pdf'}, page_content='Tzeng, Eric, Hoffman, Judy, Zhang, Ning, Saenko, Kate,\\nand Darrell, Trevor. Deep domain confusion: Maximiz-\\ning for domain invariance. CoRR, abs/1412.3474, 2014.\\nvan der Maaten, Laurens. Barnes-hut-sne. CoRR,\\nabs/1301.3342, 2013.\\nV´azquez, David, L ´opez, Antonio Manuel, Mar ´ın, Javier,\\nPonsa, Daniel, and Gomez, David Ger ´onimo. Virtual\\nand real world adaptationfor pedestrian detection. IEEE\\nTrans. Pattern Anal. Mach. Intell., 36(4):797–809, 2014.\\nZeiler, Matthew D. and Fergus, Rob. Visualizing\\nand understanding convolutional networks. CoRR,\\nabs/1311.2901, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Simultaneous Deep Transfer Across Domains and Tasks\\nEric Tzeng∗, Judy Hoffman∗, Trevor Darrell\\nUC Berkeley, EECS & ICSI\\n{etzeng,jhoffman,trevor}@eecs.berkeley.edu\\nKate Saenko\\nUMass Lowell, CS\\nsaenko@cs.uml.edu\\nAbstract\\nRecent reports suggest that a generic supervised deep\\nCNN model trained on a large-scale dataset reduces, but\\ndoes not remove, dataset bias. Fine-tuning deep models in\\na new domain can require a signiﬁcant amount of labeled\\ndata, which for many applications is simply not available.\\nWe propose a new CNN architecture to exploit unlabeled and\\nsparsely labeled target domain data. Our approach simulta-\\nneously optimizes for domain invariance to facilitate domain\\ntransfer and uses a soft label distribution matching loss to\\ntransfer information between tasks. Our proposed adapta-\\ntion method offers empirical performance which exceeds\\npreviously published results on two standard benchmark vi-\\nsual domain adaptation tasks, evaluated across supervised'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='tion method offers empirical performance which exceeds\\npreviously published results on two standard benchmark vi-\\nsual domain adaptation tasks, evaluated across supervised\\nand semi-supervised adaptation settings.\\n1. Introduction\\nConsider a group of robots trained by the manufacturer\\nto recognize thousands of common objects using standard\\nimage databases, then shipped to households around the\\ncountry. As each robot starts to operate in its own unique\\nenvironment, it is likely to have degraded performance due\\nto the shift in domain. It is clear that, given enough ex-\\ntra supervised data from the new environment, the original\\nperformance could be recovered. However, state-of-the-art\\nrecognition algorithms rely on high capacity convolutional\\nneural network (CNN) models that require millions of su-\\npervised images for initial training. Even the traditional\\napproach for adapting deep models, ﬁne-tuning [ 14, 29],\\nmay require hundreds or thousands of labeled examples for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='pervised images for initial training. Even the traditional\\napproach for adapting deep models, ﬁne-tuning [ 14, 29],\\nmay require hundreds or thousands of labeled examples for\\neach object category that needs to be adapted.\\nIt is reasonable to assume that the robot’s new owner\\nwill label a handful of examples for a few types of objects,\\nbut completely unrealistic to presume full supervision in\\nthe new environment. Therefore, we propose an algorithm\\nthat effectively adapts between the training (source) and test\\n(target) environments by utilizing both generic statistics from\\n∗ Authors contributed equally.\\n!\\n!\\nBottleMugChairLaptopKeyboard\\nBottleMugChairLaptopKeyboardBottleMugChairLaptopKeyboard\\nBottleMugChairLaptopKeyboard\\n!\\n!\\nSource domain! Target domain! \\n!\\n!\\n !\\n!\\n!\\n\\x01!\\n!\\n\\x01!\\n!\\n\\x01!\\n!\\n\\x01!\\n1. Maximize domain confusion!\\n2. Transfer task correlation!\\n!\\n!\\nFigure 1. We transfer discriminative category information from\\na source domain to a target domain via two methods. First, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='!\\n\\x01!\\n!\\n\\x01!\\n1. Maximize domain confusion!\\n2. Transfer task correlation!\\n!\\n!\\nFigure 1. We transfer discriminative category information from\\na source domain to a target domain via two methods. First, we\\nmaximize domain confusion by making the marginal distributions\\nof the two domains as similar as possible. Second, we transfer cor-\\nrelations between classes learned on the source examples directly to\\nthe target examples, thereby preserving the relationships between\\nclasses.\\nunlabeled data collected in the new environment as well as a\\nfew human labeled examples from a subset of the categories\\nof interest. Our approach performs transfer learning both\\nacross domains and across tasks (see Figure 1). Intuitively,\\ndomain transfer is accomplished by making the marginal\\nfeature distributions of source and target as similar to each\\nother as possible. Task transfer is enabled by transferring\\nempirical category correlations learned on the source to the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='feature distributions of source and target as similar to each\\nother as possible. Task transfer is enabled by transferring\\nempirical category correlations learned on the source to the\\ntarget domain. This helps to preserve relationships between\\ncategories, e.g., bottle is similar to mug but different from\\nkeyboard. Previous work proposed techniques for domain\\ntransfer with CNN models [ 12, 24] but did not utilize the\\nlearned source semantic structure for task transfer.\\nTo enable domain transfer, we use the unlabeled target\\ndata to compute an estimated marginal distribution over the\\nnew environment and explicitly optimize a feature repre-\\n1\\narXiv:1510.02192v1  [cs.CV]  8 Oct 2015'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='sentation that minimizes the distance between the source\\nand target domain distributions. Dataset bias was classi-\\ncally illustrated in computer vision by the “name the dataset”\\ngame of Torralba and Efros [31], which trained a classiﬁer\\nto predict which dataset an image originates from, thereby\\nshowing that visual datasets are biased samples of the visual\\nworld. Indeed, this turns out to be formally connected to\\nmeasures of domain discrepancy [ 21, 5]. Optimizing for\\ndomain invariance, therefore, can be considered equivalent\\nto the task of learning to predict the class labels while simul-\\ntaneously ﬁnding a representation that makes the domains\\nappear as similar as possible. This principle forms the do-\\nmain transfer component of our proposed approach. We\\nlearn deep representations by optimizing over a loss which\\nincludes both classiﬁcation error on the labeled data as well\\nas a domain confusion loss which seeks to make the domains\\nindistinguishable.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='learn deep representations by optimizing over a loss which\\nincludes both classiﬁcation error on the labeled data as well\\nas a domain confusion loss which seeks to make the domains\\nindistinguishable.\\nHowever, while maximizing domain confusion pulls the\\nmarginal distributions of the domains together, it does not\\nnecessarily align the classes in the target with those in the\\nsource. Thus, we also explicitly transfer the similarity struc-\\nture amongst categories from the source to the target and\\nfurther optimize our representation to produce the same struc-\\nture in the target domain using the few target labeled exam-\\nples as reference points. We are inspired by prior work on\\ndistilling deep models [3, 16] and extend the ideas presented\\nin these works to a domain adaptation setting. We ﬁrst com-\\npute the average output probability distribution, or “soft\\nlabel,” over the source training examples in each category.\\nThen, for each target labeled example, we directly optimize'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='pute the average output probability distribution, or “soft\\nlabel,” over the source training examples in each category.\\nThen, for each target labeled example, we directly optimize\\nour model to match the distribution over classes to the soft\\nlabel. In this way we are able to perform task adaptation by\\ntransferring information to categories with no explicit labels\\nin the target domain.\\nWe solve the two problems jointly using a new CNN ar-\\nchitecture, outlined in Figure 2. We combine a domain con-\\nfusion and softmax cross-entropy losses to train the network\\nwith the target data. Our architecture can be used to solve su-\\npervised adaptation, when a small amount of target labeled\\ndata is available from each category, and semi-supervised\\nadaptation, when a small amount of target labeled data is\\navailable from a subset of the categories. We provide a com-\\nprehensive evaluation on the popular Ofﬁce benchmark [28]\\nand the recently introduced cross-dataset collection [30] for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='available from a subset of the categories. We provide a com-\\nprehensive evaluation on the popular Ofﬁce benchmark [28]\\nand the recently introduced cross-dataset collection [30] for\\nclassiﬁcation across visually distinct domains. We demon-\\nstrate that by jointly optimizing for domain confusion and\\nmatching soft labels, we are able to outperform the current\\nstate-of-the-art visual domain adaptation results.\\n2. Related work\\nThere have been many approaches proposed in recent\\nyears to solve the visual domain adaptation problem, which\\nis also commonly framed as the visual dataset bias prob-\\nlem [31]. All recognize that there is a shift in the distri-\\nbution of the source and target data representations. In\\nfact, the size of a domain shift is often measured by the\\ndistance between the source and target subspace representa-\\ntions [5, 11, 21, 25, 27]. A large number of methods have\\nsought to overcome this difference by learning a feature'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='distance between the source and target subspace representa-\\ntions [5, 11, 21, 25, 27]. A large number of methods have\\nsought to overcome this difference by learning a feature\\nspace transformation to align the source and target repre-\\nsentations [28, 23, 11, 15]. For the supervised adaptation\\nscenario, when a limited amount of labeled data is available\\nin the target domain, some approaches have been proposed\\nto learn a target classiﬁer regularized against the source clas-\\nsiﬁer [32, 2, 1]. Others have sought to both learn a feature\\ntransformation and regularize a target classiﬁer simultane-\\nously [18, 10].\\nRecently, supervised CNN based feature representations\\nhave been shown to be extremely effective for a variety of\\nvisual recognition tasks [22, 9, 14, 29]. In particular, using\\ndeep representations dramatically reduces the effect of reso-\\nlution and lighting on domain shifts [9, 19]. Parallel CNN\\narchitectures such as Siamese networks have been shown'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='deep representations dramatically reduces the effect of reso-\\nlution and lighting on domain shifts [9, 19]. Parallel CNN\\narchitectures such as Siamese networks have been shown\\nto be effective for learning invariant representations [6, 8].\\nHowever, training these networks requires labels for each\\ntraining instance, so it is unclear how to extend these meth-\\nods to unsupervised or semi-supervised settings. Multimodal\\ndeep learning architectures have also been explored to learn\\nrepresentations that are invariant to different input modal-\\nities [26]. However, this method operated primarily in a\\ngenerative context and therefore did not leverage the full\\nrepresentational power of supervised CNN representations.\\nTraining a joint source and target CNN architecture was\\nproposed by [7], but was limited to two layers and so was\\nsigniﬁcantly outperformed by the methods which used a\\ndeeper architecture [ 22], pre-trained on a large auxiliary'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='proposed by [7], but was limited to two layers and so was\\nsigniﬁcantly outperformed by the methods which used a\\ndeeper architecture [ 22], pre-trained on a large auxiliary\\ndata source (ex: ImageNet [4]). [13] proposed pre-training\\nwith a denoising auto-encoder, then training a two-layer net-\\nwork simultaneously with the MMD domain confusion loss.\\nThis effectively learns a domain invariant representation, but\\nagain, because the learned network is relatively shallow, it\\nlacks the strong semantic representation that is learned by di-\\nrectly optimizing a classiﬁcation objective with a supervised\\ndeep CNN.\\nUsing classiﬁer output distributions instead of category\\nlabels during training has been explored in the context of\\nmodel compression or distillation [3, 16]. However, we are\\nthe ﬁrst to apply this technique in a domain adaptation setting\\nin order to transfer class correlations between domains.\\nOther works have cotemporaneously explored the idea'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='the ﬁrst to apply this technique in a domain adaptation setting\\nin order to transfer class correlations between domains.\\nOther works have cotemporaneously explored the idea\\nof directly optimizing a representation for domain invari-\\nance [12, 24]. However, they either use weaker measures\\nof domain invariance or make use of optimization methods\\nthat are less robust than our proposed method, and they do\\nnot attempt to solve the task transfer problem in the semi-\\nsupervised setting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Source Data\\nbackpack chair\\n bike\\nTarget Databackpack\\n?\\nfc8conv1 conv5 fc6 fc7\\nSource softlabels\\nall target data\\nsource data\\nlabeled target data\\nfc8conv1 conv5\\nsource data\\nsoftmax \\nhigh temp\\nsoftlabel \\nloss\\nfcD\\nfc6 fc7\\nclassiﬁcation \\nloss\\ndomain \\nconfusion \\nloss\\ndomain \\nclassiﬁer \\nloss\\nshared\\nshared\\nshared\\nshared\\nshared\\nFigure 2. Our overall CNN architecture for domain and task transfer. We use a domain confusion loss over all source and target (both labeled\\nand unlabeled) data to learn a domain invariant representation. We simultaneously transfer the learned source semantic structure to the target\\ndomain by optimizing the network to produce activation distributions that match those learned for source data in the source only CNN. Best\\nviewed in color.\\n3. Joint CNN architecture for domain and task\\ntransfer\\nWe ﬁrst give an overview of our convolutional network\\n(CNN) architecture, depicted in Figure 2, that learns a rep-\\nresentation which both aligns visual domains and transfers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='transfer\\nWe ﬁrst give an overview of our convolutional network\\n(CNN) architecture, depicted in Figure 2, that learns a rep-\\nresentation which both aligns visual domains and transfers\\nthe semantic structure from a well labeled source domain to\\nthe sparsely labeled target domain. We assume access to a\\nlimited amount of labeled target data, potentially from only\\na subset of the categories of interest. With limited labels on\\na subset of the categories, the traditional domain transfer ap-\\nproach of ﬁne-tuning on the available target data [14, 29, 17]\\nis not effective. Instead, since the source labeled data shares\\nthe label space of our target domain, we use the source data\\nto guide training of the corresponding classiﬁers.\\nOur method takes as input the labeled source data\\n{xS,yS}(blue box Figure 2) and the target data {xT,yT}\\n(green box Figure 2), where the labels yT are only provided\\nfor a subset of the target examples. Our goal is to produce'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='{xS,yS}(blue box Figure 2) and the target data {xT,yT}\\n(green box Figure 2), where the labels yT are only provided\\nfor a subset of the target examples. Our goal is to produce\\na category classiﬁer θC that operates on an image feature\\nrepresentation f(x; θrepr) parameterized by representation\\nparameters θrepr and can correctly classify target examples\\nat test time.\\nFor a setting with Kcategories, let our desired classiﬁca-\\ntion objective be deﬁned as the standard softmax loss\\nLC(x,y; θrepr,θC) =−\\n∑\\nk\\n1 [y= k] logpk (1)\\nwhere p is the softmax of the classiﬁer activations,\\np= softmax(θT\\nCf(x; θrepr)).\\nWe could use the available source labeled data to train\\nour representation and classiﬁer parameters according to\\nEquation (1), but this often leads to overﬁtting to the source\\ndistribution, causing reduced performance at test time when\\nrecognizing in the target domain. However, we note that\\nif the source and target domains are very similar then the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='distribution, causing reduced performance at test time when\\nrecognizing in the target domain. However, we note that\\nif the source and target domains are very similar then the\\nclassiﬁer trained on the source will perform well on the\\ntarget. In fact, it is sufﬁcient for the source and target data to\\nbe similar under the learned representation, θrepr.\\nInspired by the “name the dataset” game of Torralba\\nand Efros [ 31], we can directly train a domain classiﬁer\\nθD to identify whether a training example originates from\\nthe source or target domain given its feature representation.\\nIntuitively, if our choice of representation suffers from do-\\nmain shift, then they will lie in distinct parts of the feature\\nspace, and a classiﬁer will be able to easily separate the\\ndomains. We use this notion to add a new domain confusion\\nloss Lconf(xS,xT,θD; θrepr) to our objective and directly op-\\ntimize our representation so as to minimize the discrepancy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='domains. We use this notion to add a new domain confusion\\nloss Lconf(xS,xT,θD; θrepr) to our objective and directly op-\\ntimize our representation so as to minimize the discrepancy\\nbetween the source and target distributions. This loss is\\ndescribed in more detail in Section 3.1.\\nDomain confusion can be applied to learn a representation\\nthat aligns source and target data without any target labeled\\ndata. However, we also presume a handful of sparse labels\\nin the target domain, yT. In this setting, a simple approach is\\nto incorporate the target labeled data along with the source\\nlabeled data into the classiﬁcation objective of Equation (1)1.\\nHowever, ﬁne-tuning with hard category labels limits the\\nimpact of a single training example, making it hard for the\\nnetwork to learn to generalize from the limited labeled data.\\nAdditionally, ﬁne-tuning with hard labels is ineffective when\\nlabeled data is available for only a subset of the categories.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='network to learn to generalize from the limited labeled data.\\nAdditionally, ﬁne-tuning with hard labels is ineffective when\\nlabeled data is available for only a subset of the categories.\\nFor our approach, we draw inspiration from recent net-\\nwork distillation works [ 3, 16], which demonstrate that a\\nlarge network can be “distilled” into a simpler model by re-\\nplacing the hard labels with the softmax activations from the\\noriginal large model. This modiﬁcation proves to be critical,\\nas the distribution holds key information about the relation-\\n1We present this approach as one of our baselines.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ships between categories and imposes additional structure\\nduring the training process. In essence, because each train-\\ning example is paired with an output distribution, it provides\\nvaluable information about not only the category it belongs\\nto, but also each other category the classiﬁer is trained to\\nrecognize.\\nThus, we propose using the labeled target data to op-\\ntimize the network parameters through a soft label loss,\\nLsoft(xT,yT; θrepr,θC). This loss will train the network pa-\\nrameters to produce a “soft label” activation that matches\\nthe average output distribution of source examples on a net-\\nwork trained to classify source data. This loss is described in\\nmore detail in Section 3.2. By training the network to match\\nthe expected source output distributions on target data, we\\ntransfer the learned inter-class correlations from the source\\ndomain to examples in the target domain. This directly trans-\\nfers useful information from source to target, such as the fact'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='transfer the learned inter-class correlations from the source\\ndomain to examples in the target domain. This directly trans-\\nfers useful information from source to target, such as the fact\\nthat bookshelves appear more similar to ﬁling cabinets than\\nto bicycles.\\nOur full method then minimizes the joint loss function\\nL(xS,yS,xT,yT,θD;θrepr,θC) =\\nLC(xS,yS,xT,yT; θrepr,θC)\\n+ λLconf(xS,xT,θD; θrepr)\\n+ νLsoft(xT,yT; θrepr,θC).\\n(2)\\nwhere the hyperparameters λand νdetermine how strongly\\ndomain confusion and soft labels inﬂuence the optimization.\\nOur ideas of domain confusion and soft label loss for task\\ntransfer are generic and can be applied to any CNN classiﬁ-\\ncation architecture. For our experiments and for the detailed\\ndiscussion in this paper we modify the standard Krizhevsky\\narchitecture [22], which has ﬁve convolutional layers (conv1–\\nconv5) and three fully connected layers (fc6–fc8). The rep-\\nresentation parameter θrepr corresponds to layers 1–7 of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='architecture [22], which has ﬁve convolutional layers (conv1–\\nconv5) and three fully connected layers (fc6–fc8). The rep-\\nresentation parameter θrepr corresponds to layers 1–7 of the\\nnetwork, and the classiﬁcation parameter θC corresponds to\\nlayer 8. For the remainder of this section, we provide further\\ndetails on our novel loss deﬁnitions and the implementation\\nof our model.\\n3.1. Aligning domains via domain confusion\\nIn this section we describe in detail our proposed domain\\nconfusion loss objective. Recall that we introduce the domain\\nconfusion loss as a means to learn a representation that is\\ndomain invariant, and thus will allow us to better utilize a\\nclassiﬁer trained using the labeled source data. We consider\\na representation to be domain invariant if a classiﬁer trained\\nusing that representation can not distinguish examples from\\nthe two domains.\\nTo this end, we add an additional domain classiﬁcation\\nlayer, denoted as fcD in Figure 2, with parameters θD. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='using that representation can not distinguish examples from\\nthe two domains.\\nTo this end, we add an additional domain classiﬁcation\\nlayer, denoted as fcD in Figure 2, with parameters θD. This\\nlayer simply performs binary classiﬁcation using the domain\\ncorresponding to an image as its label. For a particular fea-\\nture representation, θrepr, we evaluate its domain invariance\\nby learning the best domain classiﬁer on the representation.\\nThis can be learned by optimizing the following objective,\\nwhere yD denotes the domain that the example is drawn\\nfrom:\\nLD(xS,xT,θrepr; θD) =−\\n∑\\nd\\n1 [yD = d] logqd (3)\\nwith qcorresponding to the softmax of the domain classiﬁer\\nactivation: q= softmax(θT\\nDf(x; θrepr)).\\nFor a particular domain classiﬁer, θD, we can now in-\\ntroduce our loss which seeks to “maximally confuse” the\\ntwo domains by computing the cross entropy between the\\noutput predicted domain labels and a uniform distribution\\nover domain labels:\\nLconf(xS,xT,θD; θrepr) =−\\n∑\\nd\\n1\\nDlog qd. (4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='two domains by computing the cross entropy between the\\noutput predicted domain labels and a uniform distribution\\nover domain labels:\\nLconf(xS,xT,θD; θrepr) =−\\n∑\\nd\\n1\\nDlog qd. (4)\\nThis domain confusion loss seeks to learn domain invari-\\nance by ﬁnding a representation in which the best domain\\nclassiﬁer performs poorly.\\nIdeally, we want to simultaneously minimize Equa-\\ntions (3) and (4) for the representation and the domain clas-\\nsiﬁer parameters. However, the two losses stand in direct\\nopposition to one another: learning a fully domain invariant\\nrepresentation means the domain classiﬁer must do poorly,\\nand learning an effective domain classiﬁer means that the\\nrepresentation is not domain invariant. Rather than globally\\noptimizing θD and θrepr, we instead perform iterative updates\\nfor the following two objectives given the ﬁxed parameters\\nfrom the previous iteration:\\nmin\\nθD\\nLD(xS,xT,θrepr; θD) (5)\\nmin\\nθrepr\\nLconf(xS,xT,θD; θrepr). (6)\\nThese losses are readily implemented in standard deep'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='from the previous iteration:\\nmin\\nθD\\nLD(xS,xT,θrepr; θD) (5)\\nmin\\nθrepr\\nLconf(xS,xT,θD; θrepr). (6)\\nThese losses are readily implemented in standard deep\\nlearning frameworks, and after setting learning rates properly\\nso that Equation (5) only updates θD and Equation (6) only\\nupdates θrepr, the updates can be performed via standard\\nbackpropagation. Together, these updates ensure that we\\nlearn a representation that is domain invariant.\\n3.2. Aligning source and target classes via soft labels\\nWhile training the network to confuse the domains acts\\nto align their marginal distributions, there are no guarantees\\nabout the alignment of classes between each domain. To\\nensure that the relationships between classes are preserved\\nacross source and target, we ﬁne-tune the network against\\n“soft labels” rather than the image category hard label.\\nWe deﬁne a soft label for category kas the average over\\nthe softmax of all activations of source examples in category'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='“soft labels” rather than the image category hard label.\\nWe deﬁne a soft label for category kas the average over\\nthe softmax of all activations of source examples in category\\nk, depicted graphically in Figure 3, and denote this aver-\\nage as l(k). Note that, since the source network was trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Source \\nCNN\\nSource \\nCNN\\nSource \\nCNN\\nBottleMugChairLaptopKeyboard\\nBottle Mug ChairLaptopKeyboard\\nBottle Mug ChairLaptopKeyboard\\nBottle Mug ChairLaptopKeyboard\\n+\\nsoftmax \\nhigh \\ntemp\\nsoftmax \\nhigh \\ntemp\\nsoftmax \\nhigh \\ntemp\\nFigure 3. Soft label distributions are learned by averaging the per-\\ncategory activations of source training examples using the source\\nmodel. An example, with 5 categories, depicted here to demonstrate\\nthe ﬁnal soft activation for the bottle category will be primarily\\ndominated by bottle and mug with very little mass on chair, laptop,\\nand keyboard.\\nBottleMug ChairLaptopKeyboard\\nBottleMug ChairLaptopKeyboard\\nAdapt CNN\\n“Bottle”\\nSource Activations \\nPer Class\\nbackprop\\nCross Entropy Loss\\nsoftmax \\nhigh \\ntemp\\nFigure 4. Depiction of the use of source per-category soft activa-\\ntions with the cross entropy loss function over the current target\\nactivations.\\npurely to optimize a classiﬁcation objective, a simple soft-\\nmax over each zi\\nS will hide much of the useful information'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='activations.\\npurely to optimize a classiﬁcation objective, a simple soft-\\nmax over each zi\\nS will hide much of the useful information\\nby producing a very peaked distribution. Instead, we use a\\nsoftmax with a high temperature τ so that the related classes\\nhave enough probability mass to have an effect during ﬁne-\\ntuning. With our computed per-category soft labels we can\\nnow deﬁne our soft label loss:\\nLsoft(xT,yT; θrepr,θC) =−\\n∑\\ni\\nl(yT )\\ni log pi (7)\\nwhere p denotes the soft activation of the target image,\\np = softmax(θT\\nCf(xT; θrepr)/τ). The loss above corre-\\nsponds to the cross-entropy loss between the soft activation\\nof a particular target image and the soft label corresponding\\nto the category of that image, as shown in Figure 4.\\nTo see why this will help, consider the soft label for a\\nparticular category, such as bottle. The soft label l(bottle) is\\na K-dimensional vector, where each dimension indicates\\nthe similarity of bottles to each of the Kcategories. In this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='particular category, such as bottle. The soft label l(bottle) is\\na K-dimensional vector, where each dimension indicates\\nthe similarity of bottles to each of the Kcategories. In this\\nexample, the bottle soft label will have a higher weight on\\nmug than on keyboard, since bottles and mugs are more\\nvisually similar. Thus, soft label training with this particular\\nsoft label directly enforces the relationship that bottles and\\nmugs should be closer in feature space than bottles and\\nkeyboards.\\nOne important beneﬁt of using this soft label loss is that\\nwe ensure that the parameters for categories without any\\nlabeled target data are still updated to output non-zero proba-\\nbilities. We explore this beneﬁt in Section 4, where we train\\na network using labels from a subset of the target categories\\nand ﬁnd signiﬁcant performance improvement even when\\nevaluating only on the unlabeled categories.\\n4. Evaluation\\nTo analyze the effectiveness of our method, we evaluate it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='and ﬁnd signiﬁcant performance improvement even when\\nevaluating only on the unlabeled categories.\\n4. Evaluation\\nTo analyze the effectiveness of our method, we evaluate it\\non the Ofﬁce dataset, a standard benchmark dataset for visual\\ndomain adaptation, and on a new large-scale cross-dataset\\ndomain adaptation challenge.\\n4.1. Adaptation on the Ofﬁce dataset\\nThe Ofﬁce dataset is a collection of images from three\\ndistinct domains, Amazon, DSLR, and Webcam, the largest\\nof which has 2817 labeled images [28]. The 31 categories\\nin the dataset consist of objects commonly encountered in\\nofﬁce settings, such as keyboards, ﬁle cabinets, and laptops.\\nWe evaluate our method in two different settings:\\n•Supervised adaptation Labeled training data for all\\ncategories is available in source and sparsely in target.\\n•Semi-supervised adaptation (task adaptation) La-\\nbeled training data is available in source and sparsely\\nfor a subset of the target categories.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='•Semi-supervised adaptation (task adaptation) La-\\nbeled training data is available in source and sparsely\\nfor a subset of the target categories.\\nFor all experiments we initialize the parameters of conv1–\\nfc7 using the released CaffeNet [20] weights. We then fur-\\nther ﬁne-tune the network using the source labeled data in or-\\nder to produce the soft label distributions and use the learned\\nsource CNN weights as the initial parameters for training\\nour method. All implementations are produced using the\\nopen source Caffe [20] framework, and the network deﬁni-\\ntion ﬁles and cross entropy loss layer needed for training\\nwill be released upon acceptance. We optimize the network\\nusing a learning rate of 0.001 and set the hyper-parameters\\nto λ= 0.01 (confusion) and ν = 0.1 (soft).\\nFor each of the six domain shifts, we evaluate across ﬁve\\ntrain/test splits, which are generated by sampling examples\\nfrom the full set of images per domain. In the source domain,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='For each of the six domain shifts, we evaluate across ﬁve\\ntrain/test splits, which are generated by sampling examples\\nfrom the full set of images per domain. In the source domain,\\nwe follow the standard protocol for this dataset and generate\\nsplits by sampling 20 examples per category for the Amazon\\ndomain, and 8 examples per category for the DSLR and\\nWebcam domains.\\nWe ﬁrst present results for the supervised setting, where\\n3 labeled examples are provided for each category in the\\ntarget domain. We report accuracies on the remaining un-\\nlabeled images, following the standard protocol introduced\\nwith the dataset [28]. In addition to a variety of baselines, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='A→W A →D W →A W →D D →A D →W Average\\nDLID [7] 51.9 – – 89.9 – 78.2 –\\nDeCAF6 S+T [9] 80.7 ±2.3 – – – – 94.8 ±1.2 –\\nDaNN [13] 53.6 ±0.2 – – 83.5 ±0.0 – 71.2 ±0.0 –\\nSource CNN 56.5 ±0.3 64.6 ±0.4 42.7 ±0.1 93.6 ±0.2 47.6 ±0.1 92.4 ±0.3 66.22\\nTarget CNN 80.5 ±0.5 81.8 ±1.0 59.9 ±0.3 81.8 ±1.0 59.9 ±0.3 80.5 ±0.5 74.05\\nSource+Target CNN 82.5 ±0.9 85.2 ±1.1 65.2 ±0.7 96.3 ±0.5 65.8 ±0.5 93.9 ±0.5 81.50\\nOurs: dom confusion only 82.8 ±0.9 85.9 ±1.1 64.9 ±0.5 97.5 ±0.2 66.2 ±0.4 95.6 ±0.4 82.13\\nOurs: soft labels only 82.7 ±0.7 84.9 ±1.2 65.2 ±0.6 98.3 ±0.3 66.0 ±0.5 95.9 ±0.6 82.17\\nOurs: dom confusion+soft labels 82.7 ±0.8 86.1 ±1.2 65.0 ±0.5 97.6 ±0.2 66.2 ±0.3 95.7 ±0.5 82.22\\nTable 1. Multi-class accuracy evaluation on the standard supervised adaptation setting with theOfﬁce dataset. We evaluate on all 31 categories\\nusing the standard experimental protocol from [28]. Here, we compare against three state-of-the-art domain adaptation methods as well as a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='using the standard experimental protocol from [28]. Here, we compare against three state-of-the-art domain adaptation methods as well as a\\nCNN trained using only source data, only target data, or both source and target data together.\\nA→W A →D W →A W →D D →A D →W Average\\nMMDT [18] – 44.6 ±0.3 – 58.3 ±0.5 – – –\\nSource CNN 54.2 ±0.6 63.2 ±0.4 34.7 ±0.1 94.5 ±0.2 36.4 ±0.1 89.3 ±0.5 62.0\\nOurs: dom confusion only 55.2 ±0.6 63.7 ±0.9 41.1 ±0.0 96.5 ±0.1 41.2 ±0.1 91.3 ±0.4 64.8\\nOurs: soft labels only 56.8 ±0.4 65.2 ±0.9 38.8 ±0.4 96.5 ±0.2 41.7 ±0.3 89.6 ±0.1 64.8\\nOurs: dom confusion+soft labels 59.3 ±0.6 68.0 ±0.5 40.5 ±0.2 97.5 ±0.1 43.1 ±0.2 90.0 ±0.2 66.4\\nTable 2. Multi-class accuracy evaluation on the standard semi-supervised adaptation setting with the Ofﬁce dataset. We evaluate on 16\\nheld-out categories for which we have no access to target labeled data. We show results on these unsupervised categories for the source only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='held-out categories for which we have no access to target labeled data. We show results on these unsupervised categories for the source only\\nmodel, our model trained using only soft labels for the 15 auxiliary categories, and ﬁnally using domain confusion together with soft labels\\non the 15 auxiliary categories.\\nreport numbers for both soft label ﬁne-tuning alone as well\\nas soft labels with domain confusion in Table 1. Because the\\nOfﬁce dataset is imbalanced, we report multi-class accura-\\ncies, which are obtained by computing per-class accuracies\\nindependently, then averaging over all 31 categories.\\nWe see that ﬁne-tuning with soft labels or domain con-\\nfusion provides a consistent improvement over hard label\\ntraining in 5 of 6 shifts. Combining soft labels with do-\\nmain confusion produces marginally higher performance on\\naverage. This result follows the intuitive notion that when\\nenough target labeled examples are present, directly opti-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='main confusion produces marginally higher performance on\\naverage. This result follows the intuitive notion that when\\nenough target labeled examples are present, directly opti-\\nmizing for the joint source and target classiﬁcation objective\\n(Source+Target CNN) is a strong baseline and so using ei-\\nther of our new losses adds enough regularization to improve\\nperformance.\\nNext, we experiment with the semi-supervised adaptation\\nsetting. We consider the case in which training data and\\nlabels are available for some, but not all of the categories in\\nthe target domain. We are interested in seeing whether we\\ncan transfer information learned from the labeled classes to\\nthe unlabeled classes.\\nTo do this, we consider having 10 target labeled exam-\\nples per category from only 15 of the 31 total categories,\\nfollowing the standard protocol introduced with the Ofﬁce\\ndataset [28]. We then evaluate our classiﬁcation performance\\non the remaining 16 categories for which no data was avail-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='following the standard protocol introduced with the Ofﬁce\\ndataset [28]. We then evaluate our classiﬁcation performance\\non the remaining 16 categories for which no data was avail-\\nable at training time.\\nIn Table 2 we present multi-class accuracies over the 16\\nheld-out categories and compare our method to a previous\\ndomain adaptation method [ 18] as well as a source-only\\ntrained CNN. Note that, since the performance here is com-\\nputed over only a subset of the categories in the dataset, the\\nnumbers in this table should not be directly compared to the\\nsupervised setting in Table 1.\\nWe ﬁnd that all variations of our method (only soft label\\nloss, only domain confusion, and both together) outperform\\nthe baselines. Contrary to the fully supervised case, here we\\nnote that both domain confusion and soft labels contribute\\nsigniﬁcantly to the overall performance improvement of our\\nmethod. This stems from the fact that we are now evaluat-\\ning on categories which lack labeled target data, and thus'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='signiﬁcantly to the overall performance improvement of our\\nmethod. This stems from the fact that we are now evaluat-\\ning on categories which lack labeled target data, and thus\\nthe network can not implicitly enforce domain invariance\\nthrough the classiﬁcation objective alone. Separately, the\\nfact that we get improvement from the soft label training on\\nrelated tasks indicates that information is being effectively\\ntransferred between tasks.\\nIn Figure 5, we show examples for the\\nAmazon→Webcam shift where our method correctly\\nclassiﬁes images from held out object categories and the\\nbaseline does not. We ﬁnd that our method is able to\\nconsistently overcome error cases, such as the notebooks\\nthat were previously confused with letter trays, or the black'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ring binder\\nmonitor\\nlaptop computer\\nmonitor\\nspeaker\\nmonitor\\nscissors\\nmug\\nmouse\\nmug\\nmouse\\nmug\\nlaptop computer\\npaper notebook\\nletter tray\\npaper notebook\\nletter tray\\npaper notebook\\nletter tray\\npaper notebook\\nletter tray\\npaper notebook\\nlaptop computer\\npaper notebook\\ncalculator\\nphone\\ncalculator\\nphone\\nfile cabinet\\nprinter\\nfile cabinet\\nprinter\\nfile cabinet\\nprinter\\nlaptop computer\\nprojector\\nlaptop computer\\nprojector\\nfile cabinet\\nprojector\\nphone\\nprojector\\nkeyboard\\nprojector\\ntape dispenser\\npunchers\\nlaptop computer\\nring binder\\nkeyboard\\nring binder\\nkeyboard\\nring binder\\nletter tray\\nring binder\\nlaptop computer\\nring binder\\nFigure 5. Examples from the Amazon →Webcam shift in the\\nsemi-supervised adaptation setting, where our method (the bot-\\ntom turquoise label) correctly classiﬁes images while the baseline\\n(the top purple label) does not.\\nmugs that were confused with black computer mice.\\n4.2. Adaptation between diverse domains\\nFor an evaluation with larger, more distinct domains, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='(the top purple label) does not.\\nmugs that were confused with black computer mice.\\n4.2. Adaptation between diverse domains\\nFor an evaluation with larger, more distinct domains, we\\ntest on the recent testbed for cross-dataset analysis [ 30],\\nwhich collects images from classes shared in common among\\ncomputer vision datasets. We use the dense version of this\\ntestbed, which consists of 40 categories shared between\\nthe ImageNet, Caltech-256, SUN, and Bing datasets, and\\nevaluate speciﬁcally with ImageNet as source and Caltech-\\n256 as target.\\nWe follow the protocol outlined in [30] and generate 5\\nsplits by selecting 5534 images from ImageNet and 4366\\nimages from Caltech-256 across the 40 shared categories.\\nEach split is then equally divided into a train and test set.\\nHowever, since we are most interested in evaluating in the\\nsetting with limited target data, we further subsample the\\ntarget training set into smaller sets with only 1, 3, and 5\\nlabeled examples per category.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='setting with limited target data, we further subsample the\\ntarget training set into smaller sets with only 1, 3, and 5\\nlabeled examples per category.\\nResults from this evaluation are shown in Figure 6. We\\ncompare our method to both CNNs ﬁne-tuned using only\\nsource data using source and target labeled data. Contrary to\\nthe previous supervised adaptation experiment, our method\\nsigniﬁcantly outperforms both baselines. We see that our\\nfull architecture, combining domain confusion with the soft\\nlabel loss, performs the best overall and is able to operate\\nin the regime of no labeled examples in the target (corre-\\nsponding to the red line at point 0 on the x-axis). We ﬁnd\\nthat the most beneﬁt of our method arises when there are\\nfew labeled training examples per category in the target do-\\nmain. As we increase the number of labeled examples in\\nthe target, the standard ﬁne-tuning strategy begins to ap-\\nproach the performance of the adaptation approach. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='main. As we increase the number of labeled examples in\\nthe target, the standard ﬁne-tuning strategy begins to ap-\\nproach the performance of the adaptation approach. This\\nNumber Labeled Target Examples per Category\\n0 1 3 5\\nMulti-class Accuracy\\n72\\n73\\n74\\n75\\n76\\n77\\n78\\nSource CNN\\nSource+Target CNN\\nOurs: softlabels only\\nOurs: dom confusion+softlabels\\nFigure 6. ImageNet→Caltech supervised adaptation from the Cross-\\ndataset [30] testbed with varying numbers of labeled target exam-\\nples per category. We ﬁnd that our method using soft label loss\\n(with and without domain confusion) outperforms the baselines\\nof training on source data alone or using a standard ﬁne-tuning\\nstrategy to train with the source and target data. Best viewed in\\ncolor.\\nindicates that direct joint source and target ﬁne-tuning is\\na viable adaptation approach when you have a reasonable\\nnumber of training examples per category. In comparison,\\nﬁne-tuning on the target examples alone yields accuracies'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='a viable adaptation approach when you have a reasonable\\nnumber of training examples per category. In comparison,\\nﬁne-tuning on the target examples alone yields accuracies\\nof 36.6 ±0.6, 60.9 ±0.5, and 67.7 ±0.5 for the cases of 1,\\n3, and 5 labeled examples per category, respectively. All of\\nthese numbers underperform the source only model, indicat-\\ning that adaptation is crucial in the setting of limited training\\ndata.\\nFinally, we note that our results are signiﬁcantly higher\\nthan the 24.8% result reported in [ 30], despite the use of\\nmuch less training data. This difference is explained by their\\nuse of SURF BoW features, indicating that CNN features\\nare a much stronger feature for use in adaptation tasks.\\n5. Analysis\\nOur experimental results demonstrate that our method\\nimproves classiﬁcation performance in a variety of domain\\nadaptation settings. We now perform additional analysis on\\nour method by conﬁrming our claims that it exhibits domain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='improves classiﬁcation performance in a variety of domain\\nadaptation settings. We now perform additional analysis on\\nour method by conﬁrming our claims that it exhibits domain\\ninvariance and transfers information across tasks.\\n5.1. Domain confusion enforces domain invariance\\nWe begin by evaluating the effectiveness of domain con-\\nfusion at learning a domain invariant representation. As\\npreviously explained, we consider a representation to be\\ndomain invariant if an optimal classiﬁer has difﬁculty pre-\\ndicting which domain an image originates from. Thus, for\\nour representation learned with a domain confusion loss, we\\nexpect a trained domain classiﬁer to perform poorly.\\nWe train two support vector machines (SVMs) to clas-\\nsify images into domains: one using the baseline CaffeNet\\nfc7 representation, and the other using our fc7 learned with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='Figure 7. We compare the baseline CaffeNet representation to\\nour representation learned with domain confusion by training a\\nsupport vector machine to predict the domains of Amazon and\\nWebcam images. For each representation, we plot a histogram of\\nthe classiﬁer decision scores of the test images. In the baseline\\nrepresentation, the classiﬁer is able to separate the two domains\\nwith 99% accuracy. In contrast, the representation learned with\\ndomain confusion is domain invariant, and the classiﬁer can do no\\nbetter than 56%.\\ndomain confusion. These SVMs are trained using 160 im-\\nages, 80 from Amazon and 80 from Webcam, then tested\\non the remaining images from those domains. We plot the\\nclassiﬁer scores for each test image in Figure 7. It is obvious\\nthat the domain confusion representation is domain invariant,\\nmaking it much harder to separate the two domains—the\\ntest accuracy on the domain confusion representation is only\\n56%, not much better than random. In contrast, on the base-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='making it much harder to separate the two domains—the\\ntest accuracy on the domain confusion representation is only\\n56%, not much better than random. In contrast, on the base-\\nline CaffeNet representation, the domain classiﬁer achieves\\n99% test accuracy.\\n5.2. Soft labels for task transfer\\nWe now examine the effect of soft labels in transfer-\\nring information between categories. We consider the\\nAmazon→Webcam shift from the semi-supervised adapta-\\ntion experiment in the previous section. Recall that in this\\nsetting, we have access to target labeled data for only half\\nof our categories. We use soft label information from the\\nsource domain to provide information about the held-out\\ncategories which lack labeled target examples. Figure 8\\nexamines one target example from the held-out category\\nmonitor. No labeled target monitors were available during\\ntraining; however, as shown in the upper right corner of Fig-\\nure 8, the soft labels for laptop computer was present during'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='monitor. No labeled target monitors were available during\\ntraining; however, as shown in the upper right corner of Fig-\\nure 8, the soft labels for laptop computer was present during\\ntraining and assigns a relatively high weight to the monitor\\nclass. Soft label ﬁne-tuning thus allows us to exploit the fact\\nthat these categories are similar. We see that the baseline\\nmodel misclassiﬁes this image as a ring binder, while our\\nsoft label model correctly assigns the monitor label.\\n6. Conclusion\\nWe have presented a CNN architecture that effectively\\nadapts to a new domain with limited or no labeled data per\\ntarget category. We accomplish this through a novel CNN\\narchitecture which simultaneously optimizes for domain in-\\nback pack\\nbike\\nbike helmetbookcasebottlecalculatordesk chairdesk lamp\\ndesktop computer\\nfile cabinetheadphoneskeyboard\\nlaptop computer\\nletter traymobile phone\\nmonitormousemug\\npaper notebook\\npenphoneprinterprojectorpunchersring binder\\nrulerscissorsspeakerstapler\\ntape dispenser'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='file cabinetheadphoneskeyboard\\nlaptop computer\\nletter traymobile phone\\nmonitormousemug\\npaper notebook\\npenphoneprinterprojectorpunchersring binder\\nrulerscissorsspeakerstapler\\ntape dispenser\\ntrash can\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\n0.06\\n0.07\\n0.08\\n0.09\\n0.1 Ours soft label\\nback pack\\nbike\\nbike helmetbookcasebottlecalculatordesk chairdesk lamp\\ndesktop computer\\nfile cabinetheadphoneskeyboard\\nlaptop computer\\nletter traymobile phone\\nmonitormousemug\\npaper notebook\\npenphoneprinterprojectorpunchersring binder\\nrulerscissorsspeakerstapler\\ntape dispenser\\ntrash can\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\n0.06\\n0.07\\n0.08\\n0.09\\n0.1 Baseline soft label\\nback pack bike bike helmet\\nbookcase bottle calculator\\ndesk chair desk lamp desktop computer\\nfile cabinet headphones keyboard\\nlaptop computer letter tray mobile phone\\nring binder\\nmonitor\\nBaseline soft activation Our soft activation\\nSource soft labelsTarget test image\\nFigure 8. Our method (bottom turquoise label) correctly predicts'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ring binder\\nmonitor\\nBaseline soft activation Our soft activation\\nSource soft labelsTarget test image\\nFigure 8. Our method (bottom turquoise label) correctly predicts\\nthe category of this image, whereas the baseline (top purple label)\\ndoes not. The source per-category soft labels for the 15 categories\\nwith labeled target data are shown in the upper right corner, where\\nthe x-axis of the plot represents the 31 categories and the y-axis is\\nthe output probability. We highlight the index corresponding to the\\nmonitor category in red. As no labeled target data is available for\\nthe correct category,monitor, we ﬁnd that in our method the related\\ncategory of laptop computer (outlined with yellow box) transfers\\ninformation to the monitor category. As a result, after training, our\\nmethod places the highest weight on the correct category. Probabil-\\nity score per category for the baseline and our method are shown\\nin the bottom left and right, respectively, training categories are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='ity score per category for the baseline and our method are shown\\nin the bottom left and right, respectively, training categories are\\nopaque and correct test category is shown in red.\\nvariance, to facilitate domain transfer, while transferring\\ntask information between domains in the form of a cross\\nentropy soft label loss. We demonstrate the ability of our\\narchitecture to improve adaptation performance in the super-\\nvised and semi-supervised settings by experimenting with\\ntwo standard domain adaptation benchmark datasets. In the\\nsemi-supervised adaptation setting, we see an average rela-\\ntive improvement of 13% over the baselines on the four most\\nchallenging shifts in the Ofﬁce dataset. Overall, our method\\ncan be easily implemented as an alternative ﬁne-tuning strat-\\negy when limited or no labeled data is available per category\\nin the target domain.\\nAcknowledgements This work was supported by DARPA;\\nAFRL; DoD MURI award N000141110688; NSF awards'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='egy when limited or no labeled data is available per category\\nin the target domain.\\nAcknowledgements This work was supported by DARPA;\\nAFRL; DoD MURI award N000141110688; NSF awards\\n113629, IIS-1427425, and IIS-1212798; and the Berkeley\\nVision and Learning Center.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] L. T. Alessandro Bergamo. Exploiting weakly-labeled web\\nimages to improve object classiﬁcation: a domain adaptation\\napproach. In Neural Information Processing Systems (NIPS),\\nDec. 2010. 2\\n[2] Y . Aytar and A. Zisserman. Tabula rasa: Model transfer for\\nobject category detection. In Proc. ICCV, 2011. 2\\n[3] J. Ba and R. Caruana. Do deep nets really need to be deep?\\nIn Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and\\nK. Weinberger, editors,Advances in Neural Information Pro-\\ncessing Systems 27 , pages 2654–2662. Curran Associates,\\nInc., 2014. 2, 3\\n[4] A. Berg, J. Deng, and L. Fei-Fei. ImageNet Large Scale\\nVisual Recognition Challenge 2012. 2012. 2\\n[5] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel,\\nB. Sch¨olkopf, and A. J. Smola. Integrating structured biologi-\\ncal data by kernel maximum mean discrepancy. In Bioinfor-\\nmatics, 2006. 2\\n[6] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y . LeCun,\\nC. Moore, E. S¨ackinger, and R. Shah. Signature veriﬁcation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='cal data by kernel maximum mean discrepancy. In Bioinfor-\\nmatics, 2006. 2\\n[6] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y . LeCun,\\nC. Moore, E. S¨ackinger, and R. Shah. Signature veriﬁcation\\nusing a siamese time delay neural network. International\\nJournal of Pattern Recognition and Artiﬁcial Intelligence ,\\n7(04):669–688, 1993. 2\\n[7] S. Chopra, S. Balakrishnan, and R. Gopalan. DLID: Deep\\nlearning for domain adaptation by interpolating between do-\\nmains. In ICML Workshop on Challenges in Representation\\nLearning, 2013. 2, 6\\n[8] S. Chopra, R. Hadsell, and Y . LeCun. Learning a similar-\\nity metric discriminatively, with application to face veriﬁ-\\ncation. In Computer Vision and Pattern Recognition, 2005.\\nCVPR 2005. IEEE Computer Society Conference on , vol-\\nume 1, pages 539–546. IEEE, 2005. 2\\n[9] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. In Proc.\\nICML, 2014. 2, 6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='[9] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. In Proc.\\nICML, 2014. 2, 6\\n[10] L. Duan, D. Xu, and I. W. Tsang. Learning with augmented\\nfeatures for heterogeneous domain adaptation. In Proc. ICML,\\n2012. 2\\n[11] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsu-\\npervised visual domain adaptation using subspace alignment.\\nIn Proc. ICCV, 2013. 2\\n[12] Y . Ganin and V . Lempitsky. Unsupervised Domain Adap-\\ntation by Backpropagation. ArXiv e-prints, Sept. 2014. 1,\\n2\\n[13] M. Ghifary, W. B. Kleijn, and M. Zhang. Domain adaptive\\nneural networks for object recognition. CoRR, abs/1409.6041,\\n2014. 2, 6\\n[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\\nfeature hierarchies for accurate object detection and semantic\\nsegmentation. arXiv e-prints, 2013. 1, 2, 3\\n[15] B. Gong, Y . Shi, F. Sha, and K. Grauman. Geodesic ﬂow'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='feature hierarchies for accurate object detection and semantic\\nsegmentation. arXiv e-prints, 2013. 1, 2, 3\\n[15] B. Gong, Y . Shi, F. Sha, and K. Grauman. Geodesic ﬂow\\nkernel for unsupervised domain adaptation. In Proc. CVPR,\\n2012. 2\\n[16] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\\nin a neural network. In NIPS Deep Learning and Representa-\\ntion Learning Workshop, 2014. 2, 3\\n[17] J. Hoffman, S. Guadarrama, E. Tzeng, R. Hu, J. Donahue,\\nR. Girshick, T. Darrell, and K. Saenko. LSDA: Large scale de-\\ntection through adaptation. In Neural Information Processing\\nSystems (NIPS), 2014. 3\\n[18] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Darrell.\\nEfﬁcient learning of domain-invariant image representations.\\nIn Proc. ICLR, 2013. 2, 6\\n[19] J. Hoffman, E. Tzeng, J. Donahue, , Y . Jia, K. Saenko, and\\nT. Darrell. One-shot learning of supervised deep convolutional\\nmodels. In arXiv 1312.6204; presented at ICLR Workshop,\\n2014. 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='[19] J. Hoffman, E. Tzeng, J. Donahue, , Y . Jia, K. Saenko, and\\nT. Darrell. One-shot learning of supervised deep convolutional\\nmodels. In arXiv 1312.6204; presented at ICLR Workshop,\\n2014. 2\\n[20] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-\\ntional architecture for fast feature embedding. arXiv preprint\\narXiv:1408.5093, 2014. 5\\n[21] D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in\\ndata streams. In Proc. VLDB, 2004. 2\\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\\nclassiﬁcation with deep convolutional neural networks. In\\nProc. NIPS, 2012. 2, 4\\n[23] B. Kulis, K. Saenko, and T. Darrell. What you saw is not\\nwhat you get: Domain adaptation using asymmetric kernel\\ntransforms. In Proc. CVPR, 2011. 2\\n[24] M. Long and J. Wang. Learning transferable features with\\ndeep adaptation networks. CoRR, abs/1502.02791, 2015. 1, 2\\n[25] Y . Mansour, M. Mohri, and A. Rostamizadeh. Domain adap-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='[24] M. Long and J. Wang. Learning transferable features with\\ndeep adaptation networks. CoRR, abs/1502.02791, 2015. 1, 2\\n[25] Y . Mansour, M. Mohri, and A. Rostamizadeh. Domain adap-\\ntation: Learning bounds and algorithms. In COLT, 2009.\\n2\\n[26] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y .\\nNg. Multimodal deep learning. In Proceedings of the 28th\\nInternational Conference on Machine Learning (ICML-11),\\npages 689–696, 2011. 2\\n[27] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain\\nadaptation via transfer component analysis. In IJCA, 2009. 2\\n[28] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual\\ncategory models to new domains. In Proc. ECCV, 2010. 2, 5,\\n6\\n[29] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks. CoRR,\\nabs/1312.6229, 2013. 1, 2, 3\\n[30] T. Tommasi, T. Tuytelaars, and B. Caputo. A testbed for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-10-09T00:23:33+00:00', 'author': '', 'keywords': '', 'moddate': '2015-10-09T00:23:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1510.02192v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '1510.02192v1.pdf', 'file_type': 'pdf'}, page_content='tion and detection using convolutional networks. CoRR,\\nabs/1312.6229, 2013. 1, 2, 3\\n[30] T. Tommasi, T. Tuytelaars, and B. Caputo. A testbed for\\ncross-dataset analysis. In TASK-CV Workshop, ECCV, 2014.\\n2, 7\\n[31] A. Torralba and A. Efros. Unbiased look at dataset bias. In\\nProc. CVPR, 2011. 2, 3\\n[32] J. Yang, R. Yan, and A. Hauptmann. Adapting SVM classi-\\nﬁers to data with shifted distributions. In ICDM Workshops,\\n2007. 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='On the Beneﬁt of Combining Neural, Statistical\\nand External Features for Fake News\\nIdentiﬁcation\\nGaurav Bhatt1, Aman Sharma1, Shivam Sharma1, Ankush Nagpal1,\\nBalasubramanian Raman1, and Ankush Mittal 2\\n1 Indian Institute of Technology, Roorkee, India,\\ngauravbhatt.cs.iitr@gmail.com\\n2 Graphic Era University, India\\nAbstract. Identifying the veracity of a news article is an interesting\\nproblem while automating this process can be a challenging task. Detec-\\ntion of a news article as fake is still an open question as it is contingent\\non many factors which the current state-of-the-art models fail to incor-\\nporate. In this paper, we explore a subtask to fake news identiﬁcation,\\nand that is stance detection. Given a news article, the task is to deter-\\nmine the relevance of the body and its claim. We present a novel idea\\nthat combines the neural, statistical and external features to provide\\nan eﬃcient solution to this problem. We compute the neural embedding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='that combines the neural, statistical and external features to provide\\nan eﬃcient solution to this problem. We compute the neural embedding\\nfrom the deep recurrent model, statistical features from the weighted\\nn-gram bag-of-words model and hand crafted external features with the\\nhelp of feature engineering heuristics. Finally, using deep neural layer all\\nthe features are combined, thereby classifying the headline-body news\\npair as agree, disagree, discuss, or unrelated. We compare our proposed\\ntechnique with the current state-of-the-art models on the fake news chal-\\nlenge dataset. Through extensive experiments, we ﬁnd that the proposed\\nmodel outperforms all the state-of-the-art techniques including the sub-\\nmissions to the fake news challenge.\\nKeywords: External features, Statistical features, Word embeddings,\\nFake news, Deep learning\\n1 Introduction\\nFake news being a potential threat towards journalism and public discourse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Keywords: External features, Statistical features, Word embeddings,\\nFake news, Deep learning\\n1 Introduction\\nFake news being a potential threat towards journalism and public discourse\\nhas created a buzz across the internet. With the recent advent of social media\\nplatforms such as Facebook and Twitter, it has become easier to propagate any\\ninformation to the masses within minutes. While the propagation of information\\nis proportional to growth of social media, there has been an aggravation in\\nthe authenticity of these news articles. These days it has become a lot easier\\nto mislead the masses using a single Facebook or Twitter fake post. For an\\ninstance, in the US presidential election of 2016, the fake news has been cited as\\nthe foremost contributing factor that aﬀected the outcome [24].\\narXiv:1712.03935v1  [cs.CL]  11 Dec 2017'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Headline ”Robert Plant Ripped up $800M Led Zeppelin Reunion Contract”Stance\\nBody 1 Led Zeppelin’s Robert Plant turned down 500 MILLION to reform supergroup.Agree\\nBody 2No, Robert Plant did not rip up an $800 million deal to get Led Zeppelin back together.Disagree\\nBody 3 Robert Plant reportedly tore up an $800 million Led Zeppelin reunion deal.Discuss\\nBody 4 Richard Branson’s Virgin Galactic is set to launch SpaceShipTwo today.Unrelated\\nTable 1.Headline-body pairs along with their relative stance.\\nThe root cause of this problem lies in the fact that none of the social net-\\nworking sites use any automatic system that can identify the veracity of news\\nﬂowing across these platforms. A possible reason for this failure is the open do-\\nmain nature of the problem that adds to the intricacies. The recently organized\\nFake News Challenge (FNC-1) [13] is an initiative in this direction. The aim of\\nthis challenge is to build an automatic system that has the capability to iden-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Fake News Challenge (FNC-1) [13] is an initiative in this direction. The aim of\\nthis challenge is to build an automatic system that has the capability to iden-\\ntify whether a news article is fake or not. More speciﬁcally, given a news article\\nthe task is to evaluate the relatedness of the news body towards its headline.\\nThe relatedness or stance is the relative perspective of a news article towards a\\nrelative claim (shown in Table 1).\\nThe idea behind building a countermeasure for fake news is to use machine\\nlearning and natural language processing (NLP) tools that can compute semantic\\nand contextual similarity between the headline and the body, and classify the\\npairs into one of four categories. Deep learning models have been eﬃcacious in\\nsolving many NLP problems that share similarities to fake news which includes\\nbut not limited to - computing semantic similarity between sentences [1, 18],\\ncommunity based question answering [31, 32], etc. The basic building blocks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='but not limited to - computing semantic similarity between sentences [1, 18],\\ncommunity based question answering [31, 32], etc. The basic building blocks\\nof all deep models are recurrent networks such as recurrent neural networks\\n(RNN) [23], long short-term memory networks (LSTM) [16] and gated recurrent\\nunits (GRU) [11], and convolution networks such as convolution neural networks\\n(CNN) [17]. A deep architecture encodes the given sequence of words into ﬁxed\\nlength vector representation which can be used to score the relevance of two\\ntextual entities, in our case, relevance of each headline-body pair.\\nStatistical information related to text can be encoded to vectors using the\\ntraditional bag-of-words (BOW) approach. The BOW approaches are often com-\\nbined with term frequency (TF) and inverse document frequency (IDF), and n-\\ngrams that helps to encode more information related to the text [28, 12]. These\\napproaches, however simple, have been used to ameliorate the performance of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='grams that helps to encode more information related to the text [28, 12]. These\\napproaches, however simple, have been used to ameliorate the performance of\\ndeep models in complex NLP problems such as community question answering\\n[32] and answer sentence selection [33]. Sometimes, it is beneﬁcial to leverage\\nfeature engineering heuristics when combined with statistical approaches. The\\nfeature engineering heuristics or the external features are used to aid the learn-\\ning model to successfully converge to a global solution [31, 32, 34]. The external\\nfeatures includes common observations such as number of n-grams, number of\\nwords match between headline and the body, cosine similarity between the head-\\nline and the body vector, etc. The FNC-1 baseline also includes a combination\\nof feature engineering heuristics that alone achieves a competitive performance,\\neven outperforming several widely used deep learning architectures. In this pa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='per, we combine external features introduced in the baseline with some more\\nheuristics that have been shown to be successful in other NLP tasks.\\nThese days it is common to use pre-trained word embeddings such as Word2vec\\n[20] and GloVe [25] along with deep models for NLP tasks. Similar to word em-\\nbedding, the recurrent models have been used to encode an entire sentence to a\\nvector. Some of the widely used sentence-to-vector models include doc2vec [21],\\nparagraph2vec [27] and skip-thought vectors [18]. These deep recurrent models\\nhelps to capture the semantic and contextual information of the textual pairs,\\nin our case, body and its claim. In our work, we use the skip-thought vector to\\nencode the headline and the body, and combine it with external features and\\nstatistical approaches.\\nFinally, the main contributions of the paper can be summarized as\\n1. We propose an approach that is based on the combination of statistical, neu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='statistical approaches.\\nFinally, the main contributions of the paper can be summarized as\\n1. We propose an approach that is based on the combination of statistical, neu-\\nral and feature engineering heuristics which achieves state-of-the-art perfor-\\nmance on the task of fake news identiﬁcation.\\n2. We evaluate the proposed approach on FNC-1 challenge, and compare our\\nresults with the top-4 submissions to the challenge. We also analyze the\\napplicability of several state-of-the-art deep models on FNC-1 dataset.\\nThe rest of the paper is organized as follows. In section 2, we brief the previous\\nidea over which our works builds, which is followed by applicability of state-\\nof-the-art deep architectures on the problem of stance detection. In section 4\\nwe describe the proposed approach in detail, followed by the experiment setup\\nin section 5, that includes dataset description, training parameters, evaluation\\nmetrics used and results. Finally, our work is concluded in section 6.\\n2 Related Work'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='in section 5, that includes dataset description, training parameters, evaluation\\nmetrics used and results. Finally, our work is concluded in section 6.\\n2 Related Work\\nIn this section, we discuss some previous work that is in relation to fake news\\nidentiﬁcation such as rumor detection in news articles and hoax news identiﬁca-\\ntion. We also discuss the use of deep learning architecture used by some of the\\nresearchers with whom our work shares some similarity.\\nFake news. From an NLP perspective, researchers have studied numerous\\naspects of credibility of online information. For example, [5] applied the time-\\nsensitive supervised approach by relying on the tweet content to address the\\ncredibility of a tweet in diﬀerent situations. [7] used LSTM in a similar problem\\nof early rumor detection. In an another work, [8] aimed at detecting the stance\\nof tweets and determining the veracity of the given rumor with convolution'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='of early rumor detection. In an another work, [8] aimed at detecting the stance\\nof tweets and determining the veracity of the given rumor with convolution\\nneural networks. A submission [3] to the SemEval 2016 Twitter Stance Detection\\ntask focuses on creating a bag-of-words auto encoder, and training it over the\\ntokenized tweets.\\nFNC-1 submissions. In their work, [26] achieved a preliminary score of\\n0.8080, slightly above the competition baseline of 0.7950. They experimented\\non four basic models on which the ﬁnal result was evaluated: Bag Of Words\\n(BOW), basic LSTM, LSTM with attention and conditional encoding LSTM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='with attention (CEA LSTM). In our work, instead of using the models separately,\\nwe combine the best of these models.\\nAnother team, [34], combined multiple models in an ensemble providing\\n50/50 weighted average between deep convolution neural network and a gradient-\\nboosted decision trees. Though this work seems to be similar to our work, the\\ndiﬀerence lies in the construction of ensemble of classiﬁers. In a similar attempt,\\na team [2] concatenated various features vectors and passed it through an MLP\\nmodel.\\nThe work by [28], focuses on generating lexical and similarity features using\\n(TF-IDF) representations of bag-of-words (BOW) which are then fed through\\na multi-layer perceptron (MLP) with one hidden layer. In their work, [6] di-\\nvided the problem into two groups: unrelated and related. They were able to\\nachieve 90% accuracy on the related/unrelated task by ﬁnding maximum and\\naverage Jaccard similarity score across all sentences in the article and choosing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='achieve 90% accuracy on the related/unrelated task by ﬁnding maximum and\\naverage Jaccard similarity score across all sentences in the article and choosing\\nappropriate threshold values. A similar work of splitting the problem into two\\nsubproblems (related and unrelated) is also performed by [10]. The work by [22]\\nfocuses on the use of recurrent models for fake news stance detection.\\n3 Technique Used\\n3.1 Deep Learning Architectures\\nTo predict the stance for a given sample in FNC-1 dataset, a multi-channel\\ndeep neural network can be used to encode a given headline-body pair, which\\ncan be classiﬁed into one of the four stances. This is achieved by using a multi\\nchannel convolution neural network with softmax layer at the output (shown in\\nFigure 1). Similarly, instead of using the convolution and pooling layers, LSTM\\nand GRU can be used to encode the headline-body pairs. The LSTMs and GRUs\\nencode the given sequence of words into ﬁxed length vector representation which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='and GRU can be used to encode the headline-body pairs. The LSTMs and GRUs\\nencode the given sequence of words into ﬁxed length vector representation which\\ncan be used to score the relevance of headline-body pair. However, for long\\nsequences, such as the body of a news article (which typically contain hundreds\\nof words), the RNN models fail to completely encode the entire information\\ninto a ﬁxed length vector. A solution to this problem is given in the form of\\nattentional mechanism [9] which computes a weighted sum of all the encoder\\nunits that are passed on to the decoder. The decoder is learned in such a way\\nthat it gives importance to only some of the words. The attention mechanism\\nalso alleviates the bottleneck of encoding input sequences to ﬁxed length vector\\nand have been shown to outperform other RNN based encoder-decoder models\\non longer sequences [4]. To alleviate the problem of limited memory we use\\nattention mechanism as described in [4].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='and have been shown to outperform other RNN based encoder-decoder models\\non longer sequences [4]. To alleviate the problem of limited memory we use\\nattention mechanism as described in [4].\\nWe experiment with some of the deep architectures that have been shown to\\nbe successful in NLP tasks (shown in Figure 1). Most of these architectures have\\nbeen proven to be eﬀective for non-factoid based question answering [30, 14].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Fig. 1.Deep architectures used for stance detection on the FNC-1 dataset.\\n4 Proposed Idea\\nThe unrelated headline-body pairs in the FNC-1 dataset are created by ran-\\ndomly assigning a news body to the given headline. This type of data augmen-\\ntation has been successfully used in NLP problems such as non-factoid question\\nanswering where it results in reasonable performance by the deep learning mod-\\nels [31, 19]. However, in the case of FNC-1 challenge, the agree, disagree, and\\ndiscuss headline-body pairs are relatively smaller in quantity than theunrelated\\nstance. This bias leads to a uneven distribution of dataset across the four classes,\\nwith the unrelated category being the least interesting. Interestingness of a\\nheadline-body pair is evaluated in terms of information that it contains; It is\\neasier to evaluate a unrelated pair, while the other three are contingent on\\nexploring contextual relationship between the headline and its body, and are\\nconsidered more interesting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='easier to evaluate a unrelated pair, while the other three are contingent on\\nexploring contextual relationship between the headline and its body, and are\\nconsidered more interesting.\\nThe uneven distribution of FNC-1 dataset thwarts the performance of deep\\nlearning architectures introduced in Section 3. Moreover, news articles are heav-\\nily inﬂuenced by some words that are generally associated with news to describe\\nits polarity. For example, words likecrime, accident, and scandal are often used\\nwith negative connotation. If such words are present in both the news headline,\\nor are present in one while absent from the other, then, it is easier to identify such\\na pair as agree or disagree. Deep learning models are dependent on a huge train-\\ning corpus (few million headline-body pairs) in order to identify such nuances\\nin patterns. The FNC-1 dataset, though the largest publicly available dataset\\non stance detection, does not satiate this criteria. For this reason, we introduce'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='in patterns. The FNC-1 dataset, though the largest publicly available dataset\\non stance detection, does not satiate this criteria. For this reason, we introduce\\na much simpler strategy that consists of heavy use of feature engineering. We\\nleveraged several widely used state-of-the-art features used in natural language\\nprocessing, and use a feed-forward deep neural network which aggregates all the\\nindividual features and computes a score for each headline-body pair.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Fig. 2.Combining the neural, statistical and external features using deep MLP.\\n4.1 Neural Embeddings\\nWe use skip-thought vectors which encodes sentences to vector embedding of\\nlength 4800 (shown in Figure 2). The skip-thought [18] is a encoder-decoder\\nbased recurrent model that computes the relative occurrence of sentences. In\\nour work, we use the pre-trained skip-thought embedding which is trained on\\nBookCorpus [35]. We make the use of a pre-trained model since the FNC-1\\ndataset is relatively smaller than the dataset required to eﬃciently train a re-\\ncurrent encoder-decoder model like skip-thought.\\nWe follow the work of [18, 1] and compute two features from the skip-thought\\nembeddings. These features have been shown to be eﬀective in evaluating con-\\ntextual similarity between sentences. The task of stance detection is analogous\\nto the computation of contextual similarity between two sentences - headline\\nand its body. We speculate that the features introduced by [18, 1] should be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='to the computation of contextual similarity between two sentences - headline\\nand its body. We speculate that the features introduced by [18, 1] should be\\neﬀective for stance detection as well. Given the skip-thought encoding of news\\nand headline as unews and vhead, we compute two features\\nfeat1 = unews.vhead (1)\\nfeat2 = |unews −vhead| (2)\\nwhere feat1 is the component-wise product and feat2 is the absolute diﬀer-\\nence between the skip-thought encoding of news and headlines. Both of these\\nfeatures results in a 4800 dimensional vector each.\\n4.2 Statistical Features\\nWe capture the statistical information from the text to vectors with the help of\\nBOW, TF-IDF and n-grams models. We follow the work of [28] and [12], and\\nproduce the following vectors for each headline-body pair\\n1. 1-gram TF vector of the headline.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='2. 1-gram TF vector of the body.\\nThis gives us a vector of 5000 dimension each. We concatenate both of the TF\\nvectors and pass it to a MLP layer (as shown in Figure 2).\\n4.3 External Features\\nThe external features include feature engineering heuristics such as number of\\nsimilar words in the headline and body, cosine similarity between vector encod-\\nings of headline-body pairs, number of n-grams matched between the pairs, etc.\\nWe leveraged ideas for computing the external features from the baseline and\\nadd some extra features, which includes\\n1. Number of characters n-grams match between the headline-body pair, where\\nn = 2, ··· , 16.\\n2. Number of words n-grams match between the headline-body pair, where\\nn = 2, ··· , 6.\\n3. Weighted TF-IDF score between headline and its body using the approach\\nmentioned in [33].\\n4. Sentiment diﬀerence between the headline-body pair, also termed as polarity\\nand is computed using lexicon based approach.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='mentioned in [33].\\n4. Sentiment diﬀerence between the headline-body pair, also termed as polarity\\nand is computed using lexicon based approach.\\n5. N-gram refuting feature which is constructed using BOW on a lexicon of n\\npre-deﬁned words. It is similar to polarity based features with an addition\\nof n-gram model.\\nAll the external features adds up to a 50-dimensional feature vector and is passed\\nto a MLP layer similar to neural and statistical features.\\n5 Experimentations\\n5.1 Dataset Description\\nWe use the dataset provided in the FNC-1 challenge which is derived from the\\nEmergent Dataset [15], provided by the fake news challenge administrators. The\\nformer consist of 49972 tuple with each tuple consisting of a headline-body pair\\nfollowed by a corresponding class label stance of either agree, disagree, unrelated\\nor discuss. Word counts roughly ranges between 8 to 40 for headlines and 600\\nto 7000 for article body. The distribution of FNC-1 dataset is shown in Table 2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='or discuss. Word counts roughly ranges between 8 to 40 for headlines and 600\\nto 7000 for article body. The distribution of FNC-1 dataset is shown in Table 2.\\nNews articles unrelated discuss agree disagree\\n49972 73.13 % 17.83 % 7.36 % 1.68 %\\nTable 2.FNC-1 dataset description.\\nThe ﬁnal results are evaluated over a test dataset provided by fake news\\norganization consisting of 25413 samples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Hyperparameter Skip-thought External FeaturesTF-IDF Vectors\\nMLP layers 2 1 2\\nMLP neurons 500 ; 100 50 500 ; 50\\nDropout 0.2 ; - - 0.4 ; -\\nActivation sigmoid ; sigmoid relu relu ; relu\\nRegularization L2 - 0.00000001 ; - - L2 - 0.00005 ; -\\nMLP Layers 1\\nMLP neurons 4\\nActivation Softmax\\nOptimizer Adam\\nLearning rate 0.001\\nBatch size 100\\nLoss Cross-entropy\\nTable 3.Values of hyper-parameters. The ﬁrst half of the table shows the parameters\\nused in architectures for extracting individual features. The second half shows the\\nparameter setting of the feature combination layer that is shown in Figure 2.\\n5.2 Training parameters\\nAs shown in Figure 2, the proposed model computes the feature vectors sepa-\\nrately and then combine these with the help of a MLP layer. We use cross-entropy\\nas the loss function to optimize our architecture with a softmax layer at the out-\\nput which classify the given headline-body pair into agree, disagree, discuss, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='as the loss function to optimize our architecture with a softmax layer at the out-\\nput which classify the given headline-body pair into agree, disagree, discuss, and\\nunrelated. The hyper-parameter setting is shown in Table 3.\\n5.3 Baselines and compared methods\\nOrganizers of FNC-1 have provided a baseline model that consists of a gradient-\\nboosting classiﬁer over n-gram subsequences between the headline and the body\\nalong with several external features such as word overlap, occurrence of sentiment\\nusing a lexicon of highly-polarized words (like fraud and hoax). With this simple\\nyet elegant baseline it is possible to outperform some of the highly used deep\\nlearning architectures that we have used in our work. Following the work of [26],\\nwe also introduce three new baselines for the FNC-1 dataset: word2vec+external\\nfeatures baseline, skip-thought baseline, and TF-IDF baseline. All these baselines\\nfocuses on performance of neural, statistical, and external features, when used'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='features baseline, skip-thought baseline, and TF-IDF baseline. All these baselines\\nfocuses on performance of neural, statistical, and external features, when used\\nindividually.\\nWe compare our proposed approach with the submissions of top 4 teams at\\nFNC-1 3, which includes the work by [34], [26], [2] and [28]. Apart from the\\ntop submissions at FNC-1, we also compare the proposed architecture with four\\ndeep learning architectures introduced in Section 3, namely, CNN, biLSTM,\\nBiLSTM+Attention and CNN+biLSTM.\\n3 http://www.fakenewschallenge.org/\\nhttps://competitions.codalab.org/competitions/16843#results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='5.4 Evaluation metrics\\nFrom Table 2 it is evident that the FNC-1 dataset shows a heavy bias towards\\nunrelated headline-body pairs. Recognizing this data bias and the simpler na-\\nture of the related/unrelated classiﬁcation problems, the organizers of FNC-1\\nintroduced the following weighted accuracy score as their ﬁnal evaluation metric.\\nScore1 = AccuracyRelated,Unrelated (3)\\nScore2 = AccuracyAgree,Disagree,Discuss (4)\\nScoreFNC = 0.25 ∗Score1 + 0.75 ∗Score2 (5)\\nWe use the ScoreFNC as the main evaluation criteria while comparing the\\nproposed model with other related techniques. We also use the class-wise accu-\\nracy for further evaluation of the performance of all the techniques.\\n5.5 Results\\nThe results on FNC-1 test dataset are shown in Table 4. The ﬁrst part of the ta-\\nble shows the performance of the baselines used in our work. The FNC-1 baseline\\nachieves a score of 75 .2 which is better than the performance of all deep archi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='ble shows the performance of the baselines used in our work. The FNC-1 baseline\\nachieves a score of 75 .2 which is better than the performance of all deep archi-\\ntectures introduced in Section 3. The FNC-1 baseline is comprised of training\\ngradient tree classiﬁer on the hand crafted features (described in Section 4.3).\\nProvided the simplicity of this baseline, it is indeed remarkable to achieve such a\\nhigh score. The FNC-1 baselines achieves approx 7% higher class-wise accuracy\\non unrelated stance as compared to skip-thought baseline, whereas the latter\\nreceiving a higher ScoreFNC . Skip-thought baselines achieves a higher accuracy\\non agree and discuss than the unrelated stance. Since the interestingness of\\nagree and discuss is higher than the unrealted stance, therefore, skip-thought\\nachieves a higher ScoreFNC . This also explains the reason for the introduction\\nof new scoring criterion by the FNC organizers (see Section 5.4). Finally, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='achieves a higher ScoreFNC . This also explains the reason for the introduction\\nof new scoring criterion by the FNC organizers (see Section 5.4). Finally, the\\nScoreFNC by skip-thought, external features, and TF-IDF baselines are higher\\nthan the FNC-1 baseline. Therefore, our speculation to combine these three base-\\nlines models, is guaranteed to achieve a higher score on ScoreFNC evaluation\\nmetric. Moreover, all the baselines achieves very low or zero score on the dis-\\nagree stance. Therefore, apart from the ScoreFNC , the class-wise performance\\nis worth considering as a performance criterion.\\nThe performance of top-4 teams that participated in FNC-1 are shown in the\\nmiddle part of Table 4, with SOLAT in the SWEN[34] winning the challenge\\nachieving a score of 82.05. All the teams achieved higher score and class-wise\\naccuracy on all stances except for the disagree stance. This should be a concern,\\nsince the importance of disagree is equivalent to the agree and discuss stance.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='accuracy on all stances except for the disagree stance. This should be a concern,\\nsince the importance of disagree is equivalent to the agree and discuss stance.\\nWe observed that the news pairs in the disagree category are not only very few,\\nbut also consists of divergent news articles. This is one of the reason for poor\\nperformance of most of the deep models, including the top teams, on identifying\\ndiagree stance.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='Method ScoreFNC Agree Disagree Discuss Unrelated Overall\\nFNC-1 baseline 75.20 9.09 1.00 79.65 97.97 85.44\\nWord2vec + External Features 75.78 50.70 9.61 53.38 96.05 82.79\\nSkip-thought baseline 76.18 31.8 0.00 81.20 91.18 82.48\\nTF-IDF baseline 81.72 44.04 6.60 81.38 97.90 88.46\\nSOLAT in the SWEN [34] 82.05 58.50 1.86 76.18 98.70 89.08\\nAthene [2] 81.97 44.72 9.47 80.89 99.25 89.50\\nUCL Machine Reading [28] 81.72 44.04 6.60 81.38 97.90 88.46\\nChips Ahoy! [29] 80.12 55.96 0.28 70.29 98.98 88.01\\nCNN 60.91 35.89 2.10 46.77 88.47 74.84\\nbiLSTM 63.11 38.04 4.59 58.13 78.27 69.88\\nbiLSTM + Attention 63.17 58.74 0.03 63.48 77.49 73.27\\nCNN + biLSTM 64.95 74.09 2.46 57.85 74.87 72.89\\nProposed 83.08 43.82 6.31 85.68 98.04 89.29\\nTable 4.Performance of diﬀerent models on FNC-1 Test Dataset. The ﬁrst half of the\\ntable shows the baselines, followed by the top-4 submissions, and diﬀerent architectures\\nused in our work. Column 2-5 shows the class-wise accuracy in % while the last column'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='table shows the baselines, followed by the top-4 submissions, and diﬀerent architectures\\nused in our work. Column 2-5 shows the class-wise accuracy in % while the last column\\nshows the overall accuracy.\\nAgree Disagree Discuss Unrelated Overall\\nAgree 834 15 945 109 43.82\\nDisagree 208 44 328 117 6.31\\nDiscuss 401 23 3825 215 85.68\\nUnrelated 22 12 325 17990 98.04\\nTable 5.Confusion matrix for the proposed model on FNC-1 testset.\\nThe lowest section in Table 4 shows the performance of the proposed model\\nalong with other architectures used in our work. The proposed model achieves\\nhighest score and highest class-wise accuracy ondiscuss stance whereas achieving\\nhigh accuracy on other stances that is comparable to top submissions at FNC-1.\\nFrom Table 5, it is evident that the overall accuracy achieved by the proposed\\nmodel is slightly lower than [2], although the proposed model outperformed all\\nthe other techniques by a clear margin (in terms of ScoreFNC ). The possible'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='model is slightly lower than [2], although the proposed model outperformed all\\nthe other techniques by a clear margin (in terms of ScoreFNC ). The possible\\nreason for this deviation is that the [2] gives more focus to the classiﬁcation of\\nunrelated stances rather than the rest, which is the reason for highest overall\\naccuracy. Since unrelated stances are of least interest to us, this results in lower\\nScoreFNC . Finally, a confusion matrix is given in Table 5 that provides in-detail\\nanalysis of the performance of our approach.\\n6 Conclusion\\nIn this paper, we explore the beneﬁt of incorporating neural, statistical and\\nexternal features to deep neural networks on the task of fake news stance de-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='tection. We also presented in-depth analysis of several state-of-the-art recurrent\\nand convolution architectures (shown in Figure 1). The presented idea lever-\\nages features extracted using skip-thought embeddings, n-gram TF-vectors and\\nseveral introduced hand crafted features.\\nWe found that the uneven distribution of FNC-1 dataset undermines the\\nperformance of most deep learning architectures. The fewer training samples\\nadds further to this aggravation. Creating a dataset for a complex NLP problems\\nsuch as fake news identiﬁcation is indeed a cumbersome task, and we appreciate\\nthe work by the FNC organizers, yet, a more detailed and elaborate dataset\\nshould make this challenge more suitable to evaluate.\\nReferences\\n[1] E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W. Guo. sem\\n2013 shared task: Semantic textual similarity, including a pilot on typed-\\nsimilarity. In In* SEM 2013: The Second Joint Conference on Lexical and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='2013 shared task: Semantic textual similarity, including a pilot on typed-\\nsimilarity. In In* SEM 2013: The Second Joint Conference on Lexical and\\nComputational Semantics. Association for Computational Linguistics. Cite-\\nseer, 2013.\\n[2] B. S. Andreas Hanselowski, Avinesh PVS and F. Caspelherr. Team athene\\non the fake news challenge. 2017.\\n[3] I. Augenstein, A. Vlachos, and K. Bontcheva. Usfd at semeval-2016 task\\n6: Any-target stance detection on twitter with autoencoders. In SemEval@\\nNAACL-HLT, pages 389–393, 2016.\\n[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014. URL http:\\n//arxiv.org/abs/1409.0473.\\n[5] C. Castillo, M. Mendoza, and B. Poblete. Predicting information credibility\\nin time-sensitive social media. Internet Research, 23(5):560–588, 2013.\\n[6] A. K. Chaudhry, D. Baker, and P. Thun-Hohenstein. Stance detection for\\nthe fake news challenge: Identifying textual relationships with deep neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='[6] A. K. Chaudhry, D. Baker, and P. Thun-Hohenstein. Stance detection for\\nthe fake news challenge: Identifying textual relationships with deep neural\\nnets. 2017.\\n[7] T. Chen, L. Wu, X. Li, J. Zhang, H. Yin, and Y. Wang. Call attention\\nto rumors: Deep attention based recurrent neural networks for early rumor\\ndetection. arXiv preprint arXiv:1704.05973, 2017.\\n[8] Y.-C. Chen, Z.-Y. Liu, and H.-Y. Kao. Ikm at semeval-2017 task 8: Convo-\\nlutional neural networks for stance detection and rumor veriﬁcation. Pro-\\nceedings of SemEval. ACL, 2017.\\n[9] K. Cho, B. van Merrienboer, C ¸ . G¨ ul¸ cehre, F. Bougares, H. Schwenk, and\\nY. Bengio. Learning phrase representations using RNN encoder-decoder for\\nstatistical machine translation. CoRR, abs/1406.1078, 2014. URL http:\\n//arxiv.org/abs/1406.1078.\\n[10] S. Chopra, S. Jain, and J. M. Sholar. Towards automatic identiﬁcation\\nof fake news: Headline-article stance detection with lstm attention models,\\n2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='[11] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of\\ngated recurrent neural networks on sequence modeling. arXiv preprint\\narXiv:1412.3555, 2014.\\n[12] R. Davis and C. Proctor. Fake news, real consequences: Recruiting neural\\nnetworks for the ﬁght against fake news. 2017.\\n[13] D. R. Dean Pomerleau. Fake news challenge. 2017.\\n[14] M. Feng, B. Xiang, M. R. Glass, L. Wang, and B. Zhou. Applying deep\\nlearning to answer selection: A study and an open task. InAutomatic Speech\\nRecognition and Understanding (ASRU), 2015 IEEE Workshop on, pages\\n813–820. IEEE, 2015.\\n[15] W. Ferreira and A. Vlachos. Emergent: a novel data-set for stance classiﬁca-\\ntion. In Proceedings of the 2016 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Tech-\\nnologies. ACL, 2016.\\n[16] A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with\\nbidirectional lstm and other neural network architectures. Neural Networks,\\n18(5):602–610, 2005.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='nologies. ACL, 2016.\\n[16] A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with\\nbidirectional lstm and other neural network architectures. Neural Networks,\\n18(5):602–610, 2005.\\n[17] H. He, K. Gimpel, and J. J. Lin. Multi-perspective sentence similarity\\nmodeling with convolutional neural networks. InEMNLP, pages 1576–1586,\\n2015.\\n[18] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba,\\nand S. Fidler. Skip-thought vectors. In Advances in neural information\\nprocessing systems, pages 3294–3302, 2015.\\n[19] T. Mihaylov and P. Nakov. Semanticz at semeval-2016 task 3: Ranking\\nrelevant answers in community question answering using semantic similarity\\nbased on ﬁne-tuned word embeddings. In SemEval@ NAACL-HLT, pages\\n879–886, 2016.\\n[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word\\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed\\nrepresentations of words and phrases and their compositionality. In Ad-\\nvances in neural information processing systems, pages 3111–3119, 2013.\\n[22] K. Miller and A. Oswalt. Fake news headline classiﬁcation using neural\\nnetworks with attention. 2017.\\n[23] P. Neculoiu, M. Versteegh, M. Rotaru, and T. B. Amsterdam. Learning\\ntext similarity with siamese recurrent networks. ACL 2016, page 148, 2016.\\n[24] NYTimes. As fake news spreads lies, more readers shrug at the truth. 2016.\\n[25] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for\\nword representation. In EMNLP, volume 14, pages 1532–1543, 2014.\\n[26] S. Pfohl, O. Triebe, and F. Legros. Stance detection for the fake news\\nchallenge with attention and conditional encoding.\\n[27] R. ˇReh˚ uˇ rek and P. Sojka. Software Framework for Topic Modelling with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='challenge with attention and conditional encoding.\\n[27] R. ˇReh˚ uˇ rek and P. Sojka. Software Framework for Topic Modelling with\\nLarge Corpora. In Proceedings of the LREC 2010 Workshop on New\\nChallenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010.\\nELRA. http://is.muni.cz/publication/884893/en.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='[28] B. Riedel, I. Augenstein, G. P. Spithourakis, and S. Riedel. A simple but\\ntough-to-beat baseline for the fake news challenge stance detection task.\\narXiv preprint arXiv:1707.03264, 2017.\\n[29] J. Shang. Chips ahoy! at fake news challenge. 2017.\\n[30] M. Tan, C. d. Santos, B. Xiang, and B. Zhou. Lstm-based deep learning\\nmodels for non-factoid answer selection. arXiv preprint arXiv:1511.04108,\\n2015.\\n[31] L. Yang, Q. Ai, D. Spina, R.-C. Chen, L. Pang, W. B. Croft, J. Guo, and\\nF. Scholer. Beyond factoid qa: Eﬀective methods for non-factoid answer\\nsentence retrieval. In European Conference on Information Retrieval, pages\\n115–128. Springer, 2016.\\n[32] Y. Yang, W.-t. Yih, and C. Meek. Wikiqa: A challenge dataset for open-\\ndomain question answering. In EMNLP, pages 2013–2018, 2015.\\n[33] L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman. Deep learning for\\nanswer sentence selection. arXiv preprint arXiv:1412.1632, 2014.\\n[34] S. B. Yuxi Pan, Doug Sibley. Talos. http://blog.talosintelligence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-12-12T14:36:49+00:00', 'author': '', 'keywords': '', 'moddate': '2017-12-12T14:36:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1712.03935v1.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': '1712.03935v1.pdf', 'file_type': 'pdf'}, page_content='answer sentence selection. arXiv preprint arXiv:1412.1632, 2014.\\n[34] S. B. Yuxi Pan, Doug Sibley. Talos. http://blog.talosintelligence.\\ncom/2017/06/, 2017.\\n[35] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and\\nS. Fidler. Aligning books and movies: Towards story-like visual explanations\\nby watching movies and reading books. arXiv preprint arXiv:1506.06724,\\n2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nN-BEATS: N EURAL BASIS EXPANSION ANALYSIS FOR\\nINTERPRETABLE TIME SERIES FORECASTING\\nBoris N. Oreshkin\\nElement AI\\nboris.oreshkin@gmail.com\\nDmitri Carpov\\nElement AI\\ndmitri.carpov@elementai.com\\nNicolas Chapados\\nElement AI\\nchapados@elementai.com\\nYoshua Bengio\\nMila\\nyoshua.bengio@mila.quebec\\nABSTRACT\\nWe focus on solving the univariate times series point forecasting problem using\\ndeep learning. We propose a deep neural architecture based on backward and\\nforward residual links and a very deep stack of fully-connected layers. The ar-\\nchitecture has a number of desirable properties, being interpretable, applicable\\nwithout modiﬁcation to a wide array of target domains, and fast to train. We test\\nthe proposed architecture on several well-known datasets, including M3, M4 and\\nTOURISM competition datasets containing time series from diverse domains. We\\ndemonstrate state-of-the-art performance for two conﬁgurations of N-BEATS for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='TOURISM competition datasets containing time series from diverse domains. We\\ndemonstrate state-of-the-art performance for two conﬁgurations of N-BEATS for\\nall the datasets, improving forecast accuracy by 11% over a statistical benchmark\\nand by 3% over last year’s winner of the M4 competition, a domain-adjusted\\nhand-crafted hybrid between neural network and statistical time series models.\\nThe ﬁrst conﬁguration of our model does not employ any time-series-speciﬁc\\ncomponents and its performance on heterogeneous datasets strongly suggests that,\\ncontrarily to received wisdom, deep learning primitives such as residual blocks are\\nby themselves sufﬁcient to solve a wide range of forecasting problems. Finally, we\\ndemonstrate how the proposed architecture can be augmented to provide outputs\\nthat are interpretable without considerable loss in accuracy.\\n1 I NTRODUCTION\\nTime series (TS) forecasting is an important business problem and a fruitful application area for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='that are interpretable without considerable loss in accuracy.\\n1 I NTRODUCTION\\nTime series (TS) forecasting is an important business problem and a fruitful application area for\\nmachine learning (ML). It underlies most aspects of modern business, including such critical areas as\\ninventory control and customer management, as well as business planning going from production and\\ndistribution to ﬁnance and marketing. As such, it has a considerable ﬁnancial impact, often ranging\\nin the millions of dollars for every point of forecasting accuracy gained (Jain, 2017; Kahn, 2003).\\nAnd yet, unlike areas such as computer vision or natural language processing where deep learning\\n(DL) techniques are now well entrenched, there still exists evidence that ML and DL struggle to\\noutperform classical statistical TS forecasting approaches (Makridakis et al., 2018a;b). For instance,\\nthe rankings of the six “pure” ML methods submitted to M4 competition were 23, 37, 38, 48, 54,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='outperform classical statistical TS forecasting approaches (Makridakis et al., 2018a;b). For instance,\\nthe rankings of the six “pure” ML methods submitted to M4 competition were 23, 37, 38, 48, 54,\\nand 57 out of a total of 60 entries, and most of the best-ranking methods were ensembles of classical\\nstatistical techniques (Makridakis et al., 2018b).\\nOn the other hand, the M4 competition winner (Smyl, 2020), was based on a hybrid between\\nneural residual/attention dilated LSTM stack with a classical Holt-Winters statistical model (Holt,\\n1957; 2004; Winters, 1960) with learnable parameters. Since Smyl’s approach heavily depends on\\nthis Holt-Winters component, Makridakis et al. (2018b) further argue that “hybrid approaches and\\ncombinations of method are the way forward for improving the forecasting accuracy and making\\nforecasting more valuable”. In this work we aspire to challenge this conclusion by exploring the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='combinations of method are the way forward for improving the forecasting accuracy and making\\nforecasting more valuable”. In this work we aspire to challenge this conclusion by exploring the\\npotential of pure DL architectures in the context of the TS forecasting. Moreover, in the context of\\ninterpretable DL architecture design, we are interested in answering the following question: can we\\n1\\narXiv:1905.10437v4  [cs.LG]  20 Feb 2020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\ninject a suitable inductive bias in the model to make its internal operations more interpretable, in the\\nsense of extracting some explainable driving factors combining to produce a given forecast?\\n1.1 S UMMARY OF CONTRIBUTIONS\\nDeep Neural Architecture:To the best of our knowledge, this is the ﬁrst work to empirically\\ndemonstrate that pure DL using no time-series speciﬁc components outperforms well-established\\nstatistical approaches on M3, M4 and TOURISM datasets (on M4, by 11% over statistical benchmark,\\nby 7% over the best statistical entry, and by 3% over the M4 competition winner). In our view, this\\nprovides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens\\nmotivation to continue advancing the research in this area.\\nInterpretable DL for Time Series:In addition to accuracy beneﬁts, we also show that it is fea-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='motivation to continue advancing the research in this area.\\nInterpretable DL for Time Series:In addition to accuracy beneﬁts, we also show that it is fea-\\nsible to design an architecture with interpretable outputs that can be used by practitioners in very\\nmuch the same way as traditional decomposition techniques such as the “seasonality-trend-level”\\napproach (Cleveland et al., 1990).\\n2 P ROBLEM STATEMENT\\nWe consider the univariate point forecasting problem in discrete time. Given a length- H forecast\\nhorizon a length- T observed series history [y1,..., yT ] ∈RT , the task is to predict the vector of\\nfuture values y ∈RH = [yT +1,yT +2,..., yT +H ]. For simplicity, we will later consider a lookback\\nwindow of length t ≤T ending with the last observed value yT to serve as model input, and denoted\\nx ∈Rt = [yT −t+1,..., yT ]. We denote ˆy the forecast of y. The following metrics are commonly\\nused to evaluate forecasting performance (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='x ∈Rt = [yT −t+1,..., yT ]. We denote ˆy the forecast of y. The following metrics are commonly\\nused to evaluate forecasting performance (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000;\\nMakridakis et al., 2018b; Athanasopoulos et al., 2011):\\nsMAPE = 200\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\n|yT +i|+|ˆyT +i|, MAPE = 100\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\n|yT +i| ,\\nMASE = 1\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\n1\\nT +H−m ∑T +H\\nj=m+1 |yj −yj−m|, OWA = 1\\n2\\n[ sMAPE\\nsMAPE Naïve2\\n+\\nMASE\\nMASE Naïve2\\n]\\n.\\nHere m is the periodicity of the data (e.g., 12 for monthly series). MAPE (Mean Absolute Percentage\\nError), sMAPE (symmetric MAPE ) and MASE (Mean Absolute Scaled Error) are standard scale-free\\nmetrics in the practice of forecasting (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000):\\nwhereas sMAPE scales the error by the average between the forecast and ground truth, the MASE\\nscales by the average error of the naïve predictor that simply copies the observation measured m'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='whereas sMAPE scales the error by the average between the forecast and ground truth, the MASE\\nscales by the average error of the naïve predictor that simply copies the observation measured m\\nperiods in the past, thereby accounting for seasonality. OWA (overall weighted average) is a M4-\\nspeciﬁc metric used to rank competition entries (M4 Team, 2018b), where sMAPE and MASE metrics\\nare normalized such that a seasonally-adjusted naïve forecast obtains OWA = 1.0.\\n3 N-BEATS\\nOur architecture design methodology relies on a few key principles. First, the base architecture\\nshould be simple and generic, yet expressive (deep). Second, the architecture should not rely on time-\\nseries-speciﬁc feature engineering or input scaling. These prerequisites let us explore the potential\\nof pure DL architecture in TS forecasting. Finally, as a prerequisite to explore interpretability, the\\narchitecture should be extendable towards making its outputs human interpretable. We now discuss'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='of pure DL architecture in TS forecasting. Finally, as a prerequisite to explore interpretability, the\\narchitecture should be extendable towards making its outputs human interpretable. We now discuss\\nhow those principles converge to the proposed architecture.\\n3.1 B ASIC BLOCK\\nThe proposed basic building block has a fork architecture and is depicted in Fig. 1 (left). We focus on\\ndescribing the operation of ℓ-th block in this section in detail (note that the block index ℓ is dropped\\nin Fig. 1 for brevity). The ℓ-th block accepts its respective input xℓ and outputs two vectors, ˆxℓ and\\nˆyℓ. For the very ﬁrst block in the model, its respective xℓ is the overall model input — a history\\nlookback window of certain length ending with the last measured observation. We set the length of\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nFC Stack\\n(4 layers)\\nFC FC\\n( )\\x00\\x00 \\x00\\x00( )\\x00\\x00 \\x00\\x00\\nForecastBackcast\\n\\x00\\x00\\nBlock Input\\n\\x00\\x00\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)\\nLookback window\\n(model input)\\nForecast Period\\nHorizon H\\nLookback\\xa0Period\\nHorizon nH (here n=3)\\nFigure 1: Proposed architecture. The basic building block is a multi-layer FC network with RELU\\nnonlinearities. It predicts basis expansion coefﬁcients both forward, θ f , (forecast) and backward, θb,\\n(backcast). Blocks are organized into stacks using doubly residual stacking principle. A stack may\\nhave layers with shared gb and gf . Forecasts are aggregated in hierarchical fashion. This enables\\nbuilding a very deep neural network with interpretable outputs.\\ninput window to a multiple of the forecast horizon H, and typical lengths of x in our setup range from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='building a very deep neural network with interpretable outputs.\\ninput window to a multiple of the forecast horizon H, and typical lengths of x in our setup range from\\n2H to 7H. For the rest of the blocks, their inputs xℓ are residual outputs of the previous blocks. Each\\nblock has two outputs: ˆyℓ, the block’s forward forecast of lengthH; and ˆxℓ, the block’s best estimate\\nof xℓ, also known as the ‘backcast’, given the constraints on the functional space that the block can\\nuse to approximate signals.\\nInternally, the basic building block consists of two parts. The ﬁrst part is a fully connected network\\nthat produces the forward θ f\\nℓ and the backward θb\\nℓ predictors of expansion coefﬁcients (again, note\\nthat the block index ℓ is dropped for θb\\nℓ , θ f\\nℓ , gb\\nℓ, gf\\nℓ in Fig. 1 for brevity). The second part consists of\\nthe backward gb\\nℓ and the forward gf\\nℓ basis layers that accept the respective forward θ f\\nℓ and backward\\nθb'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='ℓ , θ f\\nℓ , gb\\nℓ, gf\\nℓ in Fig. 1 for brevity). The second part consists of\\nthe backward gb\\nℓ and the forward gf\\nℓ basis layers that accept the respective forward θ f\\nℓ and backward\\nθb\\nℓ expansion coefﬁcients, project them internally on the set of basis functions and produce the\\nbackcast ˆxℓ and the forecast outputs ˆyℓ deﬁned in the previous paragraph.\\nThe operation of the ﬁrst part of the ℓ-th block is described by the following equations:\\nhℓ,1 = FCℓ,1(xℓ), hℓ,2 = FCℓ,2(hℓ,1), hℓ,3 = FCℓ,3(hℓ,2), hℓ,4 = FCℓ,4(hℓ,3).\\nθb\\nℓ = LINEAR b\\nℓ(hℓ,4), θ f\\nℓ = LINEAR f\\nℓ (hℓ,4).\\n(1)\\nHere LINEAR layer is simply a linear projection layer, i.e. θ f\\nℓ = Wf\\nℓ hℓ,4. The FC layer is a standard\\nfully connected layer with RELU non-linearity (Nair & Hinton, 2010), such that for FCℓ,1 we have,\\nfor example: hℓ,1 = RELU(Wℓ,1xℓ +bℓ,1). One task of this part of the architecture is to predict the\\nforward expansion coefﬁcients θ f\\nℓ with the ultimate goal of optimizing the accuracy of the partial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='forward expansion coefﬁcients θ f\\nℓ with the ultimate goal of optimizing the accuracy of the partial\\nforecast ˆyℓ by properly mixing the basis vectors supplied by gf\\nℓ . Additionally, this sub-network\\npredicts backward expansion coefﬁcients θb\\nℓ used by gb\\nℓ to produce an estimate of xℓ with the ultimate\\ngoal of helping the downstream blocks by removing components of their input that are not helpful for\\nforecasting.\\nThe second part of the network maps expansion coefﬁcients θ f\\nℓ and θb\\nℓ to outputs via basis layers,\\nˆyℓ = gf\\nℓ (θ f\\nℓ ) and ˆxℓ = gb\\nℓ(θb\\nℓ ). Its operation is described by the following equations:\\nˆyℓ =\\ndim(θ f\\nℓ )\\n∑\\ni=1\\nθ f\\nℓ,ivf\\ni , ˆxℓ =\\ndim(θb\\nℓ )\\n∑\\ni=1\\nθb\\nℓ,ivb\\ni .\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nHere vf\\ni and vb\\ni are forecast and backcast basis vectors, θ f\\nℓ,i is the i-th element of θ f\\nℓ . The function\\nof gb\\nℓ and gf\\nℓ is to provide sufﬁciently rich sets {vf\\ni }\\ndim(θ f\\nℓ )\\ni=1 and {vb\\ni }\\ndim(θb\\nℓ )\\ni=1 such that their respective\\noutputs can be represented adequately via varying expansion coefﬁcientsθ f\\nℓ and θb\\nℓ . As shown below,\\ngb\\nℓ and gf\\nℓ can either be chosen to be learnable or can be set to speciﬁc functional forms to reﬂect\\ncertain problem-speciﬁc inductive biases in order to appropriately constrain the structure of outputs.\\nConcrete examples of gb\\nℓ and gf\\nℓ are discussed in Section 3.3.\\n3.2 D OUBLY RESIDUAL STACKING\\nThe classical residual network architecture adds the input of the stack of layers to its output before\\npassing the result to the next stack (He et al., 2016). The DenseNet architecture proposed by Huang\\net al. (2017) extends this principle by introducing extra connections from the output of each stack to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='et al. (2017) extends this principle by introducing extra connections from the output of each stack to\\nthe input of every other stack that follows it. These approaches provide clear advantages in improving\\nthe trainability of deep architectures. Their disadvantage in the context of this work is that they result\\nin network structures that are difﬁcult to interpret. We propose a novel hierarchical doubly residual\\ntopology depicted in Fig. 1 (middle and right). The proposed architecture has two residual branches,\\none running over backcast prediction of each layer and the other one is running over the forecast\\nbranch of each layer. Its operation is described by the following equations:\\nxℓ = xℓ−1 −ˆxℓ−1, ˆy = ∑\\nℓ\\nˆyℓ.\\nAs previously mentioned, in the special case of the very ﬁrst block, its input is the model level\\ninput x, x1 ≡x. For all other blocks, the backcast residual branch xℓ can be thought of as running a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='As previously mentioned, in the special case of the very ﬁrst block, its input is the model level\\ninput x, x1 ≡x. For all other blocks, the backcast residual branch xℓ can be thought of as running a\\nsequential analysis of the input signal. Previous block removes the portion of the signal ˆxℓ−1 that\\nit can approximate well, making the forecast job of the downstream blocks easier. This structure\\nalso facilitates more ﬂuid gradient backpropagation. More importantly, each block outputs a partial\\nforecast ˆyℓ that is ﬁrst aggregated at the stack level and then at the overall network level, providing a\\nhierarchical decomposition. The ﬁnal forecast ˆy is the sum of all partial forecasts. In a generic model\\ncontext, when stacks are allowed to have arbitrary gb\\nℓ and gf\\nℓ for each layer, this makes the network\\nmore transparent to gradient ﬂows. In a special situation of deliberate structure enforced in gb\\nℓ and gf\\nℓ'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='ℓ and gf\\nℓ for each layer, this makes the network\\nmore transparent to gradient ﬂows. In a special situation of deliberate structure enforced in gb\\nℓ and gf\\nℓ\\nshared over a stack, explained next, this has the critical importance of enabling interpretability via the\\naggregation of meaningful partial forecasts.\\n3.3 I NTERPRETABILITY\\nWe propose two conﬁgurations of the architecture, based on the selection of gb\\nℓ and gf\\nℓ . One of them\\nis generic DL, the other one is augmented with certain inductive biases to be interpretable.\\nThe generic architecturedoes not rely on TS-speciﬁc knowledge. We set gb\\nℓ and gf\\nℓ to be a linear\\nprojection of the previous layer output. In this case the outputs of block ℓ are described as:\\nˆyℓ = Vf\\nℓ θ f\\nℓ +bf\\nℓ , ˆxℓ = Vb\\nℓθb\\nℓ +bb\\nℓ.\\nThe interpretation of this model is that the FC layers in the basic building block depicted in Fig. 1 learn\\nthe predictive decomposition of the partial forecast ˆyℓ in the basis Vf\\nℓ learned by the network. Matrix\\nVf'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='the predictive decomposition of the partial forecast ˆyℓ in the basis Vf\\nℓ learned by the network. Matrix\\nVf\\nℓ has dimensionality H ×dim(θ f\\nℓ ). Therefore, the ﬁrst dimension of Vf\\nℓ has the interpretation of\\ndiscrete time index in the forecast domain. The second dimension of the matrix has the interpretation\\nof the indices of the basis functions, with θ f\\nℓ being the expansion coefﬁcients for this basis. Thus the\\ncolumns of Vf\\nℓ can be thought of as waveforms in the time domain. Because no additional constraints\\nare imposed on the form of Vf\\nℓ , the waveforms learned by the deep model do not have inherent\\nstructure (and none is apparent in our experiments). This leads to ˆyℓ not being interpretable.\\nThe interpretable architecturecan be constructed by reusing the overall architectural approach in\\nFig. 1 and by adding structure to basis layers at stack level. Forecasting practitioners often use the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Fig. 1 and by adding structure to basis layers at stack level. Forecasting practitioners often use the\\ndecomposition of time series into trend and seasonality, such as those performed by theSTL (Cleveland\\net al., 1990) and X13-ARIMA (U.S. Census Bureau, 2013). We propose to design the trend and\\nseasonality decomposition into the model to make the stack outputs more easily interpretable. Note\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nthat for the generic model the notion of stack was not necessary and the stack level indexing was\\nomitted for clarity. Now we will consider both stack level and block level indexing. For example, ˆys,ℓ\\nwill denote the partial forecast of block ℓ within stack s.\\nTrend model.A typical characteristic of trend is that most of the time it is a monotonic function, or\\nat least a slowly varying function. In order to mimic this behaviour we propose to constrain gb\\ns,ℓ and\\ngf\\ns,ℓ to be a polynomial of small degree p, a function slowly varying across forecast window:\\nˆys,ℓ =\\np\\n∑\\ni=0\\nθ f\\ns,ℓ,iti. (2)\\nHere time vector t = [0,1,2,..., H −2,H −1]T /H is deﬁned on a discrete grid running from 0 to\\n(H −1)/H, forecasting H steps ahead. Alternatively, the trend forecast in matrix form will then be:\\nˆytr\\ns,ℓ = Tθ f\\ns,ℓ,\\nwhere θ f\\ns,ℓ are polynomial coefﬁcients predicted by a FC network of layer ℓ of stack s described by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='ˆytr\\ns,ℓ = Tθ f\\ns,ℓ,\\nwhere θ f\\ns,ℓ are polynomial coefﬁcients predicted by a FC network of layer ℓ of stack s described by\\nequations (1); and T = [1,t,..., tp] is the matrix of powers of t. If p is low, e.g. 2 or 3, it forces ˆytr\\ns,ℓ\\nto mimic trend.\\nSeasonality model. Typical characteristic of seasonality is that it is a regular, cyclical, recurring\\nﬂuctuation. Therefore, to model seasonality, we propose to constrain gb\\ns,ℓ and gf\\ns,ℓ to belong to the\\nclass of periodic functions, i.e. yt = yt−∆, where ∆ is a seasonality period. A natural choice for the\\nbasis to model periodic function is the Fourier series:\\nˆys,ℓ =\\n⌊H/2−1⌋\\n∑\\ni=0\\nθ f\\ns,ℓ,i cos(2πit)+ θ f\\ns,ℓ,i+⌊H/2⌋sin(2πit), (3)\\nThe seasonality forecast will then have the matrix form as follows:\\nˆyseas\\ns,ℓ = Sθ f\\ns,ℓ,\\nwhere θ f\\ns,ℓ are Fourier coefﬁcients predicted by a FC network of layer ℓ of stack s described by\\nequations (1); and S = [1,cos(2πt),... cos(2π⌊H/2−1⌋t)),sin(2πt),..., sin(2π⌊H/2−1⌋t))] is the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='s,ℓ,\\nwhere θ f\\ns,ℓ are Fourier coefﬁcients predicted by a FC network of layer ℓ of stack s described by\\nequations (1); and S = [1,cos(2πt),... cos(2π⌊H/2−1⌋t)),sin(2πt),..., sin(2π⌊H/2−1⌋t))] is the\\nmatrix of sinusoidal waveforms. The forecast ˆyseas\\ns,ℓ is then a periodic function mimicking typical\\nseasonal patterns.\\nThe overall interpretable architecture consists of two stacks: the trend stack is followed by the\\nseasonality stack. The doubly residual stacking combined with the forecast/backcast principle result\\nin (i) the trend component being removed from the input windowx before it is fed into the seasonality\\nstack and (ii) the partial forecasts of trend and seasonality are available as separate interpretable\\noutputs. Structurally, each of the stacks consists of several blocks connected with residual connections\\nas depicted in Fig. 1 and each of them shares its respective, non-learnable gb\\ns,ℓ and gf\\ns,ℓ. The number'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='as depicted in Fig. 1 and each of them shares its respective, non-learnable gb\\ns,ℓ and gf\\ns,ℓ. The number\\nof blocks is 3 for both trend and seasonality. We found that on top of sharing gb\\ns,ℓ and gf\\ns,ℓ, sharing all\\nthe weights across blocks in a stack resulted in better validation performance.\\n3.4 E NSEMBLING\\nEnsembling is used by all the top entries in the M4-competition. We rely on ensembling as well\\nto be comparable. We found that ensembling is a much more powerful regularization technique\\nthan the popular alternatives, e.g. dropout or L2-norm penalty. The addition of those methods\\nimproved individual models, but was hurting the performance of the ensemble. The core property of\\nan ensemble is diversity. We build an ensemble using several sources of diversity. First, the ensemble\\nmodels are ﬁt on three different metrics: sMAPE ,MASE and MAPE , a version of sMAPE that has only\\nthe ground truth value in the denominator. Second, for every horizonH, individual models are trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='models are ﬁt on three different metrics: sMAPE ,MASE and MAPE , a version of sMAPE that has only\\nthe ground truth value in the denominator. Second, for every horizonH, individual models are trained\\non input windows of different length: 2H,3H,..., 7H, for a total of six window lengths. Thus the\\noverall ensemble exhibits a multi-scale aspect. Finally, we perform a bagging procedure (Breiman,\\n1996) by including models trained with different random initializations. We use 180 total models to\\nreport results on the test set (please refer to Appendix B for the ablation of ensemble size). We use\\nthe median as ensemble aggregation function.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 1: Performance on the M4, M3, TOURISM test sets, aggregated over each dataset. Evaluation\\nmetrics are speciﬁed for each dataset; lower values are better. The number of time series in each\\ndataset is provided in brackets.\\nM4 Average (100,000) M3 Average (3,003) TOURISM Average (1,311)\\nsMAPE OWA sMAPE MAPE\\nPure ML 12.894 0.915 Comb S-H-D 13.52 ETS 20.88\\nStatistical 11.986 0.861 ForecastPro 13.19 Theta 20.88\\nProLogistica 11.845 0.841 Theta 13.01 ForePro 19.84\\nML/TS combination 11.720 0.838 DOTM 12.90 Stratometrics 19.52\\nDL/TS hybrid 11.374 0.821 EXP 12.71 LeeCBaker 19.35\\nN-BEATS-G 11.168 0.797 12.47 18.47\\nN-BEATS-I 11.174 0.798 12.43 18.97\\nN-BEATS-I+G 11.135 0.795 12.37 18.52\\n4 R ELATED WORK\\nThe approaches to TS forecasting can be split in a few distinct categories. The statistical model-\\ning approaches based on exponential smoothing and its different ﬂavors are well established and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='The approaches to TS forecasting can be split in a few distinct categories. The statistical model-\\ning approaches based on exponential smoothing and its different ﬂavors are well established and\\nare often considered a default choice in the industry (Holt, 1957; 2004; Winters, 1960). More\\nadvanced variations of exponential smoothing include the winner of M3 competition, the Theta\\nmethod (Assimakopoulos & Nikolopoulos, 2000) that decomposes the forecast into several theta-lines\\nand statistically combines them. The pinnacle of the statistical approach encapsulates ARIMA,\\nauto-ARIMA and in general, the uniﬁed state-space modeling approach, that can be used to ex-\\nplain and analyze all of the approaches mentioned above (see Hyndman & Khandakar (2008) for\\nan overview). More recently, ML/TS combination approaches started inﬁltrating the domain with\\ngreat success, showing promising results by using the outputs of statistical engines as features. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='an overview). More recently, ML/TS combination approaches started inﬁltrating the domain with\\ngreat success, showing promising results by using the outputs of statistical engines as features. In\\nfact, 2 out of top-5 entries in the M4 competition are approaches of this type, including the second\\nentry (Montero-Manso et al., 2019). The second entry computes the outputs of several statistical\\nmethods on the M4 dataset and combines them using gradient boosted tree (Chen & Guestrin, 2016).\\nSomewhat independently, the work in the modern deep learning TS forecasting developed based on\\nvariations of recurrent neural networks (Flunkert et al., 2017; Rangapuram et al., 2018b; Toubeau\\net al., 2019; Zia & Razzaq, 2018) being largely dominated by the electricity load forecasting in the\\nmulti-variate setup. A few earlier works explored the combinations of recurrent neural networks\\nwith dilation, residual connections and attention (Chang et al., 2017; Kim et al., 2017; Qin et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='multi-variate setup. A few earlier works explored the combinations of recurrent neural networks\\nwith dilation, residual connections and attention (Chang et al., 2017; Kim et al., 2017; Qin et al.,\\n2017). These served as a basis for the winner of the M4 competition (Smyl, 2020). The winning\\nentry combines a Holt-Winters style seasonality model with its parameters ﬁtted to a given TS via\\ngradient descent and a unique combination of dilation/residual/attention approaches for each forecast\\nhorizon. The resulting model is a hybrid model that architecturally heavily relies on a time-series\\nengine. It is hand crafted to each speciﬁc horizon of M4, making this approach hard to generalize to\\nother datasets.\\n5 E XPERIMENTAL RESULTS\\nOur key empirical results based on aggregate performance metrics over several datasets—M4 (M4\\nTeam, 2018b; Makridakis et al., 2018b), M3 (Makridakis & Hibon, 2000; Makridakis et al., 2018a)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Our key empirical results based on aggregate performance metrics over several datasets—M4 (M4\\nTeam, 2018b; Makridakis et al., 2018b), M3 (Makridakis & Hibon, 2000; Makridakis et al., 2018a)\\nand TOURISM (Athanasopoulos et al., 2011)—appear in Table 1. More detailed descriptions of the\\ndatasets are provided in Section 5.1 and Appendix A. For each dataset, we compare our results with\\nbest 5 entries for this dataset reported in the literature, according to the customary metrics speciﬁc to\\neach dataset (M4: OWA and sMAPE , M3: sMAPE , TOURISM : MAPE ). More granular dataset-speciﬁc\\nresults with data splits over forecast horizons and types of time series appear in respective appendices\\n(M4: Appendix C.1; M3: Appendix C.2; TOURISM : Appendix C.3).\\nIn Table 1, we study the performance of two N-BEATS conﬁgurations: generic (N-BEATS-G) and\\ninterpretable (N-BEATS-I), as well as N-BEATS-I+G (ensemble of all models from N-BEATS-G and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='In Table 1, we study the performance of two N-BEATS conﬁgurations: generic (N-BEATS-G) and\\ninterpretable (N-BEATS-I), as well as N-BEATS-I+G (ensemble of all models from N-BEATS-G and\\nN-BEATS-I).On M4 dataset, we compare against 5 representatives from the M4 competition (Makri-\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\ndakis et al., 2018b): each best in their respective model class. Pure ML is the submission by B. Trotta,\\nthe best entry among the 6 pure ML models. Statistical is the best pure statistical model by N.Z.\\nLegaki and K. Koutsouri. ML/TS combination is the model by P. Montero-Manso, T. Talagala, R.J.\\nHyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few statistical time\\nseries models. ProLogistica is the third entry in M4 based on the weighted ensemble of statistical\\nmethods. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020). On the M3 dataset,\\nwe compare against the Theta method (Assimakopoulos & Nikolopoulos, 2000), the winner of M3;\\nDOTA, a dynamically optimized Theta model (Fiorucci et al., 2016); EXP, the most resent statistical\\napproach and the previous state-of-the-art on M3 (Spiliotis et al., 2019); as well as ForecastPro, an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='approach and the previous state-of-the-art on M3 (Spiliotis et al., 2019); as well as ForecastPro, an\\noff-the-shelf forecasting software that is based on model selection between exponential smoothing,\\nARIMA and moving average (Athanasopoulos et al., 2011; Assimakopoulos & Nikolopoulos, 2000).\\nOn theTOURISM dataset, we compare against 3 statistical benchmarks (Athanasopoulos et al.,\\n2011): ETS, exponential smoothing with cross-validated additive/multiplicative model;Theta method;\\nForePro, same as ForecastProin M3; as well as top 2 entries from the TOURISM Kaggle competi-\\ntion (Athanasopoulos & Hyndman, 2011): Stratometrics, an unknown technique; LeeCBaker (Baker\\n& Howard, 2011), a weighted combination of Naïve, linear trend model, and exponentially weighted\\nleast squares regression trend.\\nAccording to Table 1, N-BEATS demonstrates state-of-the-art performance on three challenging\\nnon-overlapping datasets containing time series from very different domains, sampling frequencies'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='According to Table 1, N-BEATS demonstrates state-of-the-art performance on three challenging\\nnon-overlapping datasets containing time series from very different domains, sampling frequencies\\nand seasonalities. As an example, on M4 dataset, the OWA gap between N-BEATS and the M4\\nwinner (0.821 −0.795 = 0.026) is greater than the gap between the M4 winner and the second entry\\n(0.838 −0.821 = 0.017). Generic N-BEATS model uses as little prior knowledge as possible, with\\nno feature engineering, no scaling and no internal architectural components that may be considered\\nTS-speciﬁc. Thus the result in Table 1 leads us to the conclusion that DL does not need support\\nfrom the statistical approaches or hand-crafted feature engineering and domain knowledge to perform\\nextremely well on a wide array of TS forecasting tasks. On top of that, the proposed general\\narchitecture performs very well on three different datasets outperforming a wide variety of models,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='extremely well on a wide array of TS forecasting tasks. On top of that, the proposed general\\narchitecture performs very well on three different datasets outperforming a wide variety of models,\\nboth generic and manually crafted to respective dataset, including the winner of M4, a model\\narchitecturally adjusted by hand to each forecast-horizon subset of the M4 data.\\n5.1 D ATASETS\\nM4 (M4 Team, 2018b; Makridakis et al., 2018b) is the latest in an inﬂuential series of forecasting\\ncompetitions organized by Spyros Makridakis since 1982 (Makridakis et al., 1982). The 100k-series\\ndataset is large and diverse, consisting of data frequently encountered in business, ﬁnancial and\\neconomic forecasting, and sampling frequencies ranging from hourly to yearly. A table with summary\\nstatistics is presented in Appendix A.1, showing wide variability in TS characteristics.\\nM3 (Makridakis & Hibon, 2000) is similar in its composition to M4, but has a smaller overall scale'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='statistics is presented in Appendix A.1, showing wide variability in TS characteristics.\\nM3 (Makridakis & Hibon, 2000) is similar in its composition to M4, but has a smaller overall scale\\n(3003 time series total vs. 100k in M4). A table with summary statistics is presented in Appendix A.2.\\nOver the past 20 years, this dataset has supported signiﬁcant efforts in the design of more optimal\\nstatistical models, e.g. Theta and its variants (Assimakopoulos & Nikolopoulos, 2000; Fiorucci et al.,\\n2016; Spiliotis et al., 2019). Furthermore, a recent publication (Makridakis et al., 2018a) based on a\\nsubset of M3 presented evidence that ML models are inferior to the classical statistical models.\\nTOURISM (Athanasopoulos et al., 2011) dataset was released as part of the respective Kaggle\\ncompetition conducted by Athanasopoulos & Hyndman (2011). The data include monthly, quarterly\\nand yearly series supplied by both governmental tourism organizations (e.g. Tourism Australia, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='competition conducted by Athanasopoulos & Hyndman (2011). The data include monthly, quarterly\\nand yearly series supplied by both governmental tourism organizations (e.g. Tourism Australia, the\\nHong Kong Tourism Board and Tourism New Zealand) as well as various academics, who had used\\nthem in previous studies. A table with summary statistics is presented in Appendix A.3.\\n5.2 T RAINING METHODOLOGY\\nWe split each dataset into train, validation and test subsets. The test subset is the standard test set\\npreviously deﬁned for each dataset (M4 Team, 2018a; Makridakis & Hibon, 2000; Athanasopoulos\\net al., 2011). The validation and train subsets for each dataset are obtained by splitting their full train\\nsets at the boundary of the last horizon of each time series. We use the train and validation subsets to\\ntune hyperparameters. Once the hyperparameters are determined, we train the model on the full train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='tune hyperparameters. Once the hyperparameters are determined, we train the model on the full train\\nset and report results on the test set. Please refer to Appendix D for detailed hyperparameter settings\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nat the block level. N-BEATS is implemented and trained in Tensorﬂow (Abadi et al., 2015). We\\nshare parameters of the network across horizons, therefore we train one model per horizon for each\\ndataset. If every time series is interpreted as a separate task, this can be linked back to the multitask\\nlearning and furthermore to meta-learning (see discussion in Section 6), in which a neural network\\nis regularized by learning on multiple tasks to improve generalization. We would like to stress that\\nmodels for different horizons and datasets reuse the same architecture. Architectural hyperparameters\\n(width, number of layers, number of stacks, etc.) are ﬁxed to the same values across horizons and\\nacross datasets (see Appendix D). The fact that we can reuse architecture and even hyperparameters\\nacross horizons indicates that the proposed architecture design generalizes well across time series of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='across horizons indicates that the proposed architecture design generalizes well across time series of\\ndifferent nature. The same architecture is successfully trained on the M4 Monthly subset with 48k\\ntime series and the M3 Others subset with 174 time series. This is a much stronger result than e.g. the\\nresult of S. Smyl (Makridakis et al., 2018b) who had to use very different architectures hand crafted\\nfor different horizons.\\nTo update network parameters for one horizon, we sample train batches of ﬁxed size 1024. We pick\\n1024 TS ids from this horizon, uniformly at random with replacement. For each selected TS id we\\npick a random forecast point from the historical range of length LH immediately preceding the last\\npoint in the train part of the TS. LH is a cross-validated hyperparameter. We observed that for subsets\\nwith large number of time series it tends to be smaller and for subsets with smaller number of time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='with large number of time series it tends to be smaller and for subsets with smaller number of time\\nseries it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4LH is\\nequal to 1.5; and in moderate to small Weekly, Daily, Hourly subsets of M4LH is equal to 10. Given\\na sampled forecast point, we set one horizon worth of points following it to be the target forecast\\nwindow y and we set the history of points of one of lengths 2H,3H,..., 7H preceding it to be the\\ninput x to the network. We use the Adam optimizer with default settings and initial learning rate\\n0.001. While optimising the ensemble members relying on the minimization of sMAPE metric, we\\nstop the gradient ﬂows in the denominator to make training numerically stable. The neural network\\ntraining is run with early stopping and the number of batches is determined on the validation set. The\\nGPU based training of one ensemble member for entire M4 dataset takes between 30 min and 2 hours'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='training is run with early stopping and the number of batches is determined on the validation set. The\\nGPU based training of one ensemble member for entire M4 dataset takes between 30 min and 2 hours\\ndepending on neural network settings and hardware.\\n5.3 I NTERPRETABILITY RESULTS\\nFig. 2 studies the outputs of the proposed model in the generic and the interpretable conﬁgurations.\\nAs discussed in Section 3.3, to make the generic architecture presented in Fig. 1 interpretable, we\\nconstrain gθ in the ﬁrst stack to have the form of polynomial (2) while the second one has the form\\nof Fourier basis (3). Furthermore, we use the outputs of the generic conﬁguration of N-BEATS as\\ncontrol group (the generic model of 30 residual blocks depicted in Fig. 1 is divided into two stacks)\\nand we plot both generic (sufﬁx “-G”) and interpretable (sufﬁx “-I”) stack outputs side by side in\\nFig. 2. The outputs of generic model are arbitrary and non-interpretable: either trend or seasonality'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Fig. 2. The outputs of generic model are arbitrary and non-interpretable: either trend or seasonality\\nor both of them are present at the output of both stacks. The magnitude of the output (peak-to-peak)\\nis generally smaller at the output of the second stack. The outputs of the interpretable model exhibit\\ndistinct properties: the trend output is monotonic and slowly moving, the seasonality output is\\nregular, cyclical and has recurring ﬂuctuations. The peak-to-peak magnitude of the seasonality output\\nis signiﬁcantly larger than that of the trend, if signiﬁcant seasonality is present in the time series.\\nSimilarly, the peak-to-peak magnitude of trend output tends to be small when no obvious trend\\nis present in the ground truth signal. Thus the proposed interpretable architecture decomposes its\\nforecast into two distinct components. Our conclusion is that the outputs of the DL model can be\\nmade interpretable by encoding a sensible inductive bias in the architecture. Table 1 conﬁrms that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='forecast into two distinct components. Our conclusion is that the outputs of the DL model can be\\nmade interpretable by encoding a sensible inductive bias in the architecture. Table 1 conﬁrms that\\nthis does not result in performance drop.\\n6 D ISCUSSION : C ONNECTIONS TO META-LEARNING\\nMeta-learning deﬁnes an inner learning procedure and an outer learning procedure. The inner\\nlearning procedure is parameterized, conditioned or otherwise inﬂuenced by the outer learning\\nprocedure (Bengio et al., 1991). The prototypical inner vs. outer learning is individual learning in\\nthe lifetime of an animal vs. evolution of the inner learning procedure itself over many generations\\nof individuals. To see the two levels, it often helps to refer to two sets of parameters, the inner\\nparameters (e.g. synaptic weights) which are modiﬁed inside the inner learning procedure, and the\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n0 1 2 3 4 5\\nt\\n0.8\\n0.9\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90 STACK1-G\\n0 1 2 3 4 5\\nt\\n0.025\\n0.050\\n0.075 STACK2-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90\\n0.95\\nSTACK1-I\\n0 1 2 3 4 5\\nt\\n0.02\\n0.03\\n0.04\\n0.05 STACK2-I\\n0 2 4 6\\nt\\n0.85\\n0.90\\n0.95\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6\\nt\\n0.86\\n0.88\\n0.90 STACK1-G\\n0 2 4 6\\nt\\n0.025\\n0.000\\n0.025\\n0.050\\nSTACK2-G\\n0 2 4 6\\nt\\n0.88\\n0.89\\n0.90\\nSTACK1-I\\n0 2 4 6\\nt\\n0.05\\n0.00\\n0.05\\nSTACK2-I\\n0 5 10 15\\nt\\n0.4\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 5 10 15\\nt\\n0.8\\n0.9 STACK1-G\\n0 5 10 15\\nt\\n0.1\\n0.0\\nSTACK2-G\\n0 5 10 15\\nt\\n0.85\\n0.90\\nSTACK1-I\\n0 5 10 15\\nt\\n0.3\\n0.2\\n0.1\\n0.0 STACK2-I\\n0 2 4 6 8 10 12\\nt\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80\\nSTACK1-G\\n0 2 4 6 8 10 12\\nt\\n0.000\\n0.025\\n0.050\\n0.075\\nSTACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='STACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.002\\n0.001\\n STACK2-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.0003\\n0.0002\\n0.0001\\n STACK2-I\\n0 10 20 30 40\\nt\\n0.25\\n0.50\\n0.75\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n(a) Combined\\n0 10 20 30 40\\nt\\n0.2\\n0.4\\n0.6\\nSTACK1-G (b) Stack1-G\\n0 10 20 30 40\\nt\\n0.02\\n0.00\\nSTACK2-G (c) Stack2-G\\n0 10 20 30 40\\nt\\n0.36\\n0.38\\n0.40\\nSTACK1-I (d) StackT-I\\n0 10 20 30 40\\nt\\n0.2\\n0.0\\n0.2\\nSTACK2-I (e) StackS-I\\nFigure 2: The outputs of generic and the interpretable conﬁgurations, M4 dataset. Each row is one\\ntime series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='time series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\\nare normalized by the maximal value of the actual time series for convenience. Column (a) shows the\\nactual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\\nforecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\\nrespectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\\nand the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\\nouter parameters or meta-parameters (e.g. genes) which get modiﬁed only in the outer learning\\nprocedure.\\nN-BEATS can be cast as an instance of meta-learning by drawing the following parallels. The outer\\nlearning procedure is encapsulated in the parameters of the whole network, learned by gradient'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='N-BEATS can be cast as an instance of meta-learning by drawing the following parallels. The outer\\nlearning procedure is encapsulated in the parameters of the whole network, learned by gradient\\ndescent. The inner learning procedure is encapsulated in the set of basic building blocks and modiﬁes\\nthe expansion coefﬁcients θ f that basis gf takes as inputs. The inner learning proceeds through a\\nsequence of stages, each corresponding to a block within the stack of the architecture. Each of the\\nblocks can be thought of as performing the equivalent of an update step which gradually modiﬁes\\nthe expansion coefﬁcients θ f which eventually feed into gf in each block (which get added together\\nto form the ﬁnal prediction). The inner learning procedure takes a single history from a piece of a\\nTS and sees that history as a training set. It produces forward expansion coefﬁcients θ f (see Fig. 1),\\nwhich parametrically map inputs to predictions. In addition, each preceding block modiﬁes the input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='which parametrically map inputs to predictions. In addition, each preceding block modiﬁes the input\\nto the next block by producing backward expansion coefﬁcients θb, thus conditioning the learning\\nand the output of the next block. In the case of the interpretable model, the meta-parameters are only\\nin the FC layers because the gf ’s are ﬁxed. In the case of the generic model, the meta-parameters\\nalso include the V’s which deﬁne thegf non-parametrically. This point of view is further reinforced\\nby the results of the ablation study reported in Appendix B showing that increasing the number of\\nblocks in the stack, as well as the number of stacks improves generalization performance, and can be\\ninterpreted as more iterations of the inner learning procedure.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n7 C ONCLUSIONS\\nWe proposed and empirically validated a novel architecture for univariate TS forecasting. We showed\\nthat the architecture is general, ﬂexible and it performs well on a wide array of TS forecasting prob-\\nlems. We applied it to three non-overlapping challenging competition datasets: M4, M3 andTOURISM\\nand demonstrated state-of-the-art performance in two conﬁgurations: generic and interpretable. This\\nallowed us to validate two important hypotheses: (i) the generic DL approach performs exceptionally\\nwell on heterogeneous univariate TS forecasting problems using no TS domain knowledge, (ii) it is\\nviable to additionally constrain a DL model to force it to decompose its forecast into distinct human\\ninterpretable outputs. We also demonstrated that the DL models can be trained on multiple time series\\nin a multi-task fashion, successfully transferring and sharing individual learnings. We speculate that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='in a multi-task fashion, successfully transferring and sharing individual learnings. We speculate that\\nN-BEATS’s performance can be attributed in part to it carrying out a form of meta-learning, a deeper\\ninvestigation of which should be the subject of future work.\\nREFERENCES\\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\\nKudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike\\nSchuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent\\nVanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,\\nMartin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,\\nMartin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\\non heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from\\ntensorﬂow.org.\\nV . Assimakopoulos and K. Nikolopoulos. The theta model: a decomposition approach to forecasting.\\nInternational Journal of Forecasting, 16(4):521–530, 2000.\\nGeorge Athanasopoulos and Rob J. Hyndman. The value of feedback in forecasting competitions.\\nInternational Journal of Forecasting, 27(3):845–849, 2011.\\nGeorge Athanasopoulos, Rob J. Hyndman, Haiyan Song, and Doris C. Wu. The tourism forecasting\\ncompetition. International Journal of Forecasting, 27(3):822–844, 2011.\\nLee C. Baker and Jeremy Howard. Winning methods for forecasting tourism time series.International\\nJournal of Forecasting, 27(3):850–852, 2011.\\nYoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. InProceedings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Journal of Forecasting, 27(3):850–852, 2011.\\nYoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. InProceedings\\nof the International Joint Conference on Neural Networks, pp. II–A969, Seattle, USA, 1991.\\nChristoph Bergmeir, Rob J. Hyndman, and José M. Benítez. Bagging exponential smoothing methods\\nusing STL decomposition and Box–Cox transformation. International Journal of Forecasting, 32\\n(2):303–312, 2016.\\nLeo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, Aug 1996.\\nPhil Brierley. Winning methods for forecasting seasonal tourism time series. International Journal of\\nForecasting, 27(3):853–854, 2011.\\nShiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael\\nWitbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.\\nIn NIPS, pp. 77–87, 2017.\\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In ACM SIGKDD, pp.\\n785–794, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='In NIPS, pp. 77–87, 2017.\\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In ACM SIGKDD, pp.\\n785–794, 2016.\\nRobert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning. STL: A seasonal-\\ntrend decomposition procedure based on Loess (with discussion). Journal of Ofﬁcial Statistics, 6:\\n3–73, 1990.\\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.\\nuci.edu/ml.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, and Anne B. Koehler.\\nModels for optimising the Theta method and their relationship to state space models. International\\nJournal of Forecasting, 32(4):1151–1161, 2016.\\nValentin Flunkert, David Salinas, and Jan Gasthaus. DeepAR: Probabilistic forecasting with autore-\\ngressive recurrent networks. CoRR, abs/1704.04110, 2017.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In CVPR, pp. 770–778. IEEE Computer Society, 2016.\\nC. C. Holt. Forecasting trends and seasonals by exponentially weighted averages. Technical Report\\nONR memorandum no. 5, Carnegie Institute of Technology, Pittsburgh, PA, 1957.\\nCharles C. Holt. Forecasting seasonals and trends by exponentially weighted moving averages.\\nInternational Journal of Forecasting, 20(1):5–10, 2004.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Charles C. Holt. Forecasting seasonals and trends by exponentially weighted moving averages.\\nInternational Journal of Forecasting, 20(1):5–10, 2004.\\nGao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected\\nconvolutional networks. In CVPR, pp. 2261–2269. IEEE Computer Society, 2017.\\nRob Hyndman and Anne B. Koehler. Another look at measures of forecast accuracy. International\\nJournal of Forecasting, 22(4):679–688, 2006.\\nRob J Hyndman and Yeasmin Khandakar. Automatic time series forecasting: the forecast package\\nfor R. Journal of Statistical Software, 26(3):1–22, 2008.\\nChaman L. Jain. Answers to your forecasting questions. Journal of Business Forecasting, 36, Spring\\n2017.\\nKenneth B. Kahn. How to measure the impact of a forecast error on an enterprise? The Journal of\\nBusiness Forecasting Methods & Systems, 22(1), Spring 2003.\\nJaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. Residual lstm: Design of a deep recurrent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Business Forecasting Methods & Systems, 22(1), Spring 2003.\\nJaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. Residual lstm: Design of a deep recurrent\\narchitecture for distant speech recognition. In Interspeech 2017, pp. 1591–1595, 2017.\\nM4 Team. M4 dataset, 2018a. URLhttps://github.com/M4Competition/M4-methods/tree/\\nmaster/Dataset.\\nM4 Team. M4 competitor’s guide: prizes and rules, 2018b. URL www.m4.unic.ac.cy/\\nwp-content/uploads/2018/03/M4-CompetitorsGuide.pdf.\\nS Makridakis, E Spiliotis, and V Assimakopoulos. Statistical and machine learning forecasting\\nmethods: Concerns and ways forward. PLoS ONE, 13(3), 2018a.\\nSpyros Makridakis and Michèle Hibon. The M3-Competition: results, conclusions and implications.\\nInternational Journal of Forecasting, 16(4):451–476, 2000.\\nSpyros Makridakis, A Andersen, Robert Carbone, Robert Fildes, Michele Hibon, Rudolf\\nLewandowski, Joseph Newton, Emanuel Parzen, and Robert Winkler. The accuracy of extrapo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Spyros Makridakis, A Andersen, Robert Carbone, Robert Fildes, Michele Hibon, Rudolf\\nLewandowski, Joseph Newton, Emanuel Parzen, and Robert Winkler. The accuracy of extrapo-\\nlation (time series) methods: Results of a forecasting competition. Journal of forecasting, 1(2):\\n111–153, 1982.\\nSpyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The M4-Competition:\\nResults, ﬁndings, conclusion and way forward. International Journal of Forecasting , 34(4):\\n802–808, 2018b.\\nPablo Montero-Manso, George Athanasopoulos, Rob J Hyndman, and Thiyanga S Talagala.\\nFFORMA: Feature-based Forecast Model Averaging. International Journal of Forecasting, 2019.\\nto appear.\\nVinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\\nICML, pp. 807–814, 2010.\\nYao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison W. Cottrell. A\\ndual-stage attention-based recurrent neural network for time series prediction. In IJCAI-17, pp.\\n2627–2633, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='dual-stage attention-based recurrent neural network for time series prediction. In IJCAI-17, pp.\\n2627–2633, 2017.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 11, 'page_label': '12', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nSyama Sundar Rangapuram, Matthias Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim\\nJanuschowski. Deep state space models for time series forecasting. In NeurIPS, 2018a.\\nSyama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and\\nTim Januschowski. Deep state space models for time series forecasting. In NeurIPS 31, pp.\\n7785–7794, 2018b.\\nSlawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time\\nseries forecasting. International Journal of Forecasting, 36(1):75 – 85, 2020.\\nSlawek Smyl and Karthik Kuber. Data preprocessing and augmentation for multiple short time series\\nforecasting with recurrent neural networks. In 36th International Symposium on Forecasting, 2016.\\nEvangelos Spiliotis, Vassilios Assimakopoulos, and Konstantinos Nikolopoulos. Forecasting with a\\nhybrid method utilizing data smoothing, a variation of the theta method and shrinkage of seasonal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 11, 'page_label': '12', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Evangelos Spiliotis, Vassilios Assimakopoulos, and Konstantinos Nikolopoulos. Forecasting with a\\nhybrid method utilizing data smoothing, a variation of the theta method and shrinkage of seasonal\\nfactors. International Journal of Production Economics, 209:92–102, 2019.\\nA. A. Syntetos, J. E. Boylan, and J. D. Croston. On the categorization of demand patterns. Journal of\\nthe Operational Research Society, 56(5):495–503, 2005.\\nJ. Toubeau, J. Bottieau, F. Vallée, and Z. De Grève. Deep learning-based multivariate probabilistic\\nforecasting for short-term scheduling in power markets. IEEE Transactions on Power Systems, 34\\n(2):1203–1215, March 2019.\\nU.S. Census Bureau. Reference manual for the X-13ARIMA-SEATS Program, version 1.0, 2013.\\nURL http://www.census.gov/ts/x13as/docX13AS.pdf.\\nYuyang Wang, Alex Smola, Danielle C. Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.\\nDeep factors for forecasting. In ICML, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 11, 'page_label': '12', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='URL http://www.census.gov/ts/x13as/docX13AS.pdf.\\nYuyang Wang, Alex Smola, Danielle C. Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.\\nDeep factors for forecasting. In ICML, 2019.\\nPeter R. Winters. Forecasting sales by exponentially weighted moving averages. Management\\nScience, 6(3):324–342, 1960.\\nHsiang-Fu Yu, Nikhil Rao, and Inderjit S. Dhillon. Temporal regularized matrix factorization for\\nhigh-dimensional time series prediction. In NIPS, 2016.\\nTehseen Zia and Saad Razzaq. Residual recurrent highway networks for learning deep sequence\\nprediction models. Journal of Grid Computing, Jun 2018.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 2: Composition of the M4 dataset: the number of time series based on their sampling frequency\\nand type.\\nFrequency / Horizon\\nType Yearly/6 Qtly/8 Monthly/18 Wkly/13 Daily/14 Hrly/48 Total\\nDemographic 1,088 1,858 5,728 24 10 0 8,708\\nFinance 6,519 5,305 10,987 164 1,559 0 24,534\\nIndustry 3,716 4,637 10,017 6 422 0 18,798\\nMacro 3,903 5,315 10,016 41 127 0 19,402\\nMicro 6,538 6,020 10,975 112 1,476 0 25,121\\nOther 1,236 865 277 12 633 414 3,437\\nTotal 23,000 24,000 48,000 359 4,227 414 100,000\\nMin. Length 19 24 60 93 107 748\\nMax. Length 841 874 2812 2610 9933 1008\\nMean Length 37.3 100.2 234.3 1035.0 2371.4 901.9\\nSD Length 24.5 51.1 137.4 707.1 1756.6 127.9\\n% Smooth 82% 89% 94% 84% 98% 83%\\n% Erratic 18% 11% 6% 16% 2% 17%\\nA D ATASET DETAILS\\nA.1 M4 D ATASET DETAILS\\nTable 2 outlines the composition of the M4 dataset across domains and forecast horizons by listing the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='% Erratic 18% 11% 6% 16% 2% 17%\\nA D ATASET DETAILS\\nA.1 M4 D ATASET DETAILS\\nTable 2 outlines the composition of the M4 dataset across domains and forecast horizons by listing the\\nnumber of time series based on their frequency and type (M4 Team, 2018b). The M4 dataset is large\\nand diverse: all forecast horizons are composed of heterogeneous time series types (with exception of\\nHourly) frequently encountered in business, ﬁnancial and economic forecasting. Summary statistics\\non series lengths are also listed, showing wide variability therein, as well as a characterization (smooth\\nvs erratic) that follows Syntetos et al. (2005), and is based on the squared coefﬁcient of variation of\\nthe series. All series have positive observed values at all time-steps; as such, none can be considered\\nintermittent or lumpy per Syntetos et al. (2005).\\nA.2 M3 D ATASET DETAILS\\nTable 3 outlines the composition of the M3 dataset across domains and forecast horizons by listing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='intermittent or lumpy per Syntetos et al. (2005).\\nA.2 M3 D ATASET DETAILS\\nTable 3 outlines the composition of the M3 dataset across domains and forecast horizons by listing\\nthe number of time series based on their frequency and type (Makridakis & Hibon, 2000). The\\nM3 is smaller than the M4, but it is still large and diverse: all forecast horizons are composed\\nof heterogeneous time series types frequently encountered in business, ﬁnancial and economic\\nforecasting. Summary statistics on series lengths are also listed, showing wide variability in length,\\nas well as a characterization ( smooth vs erratic) that follows Syntetos et al. (2005), and is based\\non the squared coefﬁcient of variation of the series. All series have positive observed values at all\\ntime-steps; as such, none can be considered intermittent or lumpy per Syntetos et al. (2005).\\nA.3 TOURISM DATASET DETAILS\\nTable 4 outlines the composition of the TOURISM dataset across forecast horizons by listing the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='A.3 TOURISM DATASET DETAILS\\nTable 4 outlines the composition of the TOURISM dataset across forecast horizons by listing the\\nnumber of time series based on their frequency. Summary statistics on series lengths are listed,\\nshowing wide variability in length. All series have positive observed values at all time-steps. In\\ncontrast to M4 and M3 datasets, TOURISM includes a much higher fraction of erratic series.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 13, 'page_label': '14', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 3: Composition of the M3 dataset: the number of time series based on their sampling frequency\\nand type.\\nFrequency / Horizon\\nType Yearly/6 Quarterly/8 Monthly/18 Other/8 Total\\nDemographic 245 57 111 0 413\\nFinance 58 76 145 29 308\\nIndustry 102 83 334 0 519\\nMacro 83 336 312 0 731\\nMicro 146 204 474 4 828\\nOther 11 0 52 141 204\\nTotal 645 756 1,428 174 3,003\\nMin. Length 20 24 66 71\\nMax. Length 47 72 144 104\\nMean Length 28.4 48.9 117.3 76.6\\nSD Length 9.9 10.6 28.5 10.9\\n% Smooth 90% 99% 98% 100%\\n% Erratic 10% 1% 2% 0%\\nTable 4: Composition of the TOURISM dataset: the number of time series based on their sampling\\nfrequency.\\nFrequency / Horizon\\nYearly/4 Quarterly/8 Monthly/24 Total\\n518 427 366 1,311\\nMin. Length 11 30 91\\nMax. Length 47 130 333\\nMean Length 24.4 99.6 298\\nSD Length 5.5 20.3 55.7\\n% Smooth 77% 61% 49%\\n% Erratic 23% 39% 51%\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 5: sMAPE on the validation set, generic ar-\\nchitecture. sMAPE for varying number of stacks,\\neach having one residual block.\\nStacks s MAPE\\n1 11.154\\n3 11.061\\n9 10.998\\n18 10.950\\n30 10.937\\nTable 6: sMAPE on the validation set, inter-\\npretable architecture. Ablation of the synergy\\nof the layers with different basis functions and\\nmulti-block stack gain.\\nDetrend Seasonality s MAPE\\n0 2 11.189\\n2 0 11.572\\n1 1 11.040\\n3 3 10.986\\nB A BLATION STUDIES\\nB.1 L AYER STACKING AND BASIS SYNERGY\\nWe performed an ablation study on the validation set, using sMAPE metric as performance criterion.\\nWe addressed two speciﬁc questions with this study. First, Is stacking layers helpful? Second, Does\\nthe architecture based on the combination of layers with different basis functions results in better\\nperformance than the architecture using only one layer type?\\nLayer stacking. We start our study with the generic architecture that consists of stacks of one'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='performance than the architecture using only one layer type?\\nLayer stacking. We start our study with the generic architecture that consists of stacks of one\\nresidual block of 5 FC layers each of the form Fig. 1 and we increase the number of stacks. Results\\npresented in Table 5 conﬁrm that increasing the number of stacks decreases error and at certain point\\nthe gain saturates. We would like to mention that the network having 30 stack of depth 5 is in fact a\\nvery deep network of total depth 150 layers.\\nBasis synergy. Stacking works well for the interpretable architecture as can be seen in Table 6\\ndepicting the results of ablating the interpretable architecture conﬁguration. Here we experiment\\nwith the architecture that is composed of 2 stacks, stack one is trend model and stack two is the\\nseasonality model. Each stack has variable number of residual blocks and each residual block has 5\\nFC layers. We found that this architecture works best when all weights are shared within stack. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='seasonality model. Each stack has variable number of residual blocks and each residual block has 5\\nFC layers. We found that this architecture works best when all weights are shared within stack. We\\nclearly see that increasing the number of layers improves performance. The largest network is 60\\nlayers deep. On top of that, we observe that the architecture that consists of stacks based on different\\nbasis functions wins over the architecture based on the same stack. It looks like chaining stacks of\\ndifferent nature results in synergistic effects. This is logical as function classes that can be modelled\\nby trend and seasonality stacks have small overlap.\\nB.2 E NSEMBLE SIZE\\nFigure 3 demonstrates that increasing the ensemble size results in improved performance. Most\\nimportantly, according to Figure 3, N-BEATS achieves state-of-the-art performance even if compara-\\ntively small ensemble size of 18 models is used. Therefore, computational efﬁciency of N-BEATS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='importantly, according to Figure 3, N-BEATS achieves state-of-the-art performance even if compara-\\ntively small ensemble size of 18 models is used. Therefore, computational efﬁciency of N-BEATS\\ncan be traded very effectively for performance and there is no over-reliance of the results on large\\nensemble size.\\nB.3 D OUBLY RESIDUAL STACKING\\nIn Section 3.2 we described the proposed doubly residual stacking (DRESS) principle, which is the\\ntopological foundation of N-BEATS. The topology is based on both (i) running a residual backcast\\nconnection and (ii) producing partial block-level forecasts that are further aggregated at stack and\\nmodel levels to produce the ﬁnal model-level forecast. In this section we conduct a study to conﬁrm\\nthe accuracy effectiveness of this topology compared to several alternatives. The methodology\\nunderlying this study is that we remove either the backcast or partial forecast links or both and track'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='the accuracy effectiveness of this topology compared to several alternatives. The methodology\\nunderlying this study is that we remove either the backcast or partial forecast links or both and track\\nhow this affects the forecasting metrics. We keep the number of parameters in the network for each\\nof the architectural alternatives ﬁxed by using the same number of layers in the network (we used\\ndefault hyperparameter settings reported in Table 18). The architectural alternatives are depicted in\\nFigure 4 and described in detail below.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 15, 'page_label': '16', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n18 36 90 180\\nEnsemble Size\\n0.797\\n0.798\\n0.799\\n0.800\\n0.801\\n0.802OWA\\nFigure 3: M4 test performance (OWA) as a function of ensemble size, based on N-BEATS-G. This\\nﬁgure shows that N-BEATS loses less than 0.5% in terms of OWA performance even if 10 times\\nsmaller ensemble size is used.\\nN-BEATS-DRESSis depicted in Fig. 4a. This is the default conﬁguration of N-BEATS using doubly\\nresidual stacking described in Section 3.2.\\nPARALLEL is depicted in Fig. 4b. This is the alternative where the backward residual connection is\\ndisabled and the overall model input is fed to every block. The blocks then forecast in parallel using\\nthe same input and their individual outputs are summed to make the ﬁnal forecast.\\nNO-RESIDUAL is depicted in Fig. 4c. This is the alternative where the backward residual connection\\nis disabled. Unlike PARALLEL, in this case the backcast forecast of the previous block is fed as input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 15, 'page_label': '16', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='is disabled. Unlike PARALLEL, in this case the backcast forecast of the previous block is fed as input\\nto the next block. Unlike the usual feed-forward network, in the NO-RESIDUAL architecture, each\\nblock makes a partial forecast and their individual outputs are summed to make the ﬁnal forecast.\\nLAST-FORWARDis depicted in Fig. 4d. This is the alternative where the backward residual\\nconnection is active, however the model level forecast is derived only from the last block. So, the\\npartial forward forecasts are disabled. This is the architecture that is closest to the classical residual\\nnetwork.\\nNO-RESIDUAL-LAST-FORWARDis depicted in Fig. 4f. This is the alternative where both\\nbackward residual and the partial forward connections are disabled. This is therefore a simple\\nfeed-forward network, but very deep.\\nThe quantitative ablation study results on the M4 dataset are reported in Tables 7–10. N-BEATS-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 15, 'page_label': '16', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='feed-forward network, but very deep.\\nThe quantitative ablation study results on the M4 dataset are reported in Tables 7–10. N-BEATS-\\nDRESS model is essentially N-BEATS model in this study. For this study we used ensemble size\\nof 18. Since the ensemble size is 18 for N-BEATS-DRESS, as opposed to 180 used for N-BEATS,\\nthe OWA metric reported in Table 9 for N-BEATS-DRESS is higher than the OWA reported for\\nN-BEATS-G in Table 12. Note that both results align well withOWA reported in Figure 3 for different\\nensemble sizes, as part of the ensemble size ablation conducted in Section B.2.\\nThe results presented in Tables 7–10 demonstrate that the doubly residual stacking topology provides\\na clear overall advantage over the alternative architectures in which either backcast residual links or\\nthe partial forward forecast links are disabled.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 16, 'page_label': '17', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input\\nStack output\\n(to next stack)\\n(a) N-BEATS-DRESS\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n+\\nStack output\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input (b) PARALLEL\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n+\\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input (c) NO-RESIDUAL\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\nGlobal forecast\\n(model output)Model Input\\n(d) LAST-FORW ARD\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\nStack residual\\n(to next stack)\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\nGlobal forecast\\n(model output)Model Input (e) NO-RESIDUAL-LAST-\\nFORW ARD\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 16, 'page_label': '17', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Stack\\nforecast\\nStack 1\\nStack 2\\nStack M \\nGlobal forecast\\n(model output)Model Input (e) NO-RESIDUAL-LAST-\\nFORW ARD\\nBlock 1\\nBlock 2\\nBlock\\xa0K \\n–\\n–\\n–\\n+\\nStack Input\\nStack\\nforecast\\nStack 1\\nStack 2\\nStack M \\n+\\nGlobal forecast\\n(model output)Model Input\\nStack output\\n(to next stack)\\nModel Input\\n(f) RESIDUAL-INPUT\\nFigure 4: The architectural conﬁgurations used in the ablation study of the doubly residual stack.\\nSymbol ⋄denotes unconnected output.\\nTable 7: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\\nthe ensemble of 18 generic models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-G 13.279 9.558 12.510 3.691 11.538\\nNO-RESIDUAL-G 13.195 9.555 12.451 3.759 11.493\\nLAST-FORW ARD-G 13.200 9.322 12.352 3.703 11.387\\nNO-RESIDUAL-LAST-FORW ARD-G 15.386 11.346 15.282 6.673 13.931\\nRESIDUAL-INPUT-G 13.264 9.545 12.316 3.692 11.438\\nN-BEATS-DRESS-G 13.211 9.217 12.122 3.636 11.251'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 16, 'page_label': '17', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='NO-RESIDUAL-LAST-FORW ARD-G 15.386 11.346 15.282 6.673 13.931\\nRESIDUAL-INPUT-G 13.264 9.545 12.316 3.692 11.438\\nN-BEATS-DRESS-G 13.211 9.217 12.122 3.636 11.251\\nTable 8: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\\nthe ensemble of 18 interpretable models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-I 13.207 9.530 12.500 3.710 11.510\\nNO-RESIDUAL-I 13.075 9.707 12.708 4.007 11.637\\nLAST-FORW ARD-I 13.168 9.547 12.111 3.599 11.313\\nNO-RESIDUAL-LAST-FORW ARD-I 13.067 10.207 15.177 4.912 12.986\\nRESIDUAL-INPUT-I 13.104 9.716 12.814 4.005 11.697\\nN-BEATS-DRESS-I 13.155 9.286 12.009 3.642 11.201\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 17, 'page_label': '18', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 9: Performance on the M4 test set, OWA. Lower values are better. The results are obtained on\\nthe ensemble of 18 generic models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-G 0.780 0.832 0.852 0.844 0.822\\nNO-RESIDUAL-G 0.774 0.831 0.851 0.853 0.819\\nLAST-FORW ARD-G 0.774 0.808 0.840 0.846 0.811\\nNO-RESIDUAL-LAST-FORW ARD-G 0.948 1.029 1.095 1.296 1.030\\nRESIDUAL-INPUT-G 0.779 0.831 0.840 0.844 0.817\\nN-BEATS-DRESS-G 0.776 0.800 0.823 0.835 0.803\\nTable 10: Performance on the M4 test set, OWA. Lower values are better. The results are obtained on\\nthe ensemble of 18 interpretable models.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nPARALLEL-I 0.776 0.831 0.857 0.845 0.821\\nNO-RESIDUAL-I 0.769 0.848 0.886 0.886 0.833\\nLAST-FORW ARD-I 0.773 0.836 0.825 0.817 0.808\\nNO-RESIDUAL-LAST-FORW ARD-I 0.771 0.900 1.085 1.016 0.922\\nRESIDUAL-INPUT-I 0.771 0.848 0.892 0.887 0.836'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 17, 'page_label': '18', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='NO-RESIDUAL-I 0.769 0.848 0.886 0.886 0.833\\nLAST-FORW ARD-I 0.773 0.836 0.825 0.817 0.808\\nNO-RESIDUAL-LAST-FORW ARD-I 0.771 0.900 1.085 1.016 0.922\\nRESIDUAL-INPUT-I 0.771 0.848 0.892 0.887 0.836\\nN-BEATS-DRESS-I 0.771 0.805 0.819 0.836 0.800\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 11: Performance on the M4 test set, sMAPE . Lower values are better. Red – second best.\\nYearly Quarterly Monthly Others Average\\n(23k) (24k) (48k) (5k) (100k)\\nBest pure ML 14.397 11.031 13.973 4.566 12.894\\nBest statistical 13.366 10.155 13.002 4.682 11.986\\nBest ML/TS combination 13.528 9.733 12.639 4.118 11.720\\nDL/TS hybrid, M4 winner 13.176 9.679 12.126 4.014 11.374\\nN-BEATS-G 13.023 9.212 12.048 3.574 11.168\\nN-BEATS-I 12.924 9.287 12.059 3.684 11.174\\nN-BEATS-I+G 12.913 9.213 12.024 3.643 11.135\\nTable 12: Performance on the M4 test set, OWA and M4 rank. Lower values are better. Red – second\\nbest.\\nYearly Quarterly Monthly Others Average Rank\\n(23k) (24k) (48k) (5k) (100k)\\nBest pure ML 0.859 0.939 0.941 0.991 0.915 23\\nBest statistical 0.788 0.898 0.905 0.989 0.861 8\\nBest ML/TS combination 0.799 0.847 0.858 0.914 0.838 2\\nDL/TS hybrid, M4 winner 0.778 0.847 0.836 0.920 0.821 1\\nN-BEATS-G 0.765 0.800 0.820 0.822 0.797'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Best statistical 0.788 0.898 0.905 0.989 0.861 8\\nBest ML/TS combination 0.799 0.847 0.858 0.914 0.838 2\\nDL/TS hybrid, M4 winner 0.778 0.847 0.836 0.920 0.821 1\\nN-BEATS-G 0.765 0.800 0.820 0.822 0.797\\nN-BEATS-I 0.758 0.807 0.824 0.849 0.798\\nN-BEATS-I+G 0.758 0.800 0.819 0.840 0.795\\nC D ETAILED EMPIRICAL RESULTS\\nC.1 D ETAILED RESULTS : M4 D ATASET\\nTables 11 and 12 present our key quantitative empirical results showing that the proposed model\\nachieves the state of the art performance on the challenging M4 benchmark. We study the performance\\nof two model conﬁgurations: generic (Ours-G) and interpretable (Ours-I), as well as Ours-I+G\\n(ensemble of all models from Ours-G and Ours-I). We compare against 4 representatives from the\\nM4 competition: each best in their respective model class. Best pure ML is the submission by B.\\nTrotta, the best entry among the 6 pure ML models. Best statistical is the best pure statistical model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='M4 competition: each best in their respective model class. Best pure ML is the submission by B.\\nTrotta, the best entry among the 6 pure ML models. Best statistical is the best pure statistical model\\nby N.Z. Legaki and K. Koutsouri. Best ML/TS combination is the model by P. Montero-Manso, T.\\nTalagala, R.J. Hyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few\\nstatistical time series models. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020).\\nN-BEATS outperforms all other approaches on all the studied subsets of time series. The average\\nOWA gap between our generic model and the M4 winner (0.821 −0.795 = 0.026) is greater than the\\ngap between the M4 winner and the second entry (0.838 −0.821 = 0.017).\\nA more granular and detailed statistical analysis of our results on M4 is provided in Table 13. This\\ntable ﬁrst presents the sMAPE for N-BEATS, decomposed by M4 time series sub-type and sampling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='A more granular and detailed statistical analysis of our results on M4 is provided in Table 13. This\\ntable ﬁrst presents the sMAPE for N-BEATS, decomposed by M4 time series sub-type and sampling\\nfrequency (upper part). Then (lower part), it shows the average sMAPE difference between the\\nN-BEATS results and the M4 winner (TS/DL hybrid by S. Smyl), adding the standard error of that\\ndifference (in parentheses); bold entries indicate statistical signiﬁcance at the 99% level based on a\\ntwo-sided paired t-test.\\nWe note that each cross-section of the M4 dataset into horizon and type may be regarded as an\\nindependent mini-dataset. We observe that over those mini-datasets there is a preponderance of\\nstatistically signiﬁcant differences between N-BEATS and Smyl (18 cases out of 31) to the advantage\\nof N-BEATS. This provides evidence that (i) the improvement observed on average in Tables 11\\nand 12 is statistically signiﬁcant and consistent over smaller subsets of M4 and (ii) N-BEATS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='of N-BEATS. This provides evidence that (i) the improvement observed on average in Tables 11\\nand 12 is statistically signiﬁcant and consistent over smaller subsets of M4 and (ii) N-BEATS\\ngeneralizes well over time series of different types and sampling frequencies.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 19, 'page_label': '20', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 13: Performance decomposition on non-overlapping subsets of the M4 test set and comparison\\nwith the Smyl model results.\\nDemographic Finance Industry Macro Micro Other\\nsMAPE per M4 series type and sampling frequency\\nYearly 8 .931 13 .741 16 .317 13 .327 10 .489 13 .320\\nQuarterly 9 .219 10 .787 8 .628 8 .576 9 .264 6 .250\\nMonthly 4 .357 13 .353 12 .657 12 .571 13 .627 11 .595\\nWeekly 4 .580 3 .004 9 .258 7 .220 10 .425 6 .183\\nDaily 6 .351 3 .467 3 .835 2 .525 2 .299 2 .885\\nHourly 8 .197\\nAverage sMAPE difference vs Smyl model, computed as N-BEATS – Smyl.\\nStandard error of the mean displayed in parenthesis.\\nBold entries are signiﬁcant at the 99% level (2-sided paired t-test).\\nYearly −0.749 −0.337 −0.065 −0.386 −0.168 −0.157\\n(0.119) ( 0.065) ( 0.087) ( 0.085) ( 0.056) ( 0.140)\\nQuarterly −0.651 −0.281 −0.328 −0.712 −0.523 −0.029\\n(0.085) ( 0.047) ( 0.043) ( 0.060) ( 0.051) ( 0.083)\\nMonthly −0.185 −0.379 −0.419 0.089 0.338 −0.279'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 19, 'page_label': '20', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Quarterly −0.651 −0.281 −0.328 −0.712 −0.523 −0.029\\n(0.085) ( 0.047) ( 0.043) ( 0.060) ( 0.051) ( 0.083)\\nMonthly −0.185 −0.379 −0.419 0.089 0.338 −0.279\\n(0.023) ( 0.034) ( 0.036) ( 0.039) ( 0.034) ( 0.162)\\nWeekly −0.336 −1.075 −0.937 −1.627 −3.029 −1.193\\n(0.270) ( 0.221) ( 1.399) ( 0.770) ( 0.378) ( 0.772)\\nDaily 0 .191 −0.098 −0.124 −0.026 −0.367 −0.037\\n(0.231) ( 0.018) ( 0.025) ( 0.057) ( 0.013) ( 0.015)\\nHourly −1.132\\n(0.163)\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 14: Performance on the M3 test set, Average sMAPE , aggregate over all forecast horizons\\n(Yearly: 1-6, Quarterly: 1-8, Monthly: 1-18, Other: 1-8, Average: 1-18). Lower values are better.\\nRed – second best. †Numbers are computed by us.\\nYearly Quarterly Monthly Others Average\\n(645) (756) (1428) (174) (3003)\\nNaïve2 17.88 9.95 16.91 6.30 15.47\\nARIMA (B–J automatic) 17.73 10.26 14.81 5.06 14.01\\nComb S-H-D 17.07 9.22 14.48 4.56 13.52\\nForecastPro 17.14 9.77 13.86 4.60 13.19\\nTheta 16.90 8.96 13.85 4.41 13.01\\nDOTM (Fiorucci et al., 2016) 15.94 9.28 13.74 4.58 12.90\\nEXP (Spiliotis et al., 2019) 16.39 8.98 13.43 5.46 12 .71†\\nLGT (Smyl & Kuber, 2016) 15.23 n/a n/a 4.26 n/a\\nBaggedETS.BC (Bergmeir et al., 2016) 17.49 9.89 13.74 n/a n/a\\nN-BEATS-G 16.2 8.92 13.19 4.19 12.47\\nN-BEATS-I 15.84 9.03 13.15 4.30 12.43\\nN-BEATS-I+G 15.93 8.84 13.11 4.24 12.37\\nC.2 D ETAILED RESULTS : M3 D ATASET'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='N-BEATS-G 16.2 8.92 13.19 4.19 12.47\\nN-BEATS-I 15.84 9.03 13.15 4.30 12.43\\nN-BEATS-I+G 15.93 8.84 13.11 4.24 12.37\\nC.2 D ETAILED RESULTS : M3 D ATASET\\nResults for M3 dataset are provided in Table 14. The performance metric is calculated using the\\nearlier version of sMAPE , deﬁned speciﬁcally for the M3 competition:1\\nsMAPE = 200\\nH\\nH\\n∑\\ni=1\\n|yT +i −ˆyT +i|\\nyT +i + ˆyT +i\\n. (4)\\nFor some of the methods, either average sMAPE was not reported or sMAPE for some of the splits was\\nnot reported in their respective publications. Below, we list those cases. BaggedETS.BC (Bergmeir\\net al., 2016) has not reported numbers on Others. LGT (Smyl & Kuber, 2016) did not report results on\\nMonthly and Quarterly data. According to the authors, the underlying RNN had problems dealing with\\nraw seasonal data, the ETS based pre-processing was not effective and the LGT pre-processing was\\nnot computationally feasible given comparatively large number of time series and their comparatively'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='raw seasonal data, the ETS based pre-processing was not effective and the LGT pre-processing was\\nnot computationally feasible given comparatively large number of time series and their comparatively\\nlarge length (Smyl & Kuber, 2016). Finally, EXP (Spiliotis et al., 2019) reported average performance\\ncomputed using a different methodology than the default M3 and M4 methodology (source: personal\\ncommunication with the authors). For the latter method we recomputed the Average sMAPE based on\\nthe previously reported Yearly, Quarterly and Monthly splits. To calculate it, we follow the M3, M4\\nand TOURISM competition methodology and compute the average metric as the average over all time\\nseries and over all forecast horizons. Given the performance metric values aggregated over Yearly,\\nQuarterly and Monthly splits, the average can be computed straightforwardly as:\\nsMAPE Average = NYear\\nNTot\\nsMAPE Year+NQuart\\nNTot\\nsMAPE Quart +NMonth\\nNTot\\nsMAPE Month +NOthers\\nNTot\\nsMAPE Others .\\n(5)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Quarterly and Monthly splits, the average can be computed straightforwardly as:\\nsMAPE Average = NYear\\nNTot\\nsMAPE Year+NQuart\\nNTot\\nsMAPE Quart +NMonth\\nNTot\\nsMAPE Month +NOthers\\nNTot\\nsMAPE Others .\\n(5)\\nHere NTot = NYear +NQuart +NMonth +NOthers and NYear = 6 ×645,NQuart = 8 ×756,NMonth = 18 ×\\n1428,NOthers = 8 ×174. It is clear that for each split, its N is the product of its respective number of\\ntime series and its largest forecast horizon.\\n1With minor differences compared to the sMAPE deﬁnition used for M4. Please refer to Appendix A\\nin (Makridakis & Hibon, 2000) for the mathematical deﬁnition.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 15: Performance on the TOURISM test set, Average MAPE , aggregate over all forecast horizons\\n(Yearly: 1-4, Quarterly: 1-8, Monthly: 1-24, Average: 1-24). Lower values are better. Red – second\\nbest.\\nYearly Quarterly Monthly Average\\n(518) (427) (366) (1311)\\nStatistical benchmarks (Athanasopoulos et al., 2011)\\nSNaïve 23.61 16.46 22.56 21.25\\nTheta 23.45 16.15 22.11 20.88\\nForePro 26.36 15.72 19.91 19.84\\nETS 27.68 16.05 21.15 20.88\\nDamped 28.15 15.56 23.47 22.26\\nARIMA 28.03 16.23 21.13 20.96\\nKaggle competitors (Athanasopoulos & Hyndman, 2011)\\nSaliMali n/a 14.83 19.64 n/a\\nLeeCBaker 22.73 15.14 20.19 19.35\\nStratometrics 23.15 15.14 20.37 19.52\\nRobert n/a 14.96 20.28 n/a\\nIdalgo n/a 15.07 20.55 n/a\\nN-BEATS-G (Ours) 21.67 14.71 19.17 18.47\\nN-BEATS-I (Ours) 21.55 15.22 19.82 18.97\\nN-BEATS-I+G (Ours) 21.44 14.78 19.29 18.52\\nC.3 D ETAILED RESULTS : TOURISM DATASET'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Idalgo n/a 15.07 20.55 n/a\\nN-BEATS-G (Ours) 21.67 14.71 19.17 18.47\\nN-BEATS-I (Ours) 21.55 15.22 19.82 18.97\\nN-BEATS-I+G (Ours) 21.44 14.78 19.29 18.52\\nC.3 D ETAILED RESULTS : TOURISM DATASET\\nDetailed results for the TOURISM competition dataset are provided in Table 15. The respective Kaggle\\ncompetition was divided into two parts: (i) Yearly time series forecasting and (ii) Quarterly/Monthly\\ntime series forecasting (Athanasopoulos & Hyndman, 2011). Some of the participants chose to\\ntake part only in the second part. Therefore, In addition to entries present in Table 1, we report\\ncompetitors from (Athanasopoulos & Hyndman, 2011) that have missing results in Yearly compe-\\ntition. In particular, SaliMali team is the winner of the Quarterly/Monthly time series forecasting\\ncompetition (Brierley, 2011). Their approach is based on a weighted ensemble of statistical methods.\\nTeams Robert and Idalgo used unknown approaches. We can see from Table 15 that N-BEATS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='competition (Brierley, 2011). Their approach is based on a weighted ensemble of statistical methods.\\nTeams Robert and Idalgo used unknown approaches. We can see from Table 15 that N-BEATS\\nachieves state-of-the-art performance on all subsets of TOURISM dataset. On average, it is state of the\\nart and it gains 4.2% over the best-known approach LeeCBaker, and 11.5% over auto-ARIMA.\\nThe average metrics have not been reported in the original competition results (Athanasopoulos et al.,\\n2011; Athanasopoulos & Hyndman, 2011). Therefore, in Table 15, we present the Average MAPE\\nmetric calculated by us based on the previously reported Yearly, Quarterly and Monthly splits. To\\ncalculate it, we follow the M4 competition methodology and compute the average metric as the\\naverage over all time series and over all forecast horizons. Given the performance metric values\\naggregated over Yearly, Quarterly and Monthly splits, the average can be computed straightforwardly\\nas:\\nMAPE Average = NYear\\nNTot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='aggregated over Yearly, Quarterly and Monthly splits, the average can be computed straightforwardly\\nas:\\nMAPE Average = NYear\\nNTot\\nMAPE Year+NQuart\\nNTot\\nMAPE Quart +NMonth\\nNTot\\nMAPE Month . (6)\\nHere NTot = NYear +NQuart +NMonth and NYear = 4 ×518,NQuart = 8 ×427,NMonth = 24 ×366. It is\\nclear that for each split, its N is the product of its respective number of time series and its largest\\nforecast horizon.\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nC.4 D ETAILED RESULTS : ELECTRICITY AND TRAFFIC DATASETS\\nIn this experiment we are comparing the performances of MatFact (Yu et al., 2016), DeepAR (Flunkert\\net al., 2017) (Amazon Labs), Deep State (Rangapuram et al., 2018a) (Amazon Labs), Deep Fac-\\ntors (Wang et al., 2019) (Amazon Labs), and N-BEATS models on ELECTRICITY 2 (Dua & Graff,\\n2017) and TRAFFIC 3 (Dua & Graff, 2017) datasets. The results are presented in in Table 16.\\nBoth datasets are aggregated to hourly data, but using different aggregation operations: sum for\\nELECTRICITY and mean for TRAFFIC . The hourly aggregation is done so that all the points available\\nin (h −1 : 00,h : 00] hours are aggregated to hour h, thus if original dataset starts on 2011-01-01\\n00:15 then the ﬁrst time point after aggregation will be 2011-01-01 01:00. For the ELECTRICITY\\ndataset we removed the ﬁrst year from training set, to match the training set used in (Yu et al., 2016),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='00:15 then the ﬁrst time point after aggregation will be 2011-01-01 01:00. For the ELECTRICITY\\ndataset we removed the ﬁrst year from training set, to match the training set used in (Yu et al., 2016),\\nbased on the aggregated dataset downloaded from, presumable authors’, github repository4. We also\\nmade sure that data points for both ELECTRICITY and TRAFFIC datasets after aggregation match\\nthose used in (Yu et al., 2016). The authors of MatFact model were using the last 7 days of datasets\\nas test set, but papers from Amazon are using different splits, where the split points are provided by a\\ndate. Changing split points without a well grounded reason adds uncertainties to the comparability of\\nthe models performances and creates challenges to the reproducibility of the results, thus we were\\ntrying to match all different splits in our experiments. It was especially challenging on TRAFFIC\\ndataset, where we had to use some heuristics to ﬁnd records dates; the dataset authors state: “ The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='trying to match all different splits in our experiments. It was especially challenging on TRAFFIC\\ndataset, where we had to use some heuristics to ﬁnd records dates; the dataset authors state: “ The\\nmeasurements cover the period from Jan. 1st 2008 to Mar. 30th 2009” and “ We remove public\\nholidays from the dataset, as well as two days with anomalies (March 8th 2009 and March 9th 2008)\\nwhere all sensors were muted between 2:00 and 3:00 AM. ” , but we failed to match a part of the\\nprovided labels of week days to actual dates. Therefore, we had to assume that the actual list of gaps,\\nwhich include holidays and anomalous days, is the following:\\n1. Jan. 1, 2008 (New Year’s Day)\\n2. Jan. 21, 2008 (Martin Luther King Jr. Day)\\n3. Feb. 18, 2008 (Washington’s Birthday)\\n4. Mar. 9, 2008 (Anomaly day)\\n5. May 26, 2008 (Memorial Day)\\n6. Jul. 4, 2008 (Independence Day)\\n7. Sep. 1, 2008 (Labor Day)\\n8. Oct. 13, 2008 (Columbus Day)\\n9. Nov. 11, 2008 (Veterans Day)\\n10. Nov. 27, 2008 (Thanksgiving)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='5. May 26, 2008 (Memorial Day)\\n6. Jul. 4, 2008 (Independence Day)\\n7. Sep. 1, 2008 (Labor Day)\\n8. Oct. 13, 2008 (Columbus Day)\\n9. Nov. 11, 2008 (Veterans Day)\\n10. Nov. 27, 2008 (Thanksgiving)\\n11. Dec. 25, 2008 (Christmas Day)\\n12. Jan. 1, 2009 (New Year’s Day)\\n13. Jan. 19, 2009 (Martin Luther King Jr. Day)\\n14. Feb. 16, 2009 (Washington’s Birthday)\\n15. Mar. 8, 2009 (Anomaly day)\\nThe ﬁrst 6 gaps were conﬁrmed by the gaps in labels, but the rest were more than 1 day apart from any\\npublic holiday of years 2008 and 2009 in San Francisco, California and US. More over the number of\\ngaps we found in the labels provided by dataset authors is 10, while the number of days between Jan.\\n1st 2008 and Mar. 30th 2009 is 455, assuming that Jan. 1st 2008 was skipped from the values and\\nlabels we should end up with either 454 −10 = 444 instead of 440 days or different end date.\\nThe metric is reported in Normalized deviation (ND) as in (Yu et al., 2016) which is equal to p50'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='labels we should end up with either 454 −10 = 444 instead of 440 days or different end date.\\nThe metric is reported in Normalized deviation (ND) as in (Yu et al., 2016) which is equal to p50\\nloss used in DeepAR, Deep State, and Deep Factors papers.\\n2https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\\n3https://archive.ics.uci.edu/ml/datasets/PEMS-SF\\n4https://github.com/rofuyu/exp-trmf-nips16/blob/master/python/exp-scripts/datasets/download-data.sh\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 23, 'page_label': '24', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nND = ∑i,t |ˆYit −Yit|\\n∑i,t |Yit| (7)\\nTable 16: ND Performance on the ELECTRICITY and TRAFFIC test sets.\\n1 Split used in DeepAR (Flunkert et al., 2017) and Deep State (Rangapuram et al., 2018a).\\n2 Split used in Deep Factors (Wang et al., 2019).\\n†Numbers reported by (Flunkert et al., 2017), which are different from the original MatFact paper,\\nhypothetically due to changed split point.\\nELECTRICITY TRAFFIC\\n2014-09-011 2014-03-312 last 7 days 2008-06-15 1 2008-01-142 last 7 days\\nMatFact 0.16 † n/a 0.255 0.20† n/a 0.187\\nDeepAR 0.07 0.272 n/a 0.17 0.296 n/a\\nDeep State 0.083 n/a n/a 0.167 n/a n/a\\nDeep Factors n/a 0.112 n/a n/a 0.225 n/a\\nN-BEATS-G (ours) 0.064 0.065 0.171 0.114 0.230 0.112\\nN-BEATS-I (ours) 0.073 0.072 0.185 0.114 0.231 0.110\\nN-BEATS-I+G (ours) 0.067 0.067 0.178 0.114 0.230 0.111\\nContrary to Amazon models N-BEATS does not use any covariates, like day-of-week, hour-of-day,\\netc.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 23, 'page_label': '24', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='N-BEATS-I+G (ours) 0.067 0.067 0.178 0.114 0.230 0.111\\nContrary to Amazon models N-BEATS does not use any covariates, like day-of-week, hour-of-day,\\netc.\\nThe N-BEATS architecture used in this experiment is exactly the same as used in M4, M3 and\\nTOURISM datasets, the only difference is history size and the number of iterations. These parameters\\nwere chosen based on performance on validation set. Where the validation set consists of 7 consecutive\\ndays right before the test set. After the parameters are chosen the model is retrained on training set\\nwhich includes the validation set, then tested on test set. The model is trained once and tested on test\\nset using rolling window operation described in (Yu et al., 2016).\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 24, 'page_label': '25', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nC.5 D ETAILED RESULTS : COMPARE TO DEEPAR, D EEP STATE SPACE MODELS\\nTable 17 compares ND (7) performance of DeepAR, DeepState models published in (Rangapuram\\net al., 2018a) and N-BEATS.\\nTable 17: ND Performance of DeepAR, Deep State Space, and N-BEATS models on M4-Hourly and\\nTOURISM datasets\\nM4 (Hourly) TOURISM (Monthly) TOURISM (Quarterly)\\nDeepAR 0.09 0.107 0.11\\nDeepState 0.044 0.138 0.098\\nN-BEATS-G (ours) 0.023 0.097 0.080\\nN-BEATS-I (ours) 0.027 0.103 0.079\\nN-BEATS-I+G (ours) 0.025 0.099 0.077\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 25, 'page_label': '26', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 18: Settings of hyperparameters across subsets of M4, M3, TOURISM datasets.\\nM4 M3 TOURISM\\nYly Qly Mly Wly Dly Hly Yly Qly Mly Other Yly Qly Mly\\nParameter N-BEATS-I\\nLH 1.5 1.5 1.5 10 10 10 20 5 5 20 20 10 20\\nIterations 15K 15K 15K 5K 5K 5K 50 6K 6K 250 30 500 300\\nLosses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\\nS-width 2048\\nS-blocks 3\\nS-block-layers 4\\nT-width 256\\nT-degree 2\\nT-blocks 3\\nT-block-layers 4\\nSharing STACK LEVEL\\nLookback period 2 H,3H,4H,5H,6H,7H\\nBatch 1024\\nParameter N-BEATS-G\\nLH 1.5 1.5 1.5 10 10 10 20 20 20 10 5 10 20\\nIterations 15K 15K 15K 5K 5K 5K 20 250 10K 250 30 100 100\\nLosses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\\nWidth 512\\nBlocks 1\\nBlock-layers 4\\nStacks 30\\nSharing NO\\nLookback period 2 H,3H,4H,5H,6H,7H\\nBatch 1024\\nD H YPER -PARAMETER SETTINGS\\nTable 18 presents the hyperparameter settings used to train models on different subsets of M4, M3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 25, 'page_label': '26', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Stacks 30\\nSharing NO\\nLookback period 2 H,3H,4H,5H,6H,7H\\nBatch 1024\\nD H YPER -PARAMETER SETTINGS\\nTable 18 presents the hyperparameter settings used to train models on different subsets of M4, M3\\nand TOURISM datasets. A brief discussion of ﬁeld names in the table is warranted.\\nSubset names Yly, Qly, Mly, Wly, Dly, Hly, Othercorrespond to yearly, quarterly, monthly, weekly,\\ndaily, hourly and other frequency subsets deﬁned in the original datasets.\\nN-BEATS-I and N-BEATS-G correspond to the interpretable and generic model conﬁgurations\\ndeﬁned in Section 3.3.\\nD.1 C OMMON PARAMETERS\\nLH is the coefﬁcient deﬁning the length of training history immediately preceding the last point in\\nthe train part of the TS that is used to generate training samples. For example, if for M4 Yearly the\\nforecast horizon is 6 and LH is 1.5, then we consider 1.5 ·6 = 9 most recent points in the train dataset\\nfor each time series to generate training samples. A training sample from a given TS in M4 Yearly is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 25, 'page_label': '26', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='for each time series to generate training samples. A training sample from a given TS in M4 Yearly is\\nthen generated by choosing one of the most recent 9 points as an anchor. All the points preceding the\\nanchor are used to create the input to N-BEATS, while the points following and including the anchor\\nbecome training target. Target and history points that fall outside of the time series limits given the\\nanchor position are ﬁlled with zeros and masked during the training. We observed that for subsets\\nwith large number of time series LH tends to be smaller and for subsets with smaller number of time\\nseries it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4LH is\\nequal to 1.5; and in moderate to small Weekly, Daily, Hourly subsets of M4LH is equal to 10.\\nIterations is the number of batches used to train N-BEATS.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nLosses is the set of loss functions that is used to build ensemble. We observed on the respective\\nvalidation sets that for M4 and M3 mixing models trained on a variety of metrics resulted in\\nperformance gain. In the case of TOURISM dataset training only on MAPE led to the best validation\\nscores.\\nSharing deﬁnes whether the coefﬁcients in the fully-connected layers are shared. We observed that\\nthe interpretable model works best when weights are shared across stack, while generic model works\\nbest when none of the weights are shared.\\nLookback periodis the length of the history window forming the input to the model (please refer to\\nFigure 1). This is the function of the forecast horizon length, H. In our experiments we mixed models\\nwith lookback periods 2H,3H,4H,5H,6H,7H in one ensemble. As an example, for a forecast\\nhorizon length H = 8 and a lookback period 7H, the model’s input will consist of the history window\\nof 7 ·8 = 56 samples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='horizon length H = 8 and a lookback period 7H, the model’s input will consist of the history window\\nof 7 ·8 = 56 samples.\\nBatch is the batch size. We used batch size of 1024. We observed that the training was faster with\\nlarger batch sizes, however in our setup little gain was observed with batch sizes beyond 1024.\\nD.2 N-BEATS-I PARAMETERS\\nS-width is the width of the fully connected layers in the blocks comprising the seasonality stack of\\nthe interpretable model (please refer to Figure 1).\\nS-blocks is the number of blocks comprising the seasonality stack of the interpretable model (please\\nrefer to Figure 1).\\nS-block-layers is the number of fully-connected layers comprising one block in the seasonality\\nstack of the interpretable model (preceding the ﬁnal fully-connected projection layers forming the\\nbackcast/forecast fork, please refer to Figure 1).\\nT-width is the width of the fully connected layers in the blocks comprising the trend stack of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='backcast/forecast fork, please refer to Figure 1).\\nT-width is the width of the fully connected layers in the blocks comprising the trend stack of the\\ninterpretable model (please refer to Figure 1).\\nT-degree is the degree p of polynomial in the trend stack of the interpretable model (please refer to\\nequation (2)).\\nT-blocks is the number of blocks comprising the trend stack of the interpretable model (please refer\\nto Figure 1).\\nT-block-layers is the number of fully-connected layers comprising one block in the trend stack\\nof the interpretable model (preceding the ﬁnal fully-connected projection layers forming the back-\\ncast/forecast fork, please refer to Figure 1).\\nD.3 N-BEATS-G PARAMETERS\\nWidth is the width of the fully connected layers in the blocks comprising the stacks of the generic\\nmodel (please refer to Figure 1).\\nBlocks is the number of blocks comprising the stack of the generic model (please refer to Figure 1).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='model (please refer to Figure 1).\\nBlocks is the number of blocks comprising the stack of the generic model (please refer to Figure 1).\\nBlock-layers is the number of fully-connected layers comprising one block in the stack of the generic\\nmodel (preceding the ﬁnal fully-connected projection layers forming the backcast/forecast fork,\\nplease refer to Figure 1).\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\n0 1 2 3 4 5\\nt\\n0.8\\n0.9\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90 STACK1-G\\n0 1 2 3 4 5\\nt\\n0.025\\n0.050\\n0.075 STACK2-G\\n0 1 2 3 4 5\\nt\\n0.80\\n0.85\\n0.90\\n0.95\\nSTACK1-I\\n0 1 2 3 4 5\\nt\\n0.02\\n0.03\\n0.04\\n0.05 STACK2-I\\n0 2 4 6\\nt\\n0.85\\n0.90\\n0.95\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6\\nt\\n0.86\\n0.88\\n0.90 STACK1-G\\n0 2 4 6\\nt\\n0.025\\n0.000\\n0.025\\n0.050\\nSTACK2-G\\n0 2 4 6\\nt\\n0.88\\n0.89\\n0.90\\nSTACK1-I\\n0 2 4 6\\nt\\n0.05\\n0.00\\n0.05\\nSTACK2-I\\n0 5 10 15\\nt\\n0.4\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 5 10 15\\nt\\n0.8\\n0.9 STACK1-G\\n0 5 10 15\\nt\\n0.1\\n0.0\\nSTACK2-G\\n0 5 10 15\\nt\\n0.85\\n0.90\\nSTACK1-I\\n0 5 10 15\\nt\\n0.3\\n0.2\\n0.1\\n0.0 STACK2-I\\n0 2 4 6 8 10 12\\nt\\n0.6\\n0.8\\n1.0\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80\\nSTACK1-G\\n0 2 4 6 8 10 12\\nt\\n0.000\\n0.025\\n0.050\\n0.075\\nSTACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='STACK2-G\\n0 2 4 6 8 10 12\\nt\\n0.65\\n0.70\\n0.75\\n0.80 STACK1-I\\n0 2 4 6 8 10 12\\nt\\n0.00\\n0.02\\n0.04 STACK2-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.96\\n0.98\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.002\\n0.001\\n STACK2-G\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.974\\n0.976\\nSTACK1-I\\n0.0 2.5 5.0 7.5 10.0 12.5\\nt\\n0.0003\\n0.0002\\n0.0001\\n STACK2-I\\n0 10 20 30 40\\nt\\n0.25\\n0.50\\n0.75\\n1.00\\nACTUAL\\nFORECAST-I\\nFORECAST-G\\n(a) Combined\\n0 10 20 30 40\\nt\\n0.2\\n0.4\\n0.6\\nSTACK1-G (b) Stack1-G\\n0 10 20 30 40\\nt\\n0.02\\n0.00\\nSTACK2-G (c) Stack2-G\\n0 10 20 30 40\\nt\\n0.36\\n0.38\\n0.40\\nSTACK1-I (d) StackT-I\\n0 10 20 30 40\\nt\\n0.2\\n0.0\\n0.2\\nSTACK2-I (e) StackS-I\\nFigure 5: The outputs of generic and the interpretable conﬁgurations, M4 dataset. Each row is one\\ntime series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='time series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\\nare normalized by the maximal value of the actual time series for convenience. Column (a) shows the\\nactual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\\nforecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\\nrespectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\\nand the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\\nE D ETAILED SIGNAL TRACES OF INTERPRETABLE INPUTS PRESENTED IN\\nFIGURE 2\\nThe goal of this section is to show the detailed traces (numeric values) of signals visualized in Fig. 2.\\nThis is to demonstrate that even though the StackT-I (Fig. 2 (d)) and StackS-I (Fig. 2 (e)) provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='This is to demonstrate that even though the StackT-I (Fig. 2 (d)) and StackS-I (Fig. 2 (e)) provide\\nresponse lines different from the counterparts in Stack1-G (Fig. 2 (b)) and Stack2-G (Fig. 2 (c)), the\\nsummations in the combined line (Fig. 2 (a)) can still be very similar.\\nFirst, we reproduce Fig. 5 for the convenience of the reader. Second, for each row in the ﬁgure, we\\nproduce a table showing the numeric values of each signal depicted in corresponding plots (please\\nrefer to Tables 19– 24). We make sure that the names of signals in ﬁgure legends and in the table\\ncolumns match, such that they can easily be cross-referenced. It can be clearly seen in Tables 19– 24\\nthat (i) traces STACK1-I and STACK2-I sum up to trace FORECAST-I, (ii) traces STACK1-G and\\nSTACK2-G sum up to trace FORECAST-G, (iii) traces FORECAST-I and FORECAST-G are overall\\nvery similar even though their components may signiﬁcantly differ from each other.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 19: Detailed traces of signals depicted in row 1 of Fig. 5, corresponding to the time series\\nYearly: id Y3974.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.780182 0.802068 0.806608 0.781290 0.020778 0.801294 0.005314\\n1 0.802337 0.829223 0.841406 0.798422 0.030801 0.825271 0.016135\\n2 0.840317 0.863683 0.883136 0.820196 0.043487 0.853114 0.030022\\n3 0.889376 0.905962 0.929258 0.850250 0.055712 0.880833 0.048425\\n4 0.930521 0.947028 0.967846 0.892221 0.054807 0.904393 0.063453\\n5 0.976414 0.982307 1.000000 0.949748 0.032559 0.921360 0.078640\\nTable 20: Detailed traces of signals depicted in row 2 of Fig. 5, corresponding to the time series\\nQuarterly: id Q11588.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.830068 0.835964 0.829417 0.880435 -0.044471 0.852018 -0.022601\\n1 0.927155 0.898949 0.891168 0.881626 0.017324 0.880124 0.011044'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='ACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.830068 0.835964 0.829417 0.880435 -0.044471 0.852018 -0.022601\\n1 0.927155 0.898949 0.891168 0.881626 0.017324 0.880124 0.011044\\n2 0.979204 0.957379 0.948799 0.882549 0.074831 0.907149 0.041650\\n3 0.857250 0.900612 0.891967 0.883830 0.016782 0.877959 0.014008\\n4 0.895082 0.857230 0.847029 0.886096 -0.028866 0.852232 -0.005204\\n5 0.981590 0.923832 0.911001 0.889972 0.033860 0.881140 0.029861\\n6 1.000000 0.978128 0.965236 0.896085 0.082043 0.907475 0.057761\\n7 0.910528 0.920632 0.915460 0.905062 0.015571 0.886941 0.028519\\nTable 21: Detailed traces of signals depicted in row 3 of Fig. 5, corresponding to the time series\\nMonthly: id M19006.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 1.000000 0.923394 0.928279 0.944660 -0.021266 0.922835 0.005444\\n1 0.865248 0.822588 0.829924 0.937575 -0.114987 0.867619 -0.037695\\n2 0.638298 0.693820 0.717119 0.930295 -0.236475 0.810818 -0.093699'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='0 1.000000 0.923394 0.928279 0.944660 -0.021266 0.922835 0.005444\\n1 0.865248 0.822588 0.829924 0.937575 -0.114987 0.867619 -0.037695\\n2 0.638298 0.693820 0.717119 0.930295 -0.236475 0.810818 -0.093699\\n3 0.531915 0.594375 0.612377 0.922890 -0.328515 0.757199 -0.144823\\n4 0.468085 0.579403 0.595221 0.915428 -0.336025 0.747151 -0.151930\\n5 0.539007 0.602615 0.620809 0.907977 -0.305362 0.755078 -0.134269\\n6 0.581560 0.653387 0.682669 0.900606 -0.247219 0.774561 -0.091891\\n7 0.666667 0.747440 0.765814 0.893385 -0.145945 0.799594 -0.033781\\n8 0.737589 0.817883 0.835577 0.886382 -0.068498 0.817218 0.018359\\n9 0.765957 0.862568 0.856962 0.879665 -0.017097 0.822099 0.034862\\n10 0.851064 0.873448 0.880074 0.873304 0.000145 0.833473 0.046601\\n11 0.893617 0.878186 0.871103 0.867367 0.010819 0.829537 0.041566\\n12 0.858156 0.834448 0.853549 0.861923 -0.027475 0.816527 0.037022\\n13 0.695035 0.785341 0.776687 0.857040 -0.071699 0.782536 -0.005850'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='12 0.858156 0.834448 0.853549 0.861923 -0.027475 0.816527 0.037022\\n13 0.695035 0.785341 0.776687 0.857040 -0.071699 0.782536 -0.005850\\n14 0.446809 0.662443 0.697788 0.852789 -0.190345 0.745623 -0.047835\\n15 0.382979 0.623196 0.624614 0.849236 -0.226040 0.711553 -0.086939\\n16 0.453901 0.598511 0.625150 0.846451 -0.247941 0.712130 -0.086980\\n17 0.539007 0.668231 0.652175 0.844504 -0.176272 0.716925 -0.064750\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 29, 'page_label': '30', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 22: Detailed traces of signals depicted in row 4 of Fig. 5, corresponding to the time series\\nWeekly: id W246.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.630056 0.629703 0.625108 0.639236 -0.009534 0.625416 -0.000309\\n1 0.607536 0.643509 0.639846 0.647549 -0.004039 0.639592 0.000254\\n2 0.641731 0.656171 0.652584 0.656696 -0.000526 0.643665 0.008919\\n3 0.628783 0.669636 0.661163 0.666739 0.002897 0.652107 0.009056\\n4 0.816799 0.687287 0.683860 0.677738 0.009549 0.662176 0.021683\\n5 0.817020 0.709211 0.717187 0.689752 0.019459 0.686589 0.030598\\n6 0.766724 0.731732 0.742824 0.702841 0.028891 0.705234 0.037590\\n7 0.770320 0.750834 0.755154 0.717066 0.033768 0.716986 0.038167\\n8 0.794113 0.769671 0.778460 0.732487 0.037184 0.731113 0.047347\\n9 0.874011 0.793373 0.810332 0.749164 0.044209 0.750939 0.059392\\n10 1.000000 0.816386 0.847545 0.767157 0.049229 0.776405 0.071140'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 29, 'page_label': '30', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='8 0.794113 0.769671 0.778460 0.732487 0.037184 0.731113 0.047347\\n9 0.874011 0.793373 0.810332 0.749164 0.044209 0.750939 0.059392\\n10 1.000000 0.816386 0.847545 0.767157 0.049229 0.776405 0.071140\\n11 0.979251 0.834532 0.858604 0.786526 0.048006 0.783939 0.074665\\n12 0.933160 0.850010 0.866116 0.807332 0.042678 0.792134 0.073982\\nTable 23: Detailed traces of signals depicted in row 5 of Fig. 5, corresponding to the time series\\nDaily: id D404.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.968704 0.972314 0.971950 0.972589 -0.000275 0.972964 -0.001014\\n1 0.954319 0.972637 0.972131 0.972808 -0.000171 0.972822 -0.000690\\n2 0.954599 0.972972 0.972188 0.973060 -0.000088 0.973798 -0.001610\\n3 0.959959 0.973230 0.972140 0.973341 -0.000112 0.973686 -0.001546\\n4 0.975472 0.973481 0.972125 0.973649 -0.000168 0.974060 -0.001934\\n5 0.970391 0.973715 0.972174 0.973979 -0.000264 0.974800 -0.002626\\n6 0.977728 0.974056 0.972403 0.974328 -0.000272 0.974368 -0.001965'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 29, 'page_label': '30', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='5 0.970391 0.973715 0.972174 0.973979 -0.000264 0.974800 -0.002626\\n6 0.977728 0.974056 0.972403 0.974328 -0.000272 0.974368 -0.001965\\n7 0.985624 0.974445 0.972428 0.974693 -0.000248 0.973870 -0.001442\\n8 0.979695 0.974823 0.972567 0.975069 -0.000246 0.974870 -0.002303\\n9 0.985345 0.975079 0.973089 0.975455 -0.000376 0.975970 -0.002881\\n10 0.983088 0.975547 0.973881 0.975845 -0.000298 0.975796 -0.001915\\n11 0.983368 0.975991 0.974537 0.976238 -0.000247 0.976757 -0.002220\\n12 0.998312 0.976365 0.974924 0.976628 -0.000263 0.977579 -0.002655\\n13 1.000000 0.976821 0.975291 0.977013 -0.000193 0.977213 -0.001922\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2020\\nTable 24: Detailed traces of signals depicted in row 6 of Fig. 5, corresponding to the time series\\nHourly: id H344.\\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\\nt\\n0 0.226804 0.256799 0.277159 0.346977 -0.090179 0.280489 -0.003329\\n1 0.175258 0.228913 0.234605 0.347615 -0.118701 0.241790 -0.007185\\n2 0.164948 0.209208 0.207347 0.348265 -0.139057 0.218575 -0.011228\\n3 0.164948 0.197360 0.193084 0.348928 -0.151568 0.208458 -0.015374\\n4 0.216495 0.190397 0.186586 0.349606 -0.159209 0.205701 -0.019115\\n5 0.195876 0.194204 0.189433 0.350297 -0.156094 0.214399 -0.024966\\n6 0.319588 0.221026 0.216221 0.351004 -0.129978 0.241574 -0.025353\\n7 0.226804 0.279857 0.276414 0.351726 -0.071869 0.293580 -0.017167\\n8 0.371134 0.357292 0.359372 0.352464 0.004828 0.364392 -0.005020\\n9 0.536082 0.438540 0.446126 0.353218 0.085322 0.442703 0.003423\\n10 0.711340 0.511441 0.519928 0.353989 0.157452 0.510142 0.009787'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='8 0.371134 0.357292 0.359372 0.352464 0.004828 0.364392 -0.005020\\n9 0.536082 0.438540 0.446126 0.353218 0.085322 0.442703 0.003423\\n10 0.711340 0.511441 0.519928 0.353989 0.157452 0.510142 0.009787\\n11 0.752577 0.571604 0.578186 0.354777 0.216827 0.571596 0.006590\\n12 0.783505 0.617085 0.618778 0.355584 0.261501 0.613425 0.005353\\n13 0.773196 0.651777 0.655123 0.356409 0.295368 0.649259 0.005864\\n14 0.618557 0.670202 0.676814 0.357253 0.312950 0.669555 0.007260\\n15 0.793814 0.679884 0.692592 0.358116 0.321768 0.684208 0.008384\\n16 0.793814 0.672488 0.696440 0.359000 0.313488 0.684764 0.011676\\n17 0.680412 0.648851 0.677696 0.359904 0.288947 0.662714 0.014983\\n18 0.525773 0.602496 0.630922 0.360828 0.241667 0.620368 0.010554\\n19 0.505155 0.537698 0.552296 0.361775 0.175923 0.552599 -0.000304\\n20 0.701031 0.463760 0.466442 0.362743 0.101016 0.477429 -0.010987\\n21 0.484536 0.395795 0.390958 0.363734 0.032061 0.408708 -0.017750\\n22 0.247423 0.337809 0.338500 0.364748 -0.026939 0.354028 -0.015528'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='21 0.484536 0.395795 0.390958 0.363734 0.032061 0.408708 -0.017750\\n22 0.247423 0.337809 0.338500 0.364748 -0.026939 0.354028 -0.015528\\n23 0.371134 0.292452 0.303902 0.365786 -0.073334 0.312588 -0.008686\\n24 0.216495 0.254359 0.258435 0.366848 -0.112489 0.270568 -0.012133\\n25 0.412371 0.227557 0.224291 0.367934 -0.140377 0.237846 -0.013555\\n26 0.237113 0.207962 0.201250 0.369046 -0.161084 0.219420 -0.018169\\n27 0.206186 0.196049 0.189439 0.370183 -0.174133 0.209743 -0.020304\\n28 0.206186 0.189030 0.182843 0.371346 -0.182316 0.207727 -0.024884\\n29 0.237113 0.194524 0.185734 0.372536 -0.178011 0.213194 -0.027460\\n30 0.206186 0.220227 0.215444 0.373753 -0.153526 0.242485 -0.027041\\n31 0.329897 0.279614 0.274624 0.374998 -0.095383 0.292834 -0.018210\\n32 0.371134 0.355078 0.358020 0.376270 -0.021193 0.365332 -0.007312\\n33 0.494845 0.437103 0.445832 0.377572 0.059531 0.441323 0.004510\\n34 0.690722 0.509515 0.520006 0.378903 0.130612 0.512064 0.007942'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='32 0.371134 0.355078 0.358020 0.376270 -0.021193 0.365332 -0.007312\\n33 0.494845 0.437103 0.445832 0.377572 0.059531 0.441323 0.004510\\n34 0.690722 0.509515 0.520006 0.378903 0.130612 0.512064 0.007942\\n35 0.989691 0.570761 0.579003 0.380263 0.190497 0.569851 0.009152\\n36 1.000000 0.615868 0.623981 0.381654 0.234214 0.617254 0.006728\\n37 0.845361 0.651487 0.656782 0.383076 0.268411 0.650336 0.006446\\n38 0.742268 0.670664 0.678412 0.384528 0.286136 0.673055 0.005357\\n39 0.721649 0.680534 0.691961 0.386013 0.294521 0.684347 0.007614\\n40 0.567010 0.671607 0.692853 0.387530 0.284078 0.683297 0.009555\\n41 0.546392 0.648851 0.672476 0.389079 0.259771 0.660613 0.011863\\n42 0.432990 0.599785 0.621940 0.390662 0.209123 0.615426 0.006514\\n43 0.391753 0.537520 0.544543 0.392279 0.145241 0.549961 -0.005417\\n44 0.443299 0.462772 0.457700 0.393930 0.068842 0.471080 -0.013380\\n45 0.422680 0.397098 0.380324 0.395616 0.001482 0.401229 -0.020905\\n46 0.381443 0.342213 0.325583 0.397337 -0.055124 0.347827 -0.022244'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-24T01:19:26+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-24T01:19:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1905.10437v4.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31', 'source_file': '1905.10437v4.pdf', 'file_type': 'pdf'}, page_content='45 0.422680 0.397098 0.380324 0.395616 0.001482 0.401229 -0.020905\\n46 0.381443 0.342213 0.325583 0.397337 -0.055124 0.347827 -0.022244\\n47 0.257732 0.297711 0.287130 0.399094 -0.101384 0.304270 -0.017140\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware\\nSequential Autoencoder\\nTiexin Qin1 Shiqi Wang1 Haoliang Li1\\nAbstract\\nDomain generalization aims to improve the gen-\\neralization capability of machine learning sys-\\ntems to out-of-distribution (OOD) data. Exist-\\ning domain generalization techniques embark\\nupon stationary and discrete environments to\\ntackle the generalization issue caused by OOD\\ndata. However, many real-world tasks in non-\\nstationary environments ( e.g., self-driven car\\nsystem, sensor measures) involve more complex\\nand continuously evolving domain drift, which\\nraises new challenges for the problem of do-\\nmain generalization. In this paper, we formu-\\nlate the aforementioned setting as the problem of\\nevolving domain generalization. Speciﬁcally, we\\npropose to introduce a probabilistic framework\\ncalled Latent Structure-aware Sequential Autoen-\\ncoder (LSSAE) to tackle the problem of evolving\\ndomain generalization via exploring the under-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='propose to introduce a probabilistic framework\\ncalled Latent Structure-aware Sequential Autoen-\\ncoder (LSSAE) to tackle the problem of evolving\\ndomain generalization via exploring the under-\\nlying continuous structure in the latent space of\\ndeep neural networks, where we aim to identify\\ntwo major factors namely covariate shift and con-\\ncept shift accounting for distribution shift in non-\\nstationary environments. Experimental results on\\nboth synthetic and real-world datasets show that\\nLSSAE can lead to superior performances based\\non the evolving domain generalization setting.\\n1. Introduction\\nThe success of machine learning techniques typically lies on\\nthe assumption that training data and test data are sampled\\nindependently and identically from similar distributions.\\nHowever, this assumption does not hold when deploying the\\ntrained model in many real-world environments where the\\ndistribution of test data varies from training data. This dis-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='However, this assumption does not hold when deploying the\\ntrained model in many real-world environments where the\\ndistribution of test data varies from training data. This dis-\\ntribution discrepancy, so-called distribution shift, can lead\\n1City University of Hong Kong, Hong Kong. Correspondence\\nto: Haoliang Li <haoliang.li1991@gmail.com>.\\nProceedings of the 39 th International Conference on Machine\\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-\\nright 2022 by the author(s).\\nto the dramatic performance decrease of machine learning\\nmodels (Torralba & Efros, 2011). To mitigate this issue,\\ndomain generalization (DG) has been proposed to learn a\\nmore robust model which can be better generalized to OOD\\ndata (Muandet et al., 2013; Balaji et al., 2018; Li et al.,\\n2018b).\\nWhile some progress is being achieved so far, existing DG\\nmethods are limited to the setting of generalization among\\ndiscrete and stationary environments. This setting can be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='2018b).\\nWhile some progress is being achieved so far, existing DG\\nmethods are limited to the setting of generalization among\\ndiscrete and stationary environments. This setting can be\\nproblematic in some real-world applications where we re-\\nquire that model can be generalized among continuous do-\\nmains (Hoffman et al., 2014). For example, a self-driving\\ncar system, when deployed in the real world, struggles to\\nperform under an open environment where the accepted data\\nchanges naturally according to the geographic location, time\\nintervals, and other factors in a gradual manner (Hoffman\\net al., 2014). For another example, the measures of sen-\\nsors can also drift over time due to the outer environments\\nand inter factors, such as aging (Vergara et al., 2012). In\\nthese scenarios, treating each domain in a separate manner\\nis unlikely to yield the desired performance as it does not\\nconsider the property of continuous domain structure.\\nAnother limitation of most of the existing DG methods is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='is unlikely to yield the desired performance as it does not\\nconsider the property of continuous domain structure.\\nAnother limitation of most of the existing DG methods is\\nthat they did not take “concept shift” into consideration.\\nSuch concept shift can also lead to performance drop (Fed-\\nerici et al., 2021). A typical example of concept shift would\\nbe that the incidence rate of a particular disease in cer-\\ntain groups may change over time due to the development\\nof treatments and preventive measures. Therefore, exist-\\ning DG techniques may fail to be applied to some other\\ncomplex real-world applications in non-stationary environ-\\nments (Sugiyama et al., 2013; Tahmasbi et al., 2021).\\nIn this paper, we propose to focus on the problem of domain\\ngeneralization based on the non-stationary setting, where\\ndata can evolve gradually with both covariate shift and con-\\ncept shift. Particularly, we formulate this non-stationary\\nscenario as evolving domain generalization where we only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='data can evolve gradually with both covariate shift and con-\\ncept shift. Particularly, we formulate this non-stationary\\nscenario as evolving domain generalization where we only\\nhave access to adequate labeled examples from the sequen-\\ntial source domains. Our objective is to develop algorithms\\nthat can explore the underlying continuous structure of dis-\\ntribution shift and generalize well to evolving target domains\\nwhere the samples are unavailable during the training stage.\\narXiv:2205.07649v2  [cs.LG]  16 Jun 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nUnlike existing DG methods that only focus on covariate\\nshift based on the stationary environment, in this paper, we\\npropose a novel framework called Latent Structure-aware\\nSequential Autoencoder (LSSAE), a dynamic probabilistic\\nframework to model the underlying latent variables across\\ndomains. More speciﬁcally, we propose to use two latent\\nvariables to represent the sampling bias in data sample space\\n(i.e., covariate shift) and data category space (i.e., concept\\nshift), and propose a domain-related module and a category-\\nrelated module to infer their dynamic transition functions\\nbased on different time stamps. We conduct extensive ex-\\nperiments to verify that our framework can successfully\\ncapture the underlying covariate shift and interpret the con-\\ncept shift simultaneously. Last but not least, we show that\\nour proposed LSSAE has a promising generation capability'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='capture the underlying covariate shift and interpret the con-\\ncept shift simultaneously. Last but not least, we show that\\nour proposed LSSAE has a promising generation capability\\nto predict unseen target domains, which can be helpful to\\nthe problem related to sequential data generation. The main\\ncontributions of this paper are summarized as follows.\\n• We propose to focus on the problem of non-stationary\\nevolving domain generalization where both covariate\\nshift and concept shift may exist in the setting.\\n• We propose a novel probabilistic framework LSSAE\\nwhich incorporates variational inference to identify the\\ncontinuous latent structures of these two shifts sepa-\\nrately and simultaneously.\\n• We provide empirical results to show that the proposed\\napproach yield better results than other DG methods\\nacross scenarios. Besides, it presents a powerful gener-\\nation ability of predicting unseen evolving domains.\\n2. Related Work\\nDomain Generalization (DG). The goal of DG is to learn'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='across scenarios. Besides, it presents a powerful gener-\\nation ability of predicting unseen evolving domains.\\n2. Related Work\\nDomain Generalization (DG). The goal of DG is to learn\\nrobust models which can generalize well towards the out-\\nof-distribution samples from unseen domains. Existing DG\\nmethods commonly rely on multiple source domains to learn\\nrepresentative features that can be better generalized. Ac-\\ncording to various strategies used to learn these representa-\\ntions, we can roughly categorize them into three catogories.\\nThe ﬁrst type is feature-based methods, which aim to learn\\ndomain-invariant representation which can be better gener-\\nalized to target domains. Speciﬁcally, it can be achieved by\\naligning the distribution of representations from all source\\ndomains (Blanchard et al., 2011; Li et al., 2018b; Albu-\\nquerque et al., 2021) and feature disentanglement (Ilse et al.,\\n2020; Wang et al., 2021; Nguyen et al., 2021). The second'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='domains (Blanchard et al., 2011; Li et al., 2018b; Albu-\\nquerque et al., 2021) and feature disentanglement (Ilse et al.,\\n2020; Wang et al., 2021; Nguyen et al., 2021). The second\\ncategory is meta-learning based methods, which utilize the\\nmodel agnostic training procedure to stimulate the train/test\\nshift for acquiring generalized models (Li et al., 2018a; Bal-\\naji et al., 2018; Dou et al., 2019). Last but not least, data\\naugmentation-based techniques, which aim to manipulate\\nthe perturbation both in original images and features to stim-\\nulate the unseen target domains, can also beneﬁt the problem\\nof domain generalization (V olpi et al., 2018; Shankar et al.,\\n2018; Zhou et al., 2021).\\nContinuous Domain Adaptation (CDA).The problem of\\ncontinuous domain adaptation (i.e., evolving domain adap-\\ntation) has attracted increasing attention recently, where\\nthe CDA methods can be categorized into the intermediate-\\ndomains based methods (Kumar et al., 2020; Gong et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='tation) has attracted increasing attention recently, where\\nthe CDA methods can be categorized into the intermediate-\\ndomains based methods (Kumar et al., 2020; Gong et al.,\\n2019; Chen & Chao, 2021), domain manifold based meth-\\nods (Hoffman et al., 2014; Li et al., 2017), adversary-based\\napproaches (Wang et al., 2020; Wulfmeier et al., 2018), and\\nmeta-learning based methods (Liu et al., 2020; Lao et al.,\\n2020). More or less, they require some samples from tar-\\nget domains for adaptation. In Mancini et al. (2019), they\\npropose to substitute this reliance with some metadata from\\ntarget domains as additional supervision. Instead, our focus\\nis continuous domain generalization where no information\\nfrom target domains is accessible for model learning, which\\nis a more challenging but realistic task for real-world appli-\\ncations.\\nSequential Data Generation.Recent process in unsuper-\\nvised sequence generation (Yingzhen & Mandt, 2018; Han'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='is a more challenging but realistic task for real-world appli-\\ncations.\\nSequential Data Generation.Recent process in unsuper-\\nvised sequence generation (Yingzhen & Mandt, 2018; Han\\net al., 2021; Park et al., 2021) suggests the importance of\\ndecoupling time-invariant and time-variant information dur-\\ning the representation learning procedure. However, these\\napproaches only take sequence generation tasks into consid-\\neration and fail to consider the category-related information,\\nwhich is important for the problem of domain generalization.\\nUnlike these approaches, we propose jointly focusing on\\nthe dynamic modeling of time-variant information on both\\ndata sample space and category space across domains.\\n3. Methodology\\nIn this section, we ﬁrst formalize the problem of evolving\\ndomain generalization based on the non-stationary environ-\\nment and then describe our framework LSSAE for address-\\ning this problem. After that, we will provide theoretical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='domain generalization based on the non-stationary environ-\\nment and then describe our framework LSSAE for address-\\ning this problem. After that, we will provide theoretical\\nanalysis for the proposed framework in Sec. 3.3 and imple-\\nmentation in Sec. 3.4.\\n3.1. Problem Formulation\\nSuppose we are given T sequentially arriving source do-\\nmains S = {D1,D2,..., DT }, where each domain Dt =\\n{(xt,i,yt,i)}nt\\ni=1 is comprised of nt labeled samples for\\nt ∈ {1,2,...,T }. The goal of our problem setting is\\nto train a classiﬁcation model on S which can be gen-\\neralized to M following arriving target domains T =\\n{DT+1,DT+2,..., DT+M }, Dt = {(xt,i)}nt\\ni=1 (t ∈{T +\\n1,T + 2,...,T + M}), which are not available during\\ntraining stage. For simplicity, we omit the index iwhen-\\never xi refers to a single data point. To further quantify\\nthe continuously evolving nature of domains, we denote'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nX Y\\nW\\nX Y\\nW\\nV\\nXt- 1 Yt- 1\\nWt- 1\\nVt- 1\\nXt Yt\\nWt\\nVt /gid00473/gid00473/gid00473\\n(a)\\n(b)\\n(c)\\n(d)\\n/gid01141/gid01141/gid01141\\nXt- 1 Yt- 1\\nWt- 1\\nX t Yt\\nWt /gid00473/gid00473/gid00473\\n/gid00473/gid00473/gid00473\\n/gid00473/gid00473/gid00473 \\n/gid00473/gid00473/gid00473\\nFigure 1.Comparison of causality diagram for stationary and non-\\nstationary domain generalization scenarios. (a) represents the\\nstandard stationary DG settings which only contains covariate shift\\n(P(X) varies for source and target domains). (b) is an extension\\nversion of (a) which contains both covariate shift and concept shift\\n(P(Y |X) varies). (c) and (d) are corresponding non-stationary\\nDG settings where there exist evolving patterns among adjacent\\ndomains.\\n0 ≤D(Dt,Dt+1) ≤ϵ for two consecutive domains un-\\nder some distribution distance function D (e.g., Kullback-\\nLeibler distance). In other words, the discrepancy between'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='domains.\\n0 ≤D(Dt,Dt+1) ≤ϵ for two consecutive domains un-\\nder some distribution distance function D (e.g., Kullback-\\nLeibler distance). In other words, the discrepancy between\\ntwo consecutive domains is bounded.\\nConventional DG setting only assumes that P(X)\\nvaries (i.e., covariate shift) for different domains (See\\nFig. 1 (a)), which may not be ideal since both P(X) and\\nP(Y|X) can be non-stationary (i.e., P(X) and P(Y|X)\\nvary over time which lead to evolving covariate shift and\\nconcept shift, respectively). To tackle this problem, in this\\npaper, we aim to explore the evolving patterns of covariate\\nshift and concept shift across domains.\\n3.2. LSSAE: Latent Structure-aware Sequential\\nAutoencoder\\nTo model the dynamic in non-stationary systems, we con-\\nsider two independent factors W and V (i.e., W ⊥ ⊥V)\\nwhich account for the distribution drift in data sample space\\n(i.e., covariate shift) and data category space (i.e., con-\\ncept shift) respectively according to different time stamps'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='which account for the distribution drift in data sample space\\n(i.e., covariate shift) and data category space (i.e., con-\\ncept shift) respectively according to different time stamps\\n(See Fig. 1 (d)). For data (xt,yt) collected at time stamp t,\\nwe denote zw\\nt and zv\\nt as the latent variables of W and V at\\ntime stamp t. For completeness, we further consider time-\\ninvariant latent code zc to capture the static information of\\nxt. We thus can deﬁne a probabilistic generative model for\\nthe joint distribution of all source domains as\\np(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\n= p(x1:T ,zw\\n1:T ,zc)p(y1:T ,zv\\n1:T |zc). (1)\\nwhere the ﬁrst term p(x1:T ,zw\\n1:T ,zc) and second term\\np(y1:T ,zv\\n1:T |x1:T ) can be formulated by using Markov\\nchain model as\\np(x1:T ,zw\\n1:T ,zc) =p(zc)\\nT∏\\nt=1\\np(zw\\nt |zw\\n<t) p(xt|zc,zw\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\ncovariate shift\\n,\\n(2)\\np(y1:T ,zv\\n1:T |zc) =\\nT∏\\nt=1\\np(zv\\nt |zv\\n<t) p(yt|zc,zv\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\nconcept shift\\n, (3)\\nand p(xt|zc,zw\\nt ) and p(yt|zc,zv\\nt ) denote covariate shift and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='<t) p(xt|zc,zw\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\ncovariate shift\\n,\\n(2)\\np(y1:T ,zv\\n1:T |zc) =\\nT∏\\nt=1\\np(zv\\nt |zv\\n<t) p(yt|zc,zv\\nt )\\ued19 \\ued18\\ued17 \\ued1a\\nconcept shift\\n, (3)\\nand p(xt|zc,zw\\nt ) and p(yt|zc,zv\\nt ) denote covariate shift and\\nconcept shift, respectively. Eq. 2 shows that the generation\\nprocess of domains data xt at time stamp tdepends on the\\ncorresponding dynamic latent code zw\\nt and static code zc,\\nand Eq. 3 shows that the inference process (i.e., classiﬁer to\\nproduce yt) rely on the corresponding zv\\nt and zc. Our ob-\\njective is to learn the classiﬁer p(yt|zc,zv\\nt ) which disposes\\nof covariate shift through zc and concept shift with dynamic\\nzv\\nt for the problem of evolving domain generalization.\\nDomain-related module for covariate shift.To model\\np(x1:T ,zw\\n1:T ,zc) where covariate shift involved, we set\\nthe prior distribution as p(zc) = N(0,I), p(zw\\nt |zw\\n<t) =\\nN(µ(zw\\nt ),σ2(zw\\nt )) which can be parameterized by some re-\\ncurrent neural networks (e.g., LSTM (Hochreiter & Schmid-\\nhuber, 1997)) by setting zw'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='t |zw\\n<t) =\\nN(µ(zw\\nt ),σ2(zw\\nt )) which can be parameterized by some re-\\ncurrent neural networks (e.g., LSTM (Hochreiter & Schmid-\\nhuber, 1997)) by setting zw\\n0 = 0 for initial state when t= 0,\\nand p(xt|zc,zw\\nt ) as a conditional decoder for the reconstruc-\\ntion of input data xt. To approximate the prior distributions\\np(zw\\nt |zw\\n<t), we propose to use variational inference to learn\\nan approximate posterior distribution qover latent variables\\ngiven data which can be formulated as\\nq(zw\\n1:T ,zc|x1:T ) =q(zc|x1:T )\\nT∏\\nt=1\\nq(zw\\nt |zw\\n<t,xt), (4)\\nwhere q(zc|x1:T ) and q(zw\\nt |zw\\n<t,xt) can be also parameter-\\nized by neural networks. The objective function for latent\\nfeature representation learning can be derived based on the\\nevidence lower bound (ELBO) form (Kingma & Welling,\\n2014) given as\\nLd =\\nT∑\\nt=1\\nEq(zc|xt)q(zw\\nt |zw\\n<t,xt)\\n[\\nlog p(xt|zc,zw\\nt )\\n]\\n−λ1DKL(q(zc|x1:T ),p(zc))\\n−λ2\\nT∑\\nt=1\\nDKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t)),\\n(5)\\nwhere the ﬁrst term denotes the reconstruction term for input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='T∑\\nt=1\\nEq(zc|xt)q(zw\\nt |zw\\n<t,xt)\\n[\\nlog p(xt|zc,zw\\nt )\\n]\\n−λ1DKL(q(zc|x1:T ),p(zc))\\n−λ2\\nT∑\\nt=1\\nDKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t)),\\n(5)\\nwhere the ﬁrst term denotes the reconstruction term for input\\ndata xt, the second and third term denote KL divergence\\nwhich are to align the posterior distributions zc and zw\\nt with\\nthe corresponding prior distributions.\\nCategory-related module for concept shift. To model\\np(y1:T ,zv\\n1:T |zc) for classiﬁcation purpose where concept'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nshift is involved, we propose to introduce another module\\nwhich can be easily integrated into our uniﬁed framework\\nunder domain generalization scenarios. Speciﬁcally, we\\npropose to model p(y1:T ,zv\\n1:T |zc) with a dynamic distri-\\nbution p(yt,zv\\nt |zc) where the zv\\nt is varied and encode the\\nshift information in the data category space ( e.g., the pro-\\nportion of each category). The module can be optimized\\nvia maximizing the distribution p(y1:T ,zv\\n1:T |zc) given a se-\\nquence of domains. In practise, we represent p(zv\\nt |zv\\n<t) as\\np(zv\\nt |zv\\n<t) =Cat(π(zv\\n<t)) which is a learnable categorical\\ndistribution. In a similar vein with zw\\nt , we utilize a distribu-\\ntion qto model the posterior distribution and approximate\\nthe prior distribution of zv\\nt by adopting variational inference\\ngiven as\\nq(zv\\n1:T |y1:T ) =\\nT∏\\nt=1\\nq(zv\\nt |zv\\n<t,yt), (6)\\nwhere q(zv\\nt |zv\\n<t,yt) can be parameterized by a recurrent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='the prior distribution of zv\\nt by adopting variational inference\\ngiven as\\nq(zv\\n1:T |y1:T ) =\\nT∏\\nt=1\\nq(zv\\nt |zv\\n<t,yt), (6)\\nwhere q(zv\\nt |zv\\n<t,yt) can be parameterized by a recurrent\\nneural network with categorical distribution as output. We\\nset zv\\n0 = 0 for initial state when t = 0. The proposed\\nmodule can be jointly trained with the inference process in\\nEq. 6 as well as classiﬁcation loss by maximizing\\nLc =\\nT∑\\nt=1\\nEq(zv\\nt |zv\\n<t,yt)\\n[\\nlog p(yt|zc,zv\\nt )\\n]\\n−λ3\\nT∑\\nt=1\\nDKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t)).\\n(7)\\nHere, the ﬁrst term denotes the classiﬁcation loss (i.e., max-\\nimizing log likelihood) and the second term denotes KL\\ndivergence which aims to align the posterior distribution of\\nzv\\nt with its prior distribution.\\nTemporal smooth constraint for better stability. We\\nempirically ﬁnd that the estimation of conditional den-\\nsity (i.e., p(zw\\nt |zw\\n<t) and p(zv\\nt |zv\\n<t)) which is to model\\nthe complex dynamics over temporal transition may not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='empirically ﬁnd that the estimation of conditional den-\\nsity (i.e., p(zw\\nt |zw\\n<t) and p(zv\\nt |zv\\n<t)) which is to model\\nthe complex dynamics over temporal transition may not\\nbe stable based on our formulation above. We conjecture\\nthe reason that some of the static information can be dis-\\ntorted by the dynamic inference module q(zw\\nt |zw\\n<t,xt) and\\nq(zv\\nt |zv\\n<t,yt) for better reconstruction quality, which fur-\\nther yields sub-optimal results for recognition tasks. Intu-\\nitively, one can tackle this limitation by reducing the dimen-\\nsion of latent codes zw\\nt and zv\\nt or decreasing the learning\\nrate of corresponding inference modules and prior modules\\nmanually to achieve better decoupling effect. However, we\\nﬁnd that such strategy may not lead to desired performance.\\nIn our work, we propose to employ Lipschitz constrain over\\nthe temporal domain to stabilize the learning of the dynamic\\ninference modules as follows\\n|q(zw\\nt |zw\\n<t,xt) −q(zw\\nt−1|zw\\n<t−1,xt−1)|≤ α,\\n|q(zv\\nt |zv\\n<t,yt) −q(zv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='the temporal domain to stabilize the learning of the dynamic\\ninference modules as follows\\n|q(zw\\nt |zw\\n<t,xt) −q(zw\\nt−1|zw\\n<t−1,xt−1)|≤ α,\\n|q(zv\\nt |zv\\n<t,yt) −q(zv\\nt−1|zv\\n<t−1,yt−1)|≤ α, (8)\\nwhere αis referred to as a Lipschitz constant. The above\\nregularization term is denoted as TS constraint for simplicity.\\nWe expect it can help with reducing the potential for volatile\\ntraining.\\nObjective function.Given training data S, our proposed\\nframework can be optimized through the objective function\\nLLSSAE = Ld + Lc with the temporal smooth constrains\\nin Eq. 8, where the ﬁrst term Ld and second term Lc aim\\nto tackle the problem of covariate shift and concept shift,\\nrespectively.\\nDiscussion. It is worth noting that there exists some works\\nusing probabilistic graph model for future video frame gen-\\neration task (Yingzhen & Mandt, 2018; Han et al., 2021),\\nwhich are similar to our proposed method at a high level.\\nNevertheless, our method is different since 1) our proposed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='eration task (Yingzhen & Mandt, 2018; Han et al., 2021),\\nwhich are similar to our proposed method at a high level.\\nNevertheless, our method is different since 1) our proposed\\nframework can be applied not only to generation task (as\\nwe show in the ablation study of experimental section) but\\nalso to evolving domain generalization task (which is the\\nmain focus in our paper); 2) in order to ﬁt our framework\\nto the non-stationary recognition task, we introduce a novel\\ncategory-related module to capture the concept shift, as\\nsuch, better generalization performance can be achieved.\\n3.3. Theoretical Analysis\\nIn this section, we aim to give a theoretical insight on our\\nproposed method by extending variational inference from\\nstationary environments to non-stationary environments.\\nProbabilistic model for stationary environments. We\\nﬁrst elaborate our proposed framework based on the sta-\\ntionary condition, where zc, zw and zv are introduced to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Probabilistic model for stationary environments. We\\nﬁrst elaborate our proposed framework based on the sta-\\ntionary condition, where zc, zw and zv are introduced to\\ncapture the domain-invariant category information, domain-\\nspeciﬁc and category information, respectively. We thus\\nhave the following theorem.\\nTheorem 3.1. For the data log likelihood log p(x,y) in a\\nstationary environment, we have the evidence lower bound\\nmax\\np,q\\nEzc,zw,zv\\n[\\nlog p(x|zc,zw)p(y|zc,zv)\\n]\\n−DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw))\\n−DKL(q(zv|y),p(zv)),\\n(9)\\nwhere zc ∼q(zc|x),zw ∼q(zw|x),zv ∼q(zv|y).\\nProof. We consider the data generation procedure\\np(x,y,zc,zw,zv). We have\\nlog p(x,y) =DKL(q(zc,zw,zv|x,y),p(zc,zw,zv|x,y))\\n+ Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y) ,\\n(10)\\nwhere the ﬁrst term is the KL divergence of the approximate\\nfrom the true posterior. Since this term is non-negative,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nthe second term is called the evidence lower bound on the\\nmarginal likelihood of data (x,y). This can be written as\\nlog p(x,y) ≥Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y)\\n= Ezc,zw,zv\\n[\\nlog p(x|zc,zw)p(y|zc,zv)\\n]\\n−DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw))\\n−DKL(q(zv|y),p(zv)).\\n(11)\\nThis completes the proof.\\nTheorem 3.1 shows that for a speciﬁed domain, the latent\\nspace can be decoupled into a domain invariant subspace, a\\ndomain related subspace and a category related subspace.\\nProbabilistic model for non-stationary environ-\\nments. We now extend the aforementioned analysis to\\nthe non-stationary environment. Speciﬁcally, for the\\nnon-stationary environment, we can cast the dynamic\\nvariational inference framework via modeling the sequence\\nof latent variables zw and zv as two parallel Markov\\nchains (i.e., p(zw) = p(zw\\nt |zw\\n<t) and p(zv) = p(zv\\nt |zv\\n<t)).\\nWe thus have the following theorem.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='of latent variables zw and zv as two parallel Markov\\nchains (i.e., p(zw) = p(zw\\nt |zw\\n<t) and p(zv) = p(zv\\nt |zv\\n<t)).\\nWe thus have the following theorem.\\nTheorem 3.2. By denoting p(zw) = p(zw\\nt |zw\\n<t) and\\np(zv) = p(zv\\nt |zv\\n<t)), LLSSAE is equivalent to the ELBO\\nof the data log likelihood log p(x1:T ,y1:T ) based on the\\nnon-stationary environment setting.\\nProof. We can reformulate the lower bound in Eq. 9 for a\\nnon-stationary environment as\\nlog p(x1:T ,y1:T ) ≥Eq log p(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\nq(zc,zw\\n1:T ,zv\\n1:T |x1:T ,y1:T ) ,\\n(12)\\nwhich can be written as\\nlog p(x1:T ,y1:T )\\n≥\\nT∑\\nt=1\\nEzc,zw\\nt ,zv\\nt\\n[\\nlog p(xt|zc,zw\\nt )p(y|xc,zv\\nt )\\n]\\n−DKL(q(zc|x1:T ),p(zc))\\n−\\nT∑\\nt=1\\nDKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t))\\n−\\nT∑\\nt=1\\nDKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t)),\\n(13)\\nwhere zc ∼ q(zc|x1:T ),zw\\nt ∼ q(zw\\nt |zw\\n<t,xt) and zv\\nt ∼\\nq(zv\\nt |zv\\n<t,yt). The detailed derivation procedure is pro-\\nvided in App. A. We can see that the reconstruction part for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='t |zv\\n<t)),\\n(13)\\nwhere zc ∼ q(zc|x1:T ),zw\\nt ∼ q(zw\\nt |zw\\n<t,xt) and zv\\nt ∼\\nq(zv\\nt |zv\\n<t,yt). The detailed derivation procedure is pro-\\nvided in App. A. We can see that the reconstruction part for\\nxt in the ﬁrst term together with the second and third terms\\ncan form our objective Ld for domain-related module (i.e.,\\ncovariate shift), and the combination of the reconstruction\\npart for yt in the ﬁrst term and the last term can form Lc for\\ncategory-related module (i.e., concept shift). As a result, the\\nformulation above is equivalent to our objective LLSSAE .\\nE\\nE\\n...\\n...\\n...\\n...\\nD\\nC\\nE ... ...\\nx\\ny/gid00466/gid00507/gid00021 \\n/gid00466/gid00507/gid00021 \\ny/gid00466/gid00507/gid00021 \\nx/gid00466/gid00507/gid00021 \\n/gid00031/gid00052/gid00041/gid00028/gid00040/gid00036/gid00030/gid00001 /gid00043/gid00535 z/gid00001/gid00001/gid00536\\n/gid00031/gid00052/gid00041/gid00028/gid00040/gid00036/gid00030/gid00001 /gid00043/gid00535 z/gid00001/gid00001/gid00001/gid00536'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='/gid00031/gid00052/gid00041/gid00028/gid00040/gid00036/gid00030/gid00001 /gid00043/gid00535 z/gid00001/gid00001/gid00001/gid00536\\n/gid00046/gid00047/gid00028/gid00047/gid00036/gid00030/gid00001 /gid00043/gid00535 z /gid00536\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00466\\n~\\n~\\nTS constraint \\nTS constraint \\n/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032 \\n/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032 \\n/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032 \\n/gid00030\\n/gid00049\\n/gid00050\\n/gid00050\\n/gid00049\\n/gid00030\\nz/gid00050\\n/gid00466/gid00507/gid00021 \\nz /gid00049\\n/gid00466/gid00507/gid00021 \\nz/gid00030\\n/gid00049\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00467\\n/gid00049\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00021\\n/gid00049\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00466\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00467\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00021\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 \\nF\\n/gid00030'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00467\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 /gid00021\\n/gid00050\\n/gid00043/gid00535 z /gid00001/gid00536 \\nF\\n/gid00030\\n/gid00050\\nF\\n/gid00049\\n/gid00043/gid00042/gid00046/gid00047/gid00032/gid00045/gid00036/gid00042/gid00045 \\n/gid00043/gid00042/gid00046/gid00047/gid00032/gid00045/gid00036/gid00042/gid00045 \\n/gid00043/gid00042/gid00046/gid00047/gid00032/gid00045/gid00036/gid00042/gid00045 \\nFigure 2.An overview of network architecture for LSSAE. Our\\nframework consists of the static variational encoding network Ec,\\ndynamic variational encoding networksEw and Ev, dynamic prior\\nnetworks Fw and Fv, a decoder D and a classiﬁer C. It is worth\\nnoting that we do not requireEv (i.e., only data from target domain\\nTavailable) during inference stage.\\n3.4. Implementation\\nThe implementation of network architecture for LSSAE is\\ndepicted in Fig. 2. It is composed of two parts: (1) the\\ndomain-related module (middle and bottom region); (2) the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='3.4. Implementation\\nThe implementation of network architecture for LSSAE is\\ndepicted in Fig. 2. It is composed of two parts: (1) the\\ndomain-related module (middle and bottom region); (2) the\\ncategory-related module (top region).\\nThe domain-related module consists of a static variational\\nencoding network Ec for q(zc|x1:T ), a dynamic variational\\nencoding network Ew associated with q(zw\\nt |zw\\n<t,xt), a dy-\\nnamic prior network Fw working for p(zw\\nt |zw\\n<t) and a de-\\ncoder D corresponding to p(xt|zc,zw\\nt ). Similar to V AE\\n(Kingma & Welling, 2014), we can apply the reparameteri-\\nzation trick (Kingma & Welling, 2014) to optimize param-\\neters of Ec, Ew and Fw. Speciﬁcally, we implement Ec\\nby a feature extractor which will be also utilized to extract\\nfeatures during test time. Ew can be implemented by a\\nfeature extractor with same architecture of Ec but not shar-\\ning network parameters, and followed by a LSTM network.\\nFw is implemented as a one-layer LSTM network. For the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='feature extractor with same architecture of Ec but not shar-\\ning network parameters, and followed by a LSTM network.\\nFw is implemented as a one-layer LSTM network. For the\\ncategory-related module, we design a dynamic inference\\nnetwork Ew which takes the one-hot code of label yt as\\nthe input and with the output of the categorical distribution\\nq(zv\\nt |zv\\n<t,yt), and a classiﬁer C which takes zc and zv\\nt as\\nthe input for p(yt|zc,zv\\nt ). Similarly, the prior network Fv\\nfor p(zv\\nt |zv\\n<t) is a LSTM network with a categorical distri-\\nbution as the output. After that, we use Gumbel-Softmax\\nreparameterization trick (Jang et al., 2017; Maddison et al.,\\n2017) to sample zv\\nt for optimization. Regarding the clas-\\nsiﬁer C, we utilize a linear layer by following Gulrajani\\n& Lopez-Paz (2021). The details of our architecture and\\nhyperparameters for objective function can be found in the\\nsupplementary material.\\nOptimization. We ﬁrst initialize zw and zv as zw\\n0 = 0 and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nzv\\n0 = 0, respectively. To train our framework, we generate\\nthe dynamic prior distributions p(zw\\nt |zw\\n<t) and p(zv\\nt |zv\\n<t)\\nthrough Fw and Fv respectively based onT source domains\\nS. For time stamp t, we sample a batch of data xt and take\\nit as input for Ec and Ew to obtain the parameters of their\\nposterior distributions. After that, the latent features zc\\nand zw\\nt are resampled separately through reparameterization\\ntrick and then concatenated together. Finally the decoder\\nD outputs the reconstruction data of xt. Meanwhile, Ev\\ntakes the corresponding labels yt of xt in one-hot format\\nas input and outputs the latent features, and then the latent\\nfeatures are also resampled through reparameterization trick\\nto obtain zv\\nt . The classiﬁer C takes zc and zv\\nt as the input\\nto predict the labels of xt. During training, we sample a\\nmini-batch from each single domain with the same number'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='to obtain zv\\nt . The classiﬁer C takes zc and zv\\nt as the input\\nto predict the labels of xt. During training, we sample a\\nmini-batch from each single domain with the same number\\nof data sample to form a large batch in order to suit our\\nframework to the temporal smooth constraint for stable\\ntraining. This optimization procedure of LSSAE is depicted\\nin Algorithm 1.\\nInference. To predict the label of xt sampled from one\\nof the target domains in Dt in T, we adopt Fv to infer the\\nlatent code zv\\nt and Ec to extract the latent features zc, and\\nthen apply the classiﬁer Cfor the prediction purpose. It is\\nworth noting that we do not require Ev (i.e., only data from\\ntarget domain Tavailable) during inference stage. We show\\nthis procedure in Algorithm 2.\\n4. Experiments\\nIn this section, we present experimental results to validate\\nthe effectiveness of our proposed LSSAE based on the set-\\nting of evolving domain generalization.\\n4.1. Experimental Setup'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='4. Experiments\\nIn this section, we present experimental results to validate\\nthe effectiveness of our proposed LSSAE based on the set-\\nting of evolving domain generalization.\\n4.1. Experimental Setup\\nWe compare the proposed LSSAE with other DG methods\\non two synthetic datesets (Circle and Sine) and four real-\\nworld datasets (Rotated MNIST, Portraits, Caltran, Power-\\nSupply). We also evaluate the results on two variants named\\nCircle-C and Sine-C derived from Circle and Sine via syn-\\nthesizing concept shift manually. We split the domains into\\nsource domains, intermediate domains and target domains\\nwith the ratio of {1/2 : 1/6 : 1/3}. The intermediate do-\\nmains are utilized as validation set. More details of dataset\\nconstruction can be found in the supplementary material.\\n(1) Circle/-C (Pesaranghader & Viktor, 2016). This dataset\\ncontains evolving 30 domains where the instance are sam-\\npled from 30 2D Gaussian distributions. For Circle-C, con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='(1) Circle/-C (Pesaranghader & Viktor, 2016). This dataset\\ncontains evolving 30 domains where the instance are sam-\\npled from 30 2D Gaussian distributions. For Circle-C, con-\\ncept shift is introduced via changing the center and radius\\nof decision boundary in a gradual manner over time.\\n(2) Sine/-C (Pesaranghader & Viktor, 2016). We rearrange\\nthis dataset by extending it to 24 evolving domains. To\\nsimulate concept drift for Sine-C, labels are reversed (i.e.,\\nAlgorithm 1Optimization procedure for LSSAE\\nInput: sequential source labeled datasetsS; static feature\\nextractor Ec; dynamic inference networks Ew, Ev and\\ntheir corresponding prior networks Fw, Fv; decoder D\\nand classiﬁer C.\\nRandomly initialize Ec,Ew,Ev,Fw,Fv,D,C\\nAssign zw\\n0 ,zv\\n0 ←0\\nfor t= 1,2,...,K do\\nGenerate prior distribution p(zw\\nt |zw\\n<t) via Fw\\nGenerate prior distribution p(zv\\nt |zv\\n<t) via Fv\\nfor i= 1,2,... do\\nSample a batch of data (xt,yt) from St\\n⊿Calculate Ld by Eq. 5 for Ec, Ew, Dand Fw'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generate prior distribution p(zw\\nt |zw\\n<t) via Fw\\nGenerate prior distribution p(zv\\nt |zv\\n<t) via Fv\\nfor i= 1,2,... do\\nSample a batch of data (xt,yt) from St\\n⊿Calculate Ld by Eq. 5 for Ec, Ew, Dand Fw\\n⊿Calculate Lc by Eq. 7 for Ec, Ev, Cand Fv\\n⊿Calculate temporal smooth constriction by Eq. 8\\nUpdate all modules by the summary of these loss\\nend for\\nend for\\nAlgorithm 2Inference procedure for LSSAE\\nInput: sequential target datasets T; static feature extrac-\\ntor Ec; dynamic prior network Fv and classiﬁer C.\\nAssign zv\\n0 ←0\\nfor t= 1,2,... do\\nSample zv\\nt ∼p(zv\\nt |zv\\n<t) via Fv\\nfor i= 1,2,... do\\n⊿Extract the feature for data xt via Ec\\n⊿Generate the prediction ˜ yt via C\\nend for\\nend for\\nfrom 0 to 1 or from 1 to 0) from the 6-th domain to the last\\none.\\n(3) Rotated MNIST (RMNIST)(Ghifary et al., 2015). Ro-\\ntated MNIST (RMNIST) is composed of MNIST digits\\nof various rotations. We extend it to 19 evolving do-\\nmains via applying the rotations with degree of R =\\n{0◦,15◦,30◦,..., 180◦}in order.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='tated MNIST (RMNIST) is composed of MNIST digits\\nof various rotations. We extend it to 19 evolving do-\\nmains via applying the rotations with degree of R =\\n{0◦,15◦,30◦,..., 180◦}in order.\\n(4) Portraits (Ginosar et al., 2015). This dataset comprises\\nphotos of high-school seniors from the 1905s to the 2005s\\nfor gender classiﬁcation. We split the dataset into 34 do-\\nmains by a ﬁxed internal over time.\\n(5) Caltran (Hoffman et al., 2014). Caltran is a real-world\\nsurveillance dataset comprising images captured from a\\nﬁxed trafﬁc camera deployed in an intersection. The task is\\nto predict the type of scene based on continuously evolving\\ndata. We divide it into 34 domains based on different times.\\n(6) PowerSupply (Dau et al., 2019). PowerSupply is con-\\nstructed for the time-section prediction of current power\\nsupply based on the hourly records of an Italy electricity\\ncompany. The concept shift may raise from the change in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 1.The comparison of accuracy (%) between LSSAE and other DG baselines on various datasets. As DG baselines do not take\\nconcept shift into consideration, we report the results on datasets without concept shift and with concept shift separately.\\nAlgorithm Circle Sine RMNIST Portraits Caltran Avg Circle-C Sine-C PowerSupply Avg\\nERM 49.9 63.0 43.6 87.8 66.3 62.1 34.0 61.5 71.0 55.5\\nMixup 48.4 62.9 44.9 87.8 66.0 62.0 33.9 60.9 70.8 55.2\\nMMD 50.7 55.8 44.8 87.3 57.1 59.1 33.7 52.7 70.9 52.4\\nMLDG 50.8 63.2 43.1 88.5 66.2 62.3 34.6 62.0 70.8 55.8\\nIRM 51.3 63.2 39.0 85.4 64.1 60.6 38.5 61.2 70.8 56.8\\nRSC 48.0 61.5 41.7 87.3 67.0 61.1 33.7 61.5 70.9 55.4\\nMTL 51.2 62.9 41.7 89.0 68.2 62.6 33.9 61.4 70.7 55.3\\nFish 48.8 62.3 44.2 88.8 68.6 62.5 34.3 62.7 70.8 55.9\\nCORAL 53.9 51.6 44.5 87.4 65.7 60.6 34.1 59.0 71.0 54.7\\nAndMask 47.9 69.3 42.8 70.3 56.9 57.4 37.7 52.7 70.7 53.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Fish 48.8 62.3 44.2 88.8 68.6 62.5 34.3 62.7 70.8 55.9\\nCORAL 53.9 51.6 44.5 87.4 65.7 60.6 34.1 59.0 71.0 54.7\\nAndMask 47.9 69.3 42.8 70.3 56.9 57.4 37.7 52.7 70.7 53.7\\nDIV A 67.9 52.9 42.7 88.2 69.2 64.2 33.9 52.9 70.8 55.1\\nLSSAE (Ours) 73.8 71.4 46.4 89.1 70.6 70.3 44.8 60.8 71.1 58.9\\nseason, weather or price. We form 30 domains according to\\ndays.\\nThe methods for comparison include: (1) ERM (Vapnik,\\n1998); (2) Mixup (Yan et al., 2020); (3) MMD (Li et al.,\\n2018b); (4) MLDG (Li et al., 2018a); (5) IRM (Rosen-\\nfeld et al., 2021); (6) RSC (Huang et al., 2020);\\n(7) MTL (Blanchard et al., 2021); (8) Fish (Shi et al., 2021);\\n(9) CORAL (Sun & Saenko, 2016); (10) AndMask (Paras-\\ncandolo et al., 2021); (11) DIV A (Ilse et al., 2020). All of our\\nexperiments are implemented in the PyTorch platform based\\non DomainBed package (Gulrajani & Lopez-Paz, 2021). For\\na fair comparison, we keep the neural network architecture\\nof encoding part and classiﬁcation part to be same for all'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='on DomainBed package (Gulrajani & Lopez-Paz, 2021). For\\na fair comparison, we keep the neural network architecture\\nof encoding part and classiﬁcation part to be same for all\\nbaselines for different benchmarks.\\n4.2. Quantitative Results\\nThe results of of our proposed LSSAE and baselines are\\npresented in Table 1. As conventional DG methods focus\\nupon covariate shift only, we separate the datasets according\\nto with or without concept shift into two parts for fairness.\\nWe can see that LSSAE consistently outperforms other base-\\nlines over all datasets, it achieves 70.3% accuracy when\\nthere exists covariate shift only (Circle, Sine, RMNIST, Por-\\ntraits and Caltran), and achieves58.9% accuracy when there\\nexist concept shift (Circle-C, Sine-C, PowerSupply). The\\nresults are signiﬁcantly better than the compared DG ap-\\nproaches, which are reasonable since existing DG methods\\ncannot deal with distribution shift well in non-stationary\\nenvironments but our proposed LSSAE can properly cap-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='proaches, which are reasonable since existing DG methods\\ncannot deal with distribution shift well in non-stationary\\nenvironments but our proposed LSSAE can properly cap-\\nture the evolving patterns to gain better performance. More\\nresults can be found in the supplementary materials.\\nTo better understand the rationality of our method, we vi-\\nsualize the decision boundaries of our method and ERM\\nbaseline on two synthetic datasets: Circle and Sine by com-\\nparing our proposed method with ERM baseline. The vi-\\nsualization results are depicted in Fig. 3 and Fig. 4. As\\n(a) Data\\nEvolving P(X)\\n(b) Ground Truth (c) ERM (d) LSSAE\\nStatic P(Y|X)\\nEvolving P(Y|X)\\nCircle Circle-C \\nFigure 3.Decision boundary visualization of Circle and Circle-C\\ndatasets each with 30 domains. (a) presents the original data in\\ndifferent domains by color, where the right half part are source\\ndomains. (b) shows the positive and negative labels in red and\\nblue dots. (c) and (d) are decision boundaries learned by ERM and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='different domains by color, where the right half part are source\\ndomains. (b) shows the positive and negative labels in red and\\nblue dots. (c) and (d) are decision boundaries learned by ERM and\\nLSSAE, respectively.\\nEvolving P(X)\\n(a) Data (b) Ground Truth (c) ERM (d) LSSAE\\nStatic P(Y|X)\\nEvolving P(Y|X)\\nSine Sine-C \\nFigure 4.Decision boundary visualization of Sine and Sine-C\\ndatasets. (a) presents 24 domains indexed by different colors,\\nwhere the left half part are source domains. (b) shows the positive\\nand negative labels in red and blue dots. (c) and (d) are decision\\nboundaries learned by ERM and LSSAE, respectively.\\nshown in Fig. 3, both of ERM and LSSAE can ﬁt the source\\ndomains well. However, different from ERM which only ﬁt\\nthe source domains, LSSAE shows a desired generalization\\nability to unseen target domains. This validates that our\\nLSSAE can capture the underlying evolving patterns across\\ndomains to achieve better results. As for Circle-C where we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='ability to unseen target domains. This validates that our\\nLSSAE can capture the underlying evolving patterns across\\ndomains to achieve better results. As for Circle-C where we\\nintroduce evolving concept shift via modifying the center\\nand radius of decision boundary over a period of time, we\\nobserve that our proposed LSSAE can still produce a more\\naccurate decision boundary compared with ERM. Similar\\nobservation can be found in Fig. 4, where LSSAE can be\\nbetter generalized to evolving domains compared with ERM.\\nHowever, we ﬁnd that our proposed LSSAE may not be able'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\n(a) random data sequences \\n(b) reconstructions\\n(c) generated sequences with fixed\\n(d) generated sequences with fixed\\nzc\\nzwt\\n/gid00083\\nFigure 5.Visualisation of generated and reconstructed data se-\\nquences on RMNIST dataset.\\nto obtain a desired boundary based on the setting of Sine-C,\\nwhere we introduce concept shift by reversing the labels.\\nWe conjecture the reason that our proposed LSSAE may\\nnot be able to well handle the abrupt concept shift where no\\ncontinuous evolving pattern exists.\\n4.3. Ablation Study\\nAnalysis on domain-related module.To better evaluate\\nwhether our proposed LSSAE can capture domain spe-\\nciﬁc information, we conduct experiments by evaluating\\nthe reconstruction and generation capability of LSSAE on\\nRMNIST dataset. The reconstruction and generation re-\\nsults are shown in Fig. 5. Each subﬁgure shows two se-\\nquences which follow the domain order (i.e., rotation degree'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='RMNIST dataset. The reconstruction and generation re-\\nsults are shown in Fig. 5. Each subﬁgure shows two se-\\nquences which follow the domain order (i.e., rotation degree\\ngradually evolves) from the left to the right. Subﬁgure (a)\\nshows the original data sequence sampled from source do-\\nmains (green bounding box), intermediate domains (orange\\nbounding box) and target domains (pink bounding box) in\\norder. Noted that we are only interested in degree of rota-\\ntion thus the category of images from different domains may\\nnot be consistent. Subﬁgure (b) shows the reconstructions\\nfor each sample in Subﬁgure (a). Based on the results, we\\nﬁnd that we can achieve a desired reconstruction quality,\\nwhich suggests that the domain-related information can be\\ncaptured even we random the category related information.\\nTo further show that our proposed method is capable for\\ngeneration task, in subﬁgure (c), we visualize the randomly\\ngenerated samples using zw\\nt ∼p(zw\\nt |zw\\n<t) while keeping zc'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='To further show that our proposed method is capable for\\ngeneration task, in subﬁgure (c), we visualize the randomly\\ngenerated samples using zw\\nt ∼p(zw\\nt |zw\\n<t) while keeping zc\\nthe same for all domains. We can see that the category infor-\\nmation (i.e., digit information) remains unchanged when we\\nﬁx zc, and can generated samples for unseen future domains\\nwhere the rotation degree of digit is gradually changed with\\nevolving prior p(zw\\nt |zw\\n<t). This observation suggests that\\nour proposed method can be used for data augmentation\\nwhen adapting to the unseen target domains. However, we\\nalso observe that there exists image quality distortion when\\ngenerating new domains. We conjecture the reason that the\\nTable 2.Comparison of different prior distributions for category-\\nrelated module on PowerSupply dataset.\\nPrior Type Accuracy (%)\\nWithout zv 70.7\\nGaussian 70.1\\nUniform 71.0\\nCategorical (Ours) 71.1\\nrange of degrees for training may not be sufﬁciently diverse,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='related module on PowerSupply dataset.\\nPrior Type Accuracy (%)\\nWithout zv 70.7\\nGaussian 70.1\\nUniform 71.0\\nCategorical (Ours) 71.1\\nrange of degrees for training may not be sufﬁciently diverse,\\nwhich limits the performance of generation to unseen digit\\nrotation degrees. Subﬁgure (d) shows randomly generated\\ndata via sampling zc ∼p(zc) while keeping zw\\nt the same for\\nall domains at different time stamp t(i.e., zw\\nt = zw\\n1 ), where\\nwe ﬁnd that the generated digit images belong to different\\ncategories but with the same rotation degree, which further\\nindicates that our proposed method can successfully extract\\ndomain related information.\\nAnalysis on category-related module.We then evaluate\\nthe effectiveness of our proposed category-related module\\non PowerSupply dataset. To this end, we consider three\\ndifferent ablation studies by 1) removing category-related\\nmodule, 2) replacing the learnable categorical prior with a\\nlearnable Gaussian prior, 3) replacing the learnable categor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='different ablation studies by 1) removing category-related\\nmodule, 2) replacing the learnable categorical prior with a\\nlearnable Gaussian prior, 3) replacing the learnable categor-\\nical prior with a ﬁx Uniform prior. The results are shown\\nin Table 2. As we can see, our proposed LSSAE based on\\ncategorical prior can achieve the best performance among\\nother baselines, which shows the effectiveness of our pro-\\nposed method. However, we observe that the performance\\ndrops to some extent by replacing the categorical prior with\\nGaussian prior, which we conjecture the reason that cate-\\ngorical distribution can better capture the category related\\ninformation which is discrete. We also observe that better\\nperformance can be achieved by comparing with the results\\nusing Uniform distribution as prior, which is reasonable\\nsince Uniform distribution does not change over time.\\nAnalysis on temporal smooth constraint. The smooth\\nconstraint is mainly designed for stabilizing the training'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='since Uniform distribution does not change over time.\\nAnalysis on temporal smooth constraint. The smooth\\nconstraint is mainly designed for stabilizing the training\\nprocedure. To verify this, we conduct experiments by con-\\nsidering “with & without this constraint” on RMNIST and\\nCalTran datasets while keeping other hyperparameters the\\nsame. The results are reported as the variance of the test\\naccuracy in the last ﬁve epochs (See Table 3). For RMNIST,\\nwhen training with constraint, the variance is 0.4; without\\nconstraint, the variance is 3.6. For CalTran, when training\\nwith constraint, the variance is 4.7, and without constraint,\\nthe variance is 10.0. We can see that there exists an obvious\\ndecrease in terms of variance when incorporating with our\\nproposed TS constraint. Besides, as our smooth constraint\\nhelps stabilize the training process, better performance can\\nbe expected. For RMNIST, training with our smooth con-\\nstraint can achieve 0.9% performance gain; For CalTran,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 3.Ablation study on temporal smooth constraint.\\nTS Constraint RMNIST CalTran\\nVar ↓ Acc (%)↑ Var ↓ Acc (%)↑\\n× 3.6 45.5 10.0 69.9\\n✓ 0.4 46.4 4.7 70.6\\nthe performance gain is 0.7%.\\n5. Conclusion\\nIn this paper, we propose to focus on the problem of evolv-\\ning domain generalization, where the covariate shift and\\nconcept shift vary over time. To tackle this problem, we\\npropose a novel framework LSSAE to model the dynamics\\nof distribution shift (i.e., covariate shift and concept shift).\\nWe also provide theoretical analysis, which shows that our\\nproposed method is equivalent to maximizing the ELBO\\nbased on the non-stationary environment setting, and justi-\\nﬁes the rationality of our proposed method for the problem\\nof evolving domain generalization. Experimental results\\non both toy data and real-world datasets across multiple\\ndomains further indicate the signiﬁcance of our proposed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='of evolving domain generalization. Experimental results\\non both toy data and real-world datasets across multiple\\ndomains further indicate the signiﬁcance of our proposed\\nmethod based on this setting.\\nAcknowledgements\\nThis work was supported in part by CityU New Research\\nInitiatives/Infrastructure Support from Central (APRC\\n9610528), CityU Applied Research Grant (ARG 9667244)\\nand Hong Kong Innovation and Technology Commission\\n(InnoHK Project CIMDA). Besides, this work was also sup-\\nported in part by the National Natural Science Foundation of\\nChina under 62022002, in part by the Hong Kong Research\\nGrants Council General Research Fund (GRF) under Grant\\n11203220.\\nReferences\\nAlbuquerque, I., Monteiro, J., Darvishi, M., Falk, T. H.,\\nand Mitliagkas, I. Generalizing to unseen domains via\\ndistribution matching. arXiv preprint arXiv:1911.00804,\\n2021.\\nBalaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg:\\nTowards domain generalization using meta-regularization.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='distribution matching. arXiv preprint arXiv:1911.00804,\\n2021.\\nBalaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg:\\nTowards domain generalization using meta-regularization.\\nAdvances in Neural Information Processing Systems, 31:\\n998–1008, 2018.\\nBlanchard, G., Lee, G., and Scott, C. Generalizing from sev-\\neral related classiﬁcation tasks to a new unlabeled sample.\\nAdvances in Neural Information Processing Systems, 24:\\n2178–2186, 2011.\\nBlanchard, G., Deshmukh, A. A., Dogan, ¨U., Lee, G., and\\nScott, C. Domain generalization by marginal transfer\\nlearning. J. Mach. Learn. Res., 22:2–1, 2021.\\nChen, H.-Y . and Chao, W.-L. Gradual domain adaptation\\nwithout indexed intermediate domains. In Thirty-Fifth\\nConference on Neural Information Processing Systems,\\n2021.\\nDau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu,\\nY ., Gharghabi, S., Ratanamahatana, C. A., and Keogh,\\nE. The ucr time series archive. IEEE/CAA Journal of\\nAutomatica Sinica, 6(6):1293–1305, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Y ., Gharghabi, S., Ratanamahatana, C. A., and Keogh,\\nE. The ucr time series archive. IEEE/CAA Journal of\\nAutomatica Sinica, 6(6):1293–1305, 2019.\\nDou, Q., Coelho de Castro, D., Kamnitsas, K., and Glocker,\\nB. Domain generalization via model-agnostic learning\\nof semantic features. Advances in Neural Information\\nProcessing Systems, 32:6450–6461, 2019.\\nFederici, M., Tomioka, R., and Forr ´e, P. An information-\\ntheoretic approach to distribution shifts. In Thirty-Fifth\\nConference on Neural Information Processing Systems,\\n2021.\\nGhifary, M., Kleijn, W. B., Zhang, M., and Balduzzi, D. Do-\\nmain generalization for object recognition with multi-task\\nautoencoders. In Proceedings of the IEEE International\\nConference on Computer Vision, pp. 2551–2559, 2015.\\nGinosar, S., Rakelly, K., Sachs, S., Yin, B., and Efros, A. A.\\nA century of portraits: A visual historical record of amer-\\nican high school yearbooks. In 2015 IEEE International\\nConference on Computer Vision Workshop (ICCVW), pp.\\n652–658, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='A century of portraits: A visual historical record of amer-\\nican high school yearbooks. In 2015 IEEE International\\nConference on Computer Vision Workshop (ICCVW), pp.\\n652–658, 2015.\\nGong, R., Li, W., Chen, Y ., and Gool, L. V . Dlow: Domain\\nﬂow for adaptation and generalization. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pp. 2477–2486, 2019.\\nGulrajani, I. and Lopez-Paz, D. In search of lost domain\\ngeneralization. In International Conference on Learning\\nRepresentations, 2021.\\nHan, J., Min, M. R., Han, L., Li, L. E., and Zhang, X.\\nDisentangled recurrent wasserstein autoencoder. In Inter-\\nnational Conference on Learning Representations, 2021.\\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\\nNeural Computation, 9(8):1735–1780, 1997.\\nHoffman, J., Darrell, T., and Saenko, K. Continuous man-\\nifold based adaptation for evolving visual domains. In\\nProceedings of the IEEE Conference on Computer Vision'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Hoffman, J., Darrell, T., and Saenko, K. Continuous man-\\nifold based adaptation for evolving visual domains. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 867–874, 2014.\\nHuang, Z., Wang, H., Xing, E. P., and Huang, D. Self-\\nchallenging improves cross-domain generalization. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nComputer Vision–ECCV 2020: 16th European Confer-\\nence, Glasgow, UK, August 23–28, 2020, Proceedings,\\nPart II 16, pp. 124–140, 2020.\\nIlse, M., Tomczak, J. M., Louizos, C., and Welling, M. Diva:\\nDomain invariant variational autoencoders. In Medical\\nImaging with Deep Learning, pp. 322–348. PMLR, 2020.\\nJang, E., Gu, S., and Poole, B. Categorical reparameteriza-\\ntion with gumbel-softmax. International Conference on\\nLearning Representations, 2017.\\nKingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization. In International Conference on Learning\\nRepresentations, 2015.\\nKingma, D. P. and Welling, M. Auto-encoding variational\\nbayes. International Conference on Learning Represen-\\ntations, 2014.\\nKumar, A., Ma, T., and Liang, P. Understanding self-training\\nfor gradual domain adaptation. In International Confer-\\nence on Machine Learning, pp. 5468–5479. PMLR, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='tations, 2014.\\nKumar, A., Ma, T., and Liang, P. Understanding self-training\\nfor gradual domain adaptation. In International Confer-\\nence on Machine Learning, pp. 5468–5479. PMLR, 2020.\\nLao, Q., Jiang, X., Havaei, M., and Bengio, Y . Continu-\\nous domain adaptation with variational domain-agnostic\\nfeature replay. arXiv preprint arXiv:2003.04382, 2020.\\nLi, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Learn-\\ning to generalize: Meta-learning for domain generaliza-\\ntion. In Thirty-Second AAAI Conference on Artiﬁcial\\nIntelligence, 2018a.\\nLi, H., Pan, S. J., Wang, S., and Kot, A. C. Domain general-\\nization with adversarial feature learning. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp. 5400–5409, 2018b.\\nLi, W., Xu, Z., Xu, D., Dai, D., and Van Gool, L. Domain\\ngeneralization and adaptation using low rank exemplar\\nsvms. IEEE Transactions on Pattern Analysis and Ma-\\nchine Intelligence, 40(5):1114–1127, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='generalization and adaptation using low rank exemplar\\nsvms. IEEE Transactions on Pattern Analysis and Ma-\\nchine Intelligence, 40(5):1114–1127, 2017.\\nLiu, H., Long, M., Wang, J., and Wang, Y . Learning to adapt\\nto evolving domains. In Advances in Neural Information\\nProcessing Systems, volume 33, pp. 22338–22348, 2020.\\nMaddison, C. J., Mnih, A., and Teh, Y . W. The concrete\\ndistribution: A continuous relaxation of discrete random\\nvariables. International Conference on Learning Repre-\\nsentations, 2017.\\nMancini, M., Bulo, S., Caputo, B., and Ricci, E. Adagraph:\\nUnifying predictive and continuous domain adaptation\\nthrough graphs. In 2019 IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pp. 6561–\\n6570, jun 2019.\\nMuandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain\\ngeneralization via invariant feature representation. In\\nInternational Conference on Machine Learning, pp. 10–\\n18, 2013.\\nNguyen, A. T., Tran, T., Gal, Y ., and Baydin, A. G. Do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='generalization via invariant feature representation. In\\nInternational Conference on Machine Learning, pp. 10–\\n18, 2013.\\nNguyen, A. T., Tran, T., Gal, Y ., and Baydin, A. G. Do-\\nmain invariant representation learning with domain den-\\nsity transformations. arXiv preprint arXiv:2102.05082,\\n2021.\\nParascandolo, G., Neitz, A., ORVIETO, A., Gresele, L., and\\nSch¨olkopf, B. Learning explanations that are hard to vary.\\nIn International Conference on Learning Representations,\\n2021.\\nPark, S. W., Shu, D. W., and Kwon, J. Generative adversarial\\nnetworks for markovian temporal dynamics: Stochastic\\ncontinuous data generation. In Proceedings of the 38th\\nInternational Conference on Machine Learning, volume\\n139, pp. 8413–8421, 18–24 Jul 2021.\\nPesaranghader, A. and Viktor, H. L. Fast hoeffding\\ndrift detection method for evolving data streams. In\\nECML/PKDD, 2016.\\nRosenfeld, E., Ravikumar, P. K., and Risteski, A. The\\nrisks of invariant risk minimization. In International'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='drift detection method for evolving data streams. In\\nECML/PKDD, 2016.\\nRosenfeld, E., Ravikumar, P. K., and Risteski, A. The\\nrisks of invariant risk minimization. In International\\nConference on Learning Representations, 2021.\\nShankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S.,\\nJyothi, P., and Sarawagi, S. Generalizing across domains\\nvia cross-gradient training. In International Conference\\non Learning Representations, 2018.\\nShi, Y ., Seely, J., Torr, P. H., Siddharth, N., Hannun, A.,\\nUsunier, N., and Synnaeve, G. Gradient matching for\\ndomain generalization. arXiv preprint arXiv:2104.09937,\\n2021.\\nSugiyama, M., Yamada, M., and du Plessis, M. C. Learning\\nunder nonstationarity: covariate shift and class-balance\\nchange. Wiley Interdisciplinary Reviews: Computational\\nStatistics, 5(6):465–477, 2013.\\nSun, B. and Saenko, K. Deep coral: Correlation alignment\\nfor deep domain adaptation. In Hua, G. and J ´egou, H.\\n(eds.), Computer Vision – ECCV 2016 Workshops , pp.\\n443–450, Cham, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Sun, B. and Saenko, K. Deep coral: Correlation alignment\\nfor deep domain adaptation. In Hua, G. and J ´egou, H.\\n(eds.), Computer Vision – ECCV 2016 Workshops , pp.\\n443–450, Cham, 2016.\\nTahmasbi, A., Jothimurugesan, E., Tirthapura, S., and Gib-\\nbons, P. B. Driftsurf: Stable-state/reactive-state learning\\nunder concept drift. In International Conference on Ma-\\nchine Learning, pp. 10054–10064. PMLR, 2021.\\nTorralba, A. and Efros, A. A. Unbiased look at dataset bias.\\nIn CVPR 2011, pp. 1521–1528. IEEE, 2011.\\nVapnik, V . Statistical learning theory wiley.New York, 1998.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nVergara, A., Vembu, S., Ayhan, T., Ryan, M. A., Homer,\\nM. L., and Huerta, R. Chemical gas sensor drift compen-\\nsation using classiﬁer ensembles. Sensors and Actuators\\nB: Chemical, 166:320–329, 2012.\\nV olpi, R., Namkoong, H., Sener, O., Duchi, J. C., Murino,\\nV ., and Savarese, S. Generalizing to unseen domains via\\nadversarial data augmentation. In Advances in Neural\\nInformation Processing Systems, volume 31, 2018.\\nWang, H., He, H., and Katabi, D. Continuously indexed\\ndomain adaptation. In III, H. D. and Singh, A. (eds.),\\nProceedings of the 37th International Conference on Ma-\\nchine Learning, volume 119, pp. 9898–9907, 13–18 Jul\\n2020.\\nWang, Y ., Li, H., Chau, L.-P., and Kot, A. C. Variational dis-\\nentanglement for domain generalization. arXiv preprint\\narXiv:2109.05826, 2021.\\nWulfmeier, M., Bewley, A., and Posner, I. Incremental\\nadversarial domain adaptation for continually changing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='entanglement for domain generalization. arXiv preprint\\narXiv:2109.05826, 2021.\\nWulfmeier, M., Bewley, A., and Posner, I. Incremental\\nadversarial domain adaptation for continually changing\\nenvironments. In 2018 IEEE International Conference on\\nRobotics and Automation (ICRA), pp. 4489–4495. IEEE,\\n2018.\\nYan, S., Song, H., Li, N., Zou, L., and Ren, L. Improve un-\\nsupervised domain adaptation with mixup training. arXiv\\npreprint arXiv:2001.00677, 2020.\\nYingzhen, L. and Mandt, S. Disentangled sequential autoen-\\ncoder. In International Conference on Machine Learning,\\npp. 5670–5679. PMLR, 2018.\\nZhou, K., Yang, Y ., Qiao, Y ., and Xiang, T. Domain general-\\nization with mixstyle. arXiv preprint arXiv:2104.02008,\\n2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nA. Proofs\\nA.1. Derivation of ELBO for Stationary Environments\\nAs we introduce two latent variables to account for the two types of distribution shift, the data generating procedure for one\\ndomain can be expressed as:\\np(x,y,zc,zw,zv) =p(zc)p(zw)p(zv)p(x|zc,zw)p(y|zc,zv)\\n= p(zw)p(zv)\\nN∏\\ni=1\\np(zc\\ni )p(xi|zc\\ni ,zw)p(yi|zc\\ni ,zv)\\n(14)\\nWe let (zc,zw) be the latent variables for x, and zv be latent variable for y, thus the distribution of these three latent\\nvariables can be inferred from the observable datapoints as p(zc|x),p(zw|x), p(zv|y), respectively. The joint distribution of\\nlatent variables is:\\np(zc,zw,zv|x,y) =p(zc|x)p(zw|x)p(zv|y) (15)\\nFor our approximating distribution in Eq 15, we can choose the form q(zc,zw,zv|x,y) =q(zc|x)q(zw|x)q(zv|y). Thus,\\nwe can get:\\nDKL(q,p) =Eq log q(zc,zw,zv|x,y)\\np(zc,zw,zv|x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv)p(x,y)\\n= Eq log q(zc,zw,zv|x,y)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='we can get:\\nDKL(q,p) =Eq log q(zc,zw,zv|x,y)\\np(zc,zw,zv|x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv)p(x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv) + Eq log p(x,y)\\n= Eq log q(zc,zw,zv|x,y)\\np(x,y,zc,zw,zv) + logp(x,y)\\n(16)\\nWe can get:\\nlog p(x,y) =DKL(q,p) +Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y) (17)\\nAs DKL(q,p) ≥0, the variational lower bound for log p(x,y) is:\\nL= Eq log p(x,y,zc,zw,zv)\\nq(zc,zw,zv|x,y) . (18)\\nThis formulation can be reorganized as:\\nL= Eq log p(x,y|zc,zw,zv)p(zc)p(zw)p(zv)\\nq(zc|x)q(zw|x)q(zv|y)\\n= Eq log p(x,y|zc,zw,zv) +Eq log p(zc)\\nq(zc|x) + Eq log p(zw)\\nq(zw|x) + Eq log p(zv)\\nq(zv|y)\\n= Eq log p(x,y|zc,zw,zv) −DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw)) −DKL(q(zv|y),p(zv)).\\n(19)\\nAs p(x,y|zc,zw,zv) =p(x|zc,zw)p(y|zc,zv), the above formulation can be rewrote as:\\nL≥ Eq log p(x|zc,zw)p(y|zc,zv) −DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw)) −DKL(q(zv|y),p(zv)). (20)\\nA.2. Derivation of ELBO for Non-stationary Environments'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='L≥ Eq log p(x|zc,zw)p(y|zc,zv) −DKL(q(zc|x),p(zc)) −DKL(q(zw|x),p(zw)) −DKL(q(zv|y),p(zv)). (20)\\nA.2. Derivation of ELBO for Non-stationary Environments\\nWe assume the prior distribution of latent variableszw and zv satisfy Markov property. This means each of them relies on\\nthe value of their previous states:\\np(zw) =p(zw\\nt |zw\\n<t),p(zv) =p(zv\\nt |zv\\n<t), (21)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nThe joint distribution of data and latent variables is:\\np(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\n=\\nT∏\\nt=1\\np(xt,yt|zc,zw\\nt ,zv\\nt )p(zc)p(zw\\nt |zw\\n<t)p(zv\\nt |zv\\n<t)\\n=\\nT∏\\nt=1\\np(zw\\nt |zw\\n<t)p(zv\\nt |zv\\n<t)\\nNt∏\\ni=1\\np(zc\\ni )p(xi|zc\\ni ,zw\\nt )p(yi|zc\\ni ,zv\\nt )\\n(22)\\nwhere p(zw\\n1 ) =p(zw\\n1 |zw\\n0 ) and p(zv\\n1) =p(zv\\n1|zv\\n0).\\nFor a non-stationary environment, we assume that (zc,zw\\nt ) are the latent variables for xt and zv\\nt is the latent variable for yt\\nat t-th time stamp. Thus the distribution of the latent variables for t-th time stamp can be express as p(zc|xt), p(zw\\nt |xt) and\\np(zv\\nt |yt), respectively. We employq(zc|xt), q(zw\\nt |zw\\n<t,xt) and q(zv\\nt |zv\\n<t,yt) to approximate the prior distributions here.\\nSimilar to Eq 18, we can draw the variational lower bound for log p(x1:T ,y1:T ) as:\\nL= Eq log p(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\nq(zc,zw\\n1:T ,zv\\n1:T |x1:T ,y1:T ) . (23)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Similar to Eq 18, we can draw the variational lower bound for log p(x1:T ,y1:T ) as:\\nL= Eq log p(x1:T ,y1:T ,zc,zw\\n1:T ,zv\\n1:T )\\nq(zc,zw\\n1:T ,zv\\n1:T |x1:T ,y1:T ) . (23)\\nWhen incorporating with Eq 22, this can be reorganized as:\\nL= Eq log\\n∏T\\nt=1 p(xt, yt|zc, zw\\nt , zv\\nt )p(zc)p(zw\\nt |zw\\n<t)p(zv\\nt |zv\\n<t)∏T\\nt=1 q(zc|xt)q(zw\\nt |zw\\n<t, xt)q(zv\\nt |zv\\n<t, yt)\\n= Eq\\n[\\nlog\\n∏T\\nt=1 p(zw\\nt |zw\\n<t)∏T\\nt=1 q(zw\\nt |zw\\n<t, xt)\\n+ log\\n∏T\\nt=1 p(zv\\nt |zv\\n<t)∏T\\nt=1 q(zv\\nt |zv\\n<t, yt)\\n+ log\\n∏T\\nt=1 p(zc)∏T\\nt=1 q(zc|xt)\\n+ log\\nT∏\\nt=1\\np(xt|zc\\nt, zw\\nt )p(yt|zc\\nt, zv\\nt )\\n]\\n= Eq\\n[\\n−\\nT∑\\nt=1\\nlog q(zw\\nt |zw\\n<t, xt)\\np(zw\\nt |zw\\n<t) −\\nT∑\\nt=1\\nlog q(zv\\nt |zv\\n<t, yt)\\np(zv\\nt |zv\\n<t) −\\nT∑\\nt=1\\nlog q(zc\\nt|xt)\\np(zc\\nt)\\n+\\nT∑\\nt=1\\nlog p(xt|zc, zw\\nt )p(yt|zc, zv\\nt )\\n]\\n(24)\\nBy Jensen’s inequality, the above formulation can be rewrote as:\\nL≥ Eq\\n[ T∑\\nt=1\\nlog p(xt|zc,zw\\nt )p(yt|zc,zv\\nt ) −DKL(q(zc|xt),p(zc)) −DKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t))\\n−DKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t))\\n] (25)\\nThis complements the proof.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='L≥ Eq\\n[ T∑\\nt=1\\nlog p(xt|zc,zw\\nt )p(yt|zc,zv\\nt ) −DKL(q(zc|xt),p(zc)) −DKL(q(zw\\nt |zw\\n<t,xt),p(zw\\nt |zw\\n<t))\\n−DKL(q(zv\\nt |zv\\n<t,yt),p(zv\\nt |zv\\n<t))\\n] (25)\\nThis complements the proof.\\nB. Probabilistic Generative Model of LSSAE\\nIn Fig. 6, we present the complete probabilistic generative graph of our LSSAE. The left part is the DAG of LSSAE presented\\nin Fig. 1(d) where we did not include zc in this ﬁgure as the main focus of Fig. 1 is the dynamic factors (i.e., W and V) for\\npresenting our main idea of evolving dynamics at a high level instead of DG. In the right part, we illustrate the relationship\\nbetween our DAG and probabilistic generative model (with zc). Here, the dependence of Xt on Yt (the dotted blue line) is\\nsubstituted with zc (the solid red line) which mainly captures the static category information in data sample space.\\nC. Additional Details on the Experimental Setup\\nC.1. Datasets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='substituted with zc (the solid red line) which mainly captures the static category information in data sample space.\\nC. Additional Details on the Experimental Setup\\nC.1. Datasets\\nOur experiments are conducted on 2 synthetic and 4 real-world datasets presented in Table 4. More detailed description are\\ngiven below.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nVt-1 Vt ...\\n(d)\\nXt-1 Yt-1 \\nWt-1 \\nX t Yt\\nWt ......\\n... zv\\nt-1 zv\\nt ...\\nProbabilisitic Generative Model of (d)\\nXt-1 Yt-1 \\nzw\\nt-1 \\nX t Yt\\nzw\\nt ......\\n...\\nzc zc\\nFigure 6.The probabilistic generative graph of LSSAE with zc presented. Here, the dependence of Xt on Yt in Fig. 1(d) is substituted\\nwith zc which mainly captures the static category information in data sample space.\\nTable 4.Brief description of employed benchmarks in this work.\\nDataset Type Number Source Domains Intermediate Domains Target Domains Total Domains\\nCircle/-C Digital 3,000 15 5 10 30\\nSine/-C Digital 2,280 12 4 8 24\\nRMNIST Image 70,000 10 3 6 19\\nPortraits Image 37,921 19 5 10 34\\nCalTran Image 5,450 19 5 10 34\\nPowerSupply Digital 29,928 15 5 10 30\\n• Circle (Pesaranghader & Viktor, 2016): Each data in this dataset owns two attributes (x,y),x,y ∈[0,1]. The label is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='CalTran Image 5,450 19 5 10 34\\nPowerSupply Digital 29,928 15 5 10 30\\n• Circle (Pesaranghader & Viktor, 2016): Each data in this dataset owns two attributes (x,y),x,y ∈[0,1]. The label is\\nassigned using a circle curve as the decision boundary following (x−x0)2 + (y−y0)2 ≤r2, where (x0,y0) are the\\nlocation of the center and ris the radius of this circle. To generate Circle-C, we inject gradual shift via modifying the\\nvalue of x0 continuously throughout the domains.\\n• Sine (Pesaranghader & Viktor, 2016): Each data in this dataset owns two attributes(x,y),x,y ∈[0,1]. The label is\\nassigned using a sine curve as the decision boundary following y≤sin(x). To simulate abrupt concept drift for Sine-C,\\nlabels are reversed (i.e., from 0 to 1 or from 1 to 0) from the 6-th domain to the last one.\\n• RMNIST (Ghifary et al., 2015): This dataset is composed of MNIST digits of various rotations. We generate 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='• RMNIST (Ghifary et al., 2015): This dataset is composed of MNIST digits of various rotations. We generate 19\\ndomains via applying the rotations with degree of R= {0◦,15◦,30◦,..., 180◦}on each domain. Note that each image\\nis seen at exactly one angle, so the training procedure cannot track a single image across different angles.\\n• Portraits (Ginosar et al., 2015): This is a real-world dataset of photos collected in American high school seniors. The\\nportraits are taken over 108 years (1905-2013) across 26 states. The goal is to classify the gender for each photo. We\\nsplit the dataset into 34 domains by a ﬁxed internal along time.\\n• CalTran (Hoffman et al., 2014): This dataset contains real-world images captured by a ﬁxed trafﬁc camera deployed in\\nan intersection over time. Frames were updated at 3 minute intervals each with a resolution 320 ×320. We divide\\nit into 34 domains by time. This is a scene classiﬁcation task to determine whether one or more cars are present in,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='it into 34 domains by time. This is a scene classiﬁcation task to determine whether one or more cars are present in,\\nor approaching the intersection. The challenge mainly raise from the continually evolving domain shift as changes\\ninclude time, illumination, weather, etc.\\n• PowerSupply (Dau et al., 2019): This dataset is comprised of records of hourly power supply collected by an Italy\\nelectricity company. We form 30 domains according to days. Each data is assigned by a binary class label which\\nrepresents which time of day the current power supply belongs to (i.e., am or pm). The concept shift may results from\\nthe change in season, weather, price or the differences between working days and weekend.\\nC.2. Model Architecture & Hyperparameters\\nNeural network architectures used for each dataset:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 5.Model architectures for different datasets.\\nDataset Encoder Decoder\\nCircle/-C\\nNon-linear Encoder Non-linear Decoder Sine/-C\\nPowerSupply\\nRMNIST MNIST ConvNet MNIST ConvTranNet\\nPortraits ResNet-18 ConvTranNetCalTran\\nNeural network architecture for digital experiments (Circle/-C, Sine/-C, PowerSupply):\\nTable 6.Implementation of Non-linear Encoder.\\n# Layer\\n1 Linear(in= d, output=512)\\n2 ReLU\\n3 Linear(in=512, output=512)\\n4 ReLU\\n5 Linear(in=512, output=512)\\n6 ReLU\\n7 Linear(in=512, output=512)\\nTable 7.Implementation of Non-linear Decoder.\\n# Layer\\n1 Linear(in= d, output=16)\\n2 BatchNorm\\n3 LeakyReLU(0.2)\\n4 Linear(in=16, output=64)\\n5 BatchNorm\\n6 LeakyReLU(0.2)\\n7 Linear(in=64, output=128)\\n8. BatchNorm\\n9 LeakyReLU(0.2)\\n10 Linear(in=128, output= d)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nNeural network architecture for RMNIST experiments:\\nTable 8.Implementation of MNIST ConvNet.\\n# Layer\\n1 Conv2D(in= d, output=64)\\n2 ReLU\\n3 GroupNorm(groupds=8)\\n4 Conv2D(in=64, output=128, stride=2)\\n5 ReLU\\n6 GroupNorm(groupds=8)\\n7 Conv2D(in=128, output=128)\\n8 ReLU\\n9 GroupNorm(groupds=8)\\n10 Conv2D(in=128, output=128)\\n11 ReLU\\n12 GroupNorm(groupds=8)\\n13 Global average-pooling\\nTable 9.Implementation of MNIST ConvTranNet.\\n# Layer\\n1 Linear(in= d, output=1024)\\n2 BatchNorm\\n3 ReLU\\n4 Upsample(8)\\n5 ConvTransposed2D(in=64, output=128, kernel=5)\\n6 BatchNorm\\n7 ReLU\\n8 Upsample(24)\\n9 ConvTransposed2D(in=128, output=256, kernel=5)\\n10 BatchNorm\\n11 ReLU\\n12 Conv2D(in=256, output=1, kernel=1)\\n13 Sigmoid'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nNeural network architecture for Portraits and CalTran experiments:\\nTable 10.Implementation of ConvTranNet.\\n# Layer\\n1 Linear(in= d, output=1024)\\n2 BatchNorm\\n3 ReLU\\n4 Upsample(16)\\n5 ConvTransposed2D(in=64, output=128, kernel=5)\\n6 BatchNorm\\n7 ReLU\\n8 Upsample(40)\\n9 ConvTransposed2D(in=128, output=256, kernel=5)\\n10 BatchNorm\\n11 ReLU\\n12 Upsample(80)\\n13 ConvTransposed2D(in=256, output=3, kernel=5)\\n14 BatchNorm\\n15 ReLU\\n16 Sigmoid\\nThe model architecture of the encoder is ResNet-18, we replace the ﬁnal softmax layer of the ofﬁcial version following Gul-\\nrajani & Lopez-Paz (2021). Besides, a dropout layer before the ﬁnal linear layer is inserted. This network is initialized by\\nrandom rather than loading the pretrained parameters on ImageNet.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nWe list the values of hyperparameters for different datasets below. All models are optimized by Adam (Kingma & Ba, 2015).\\nIn our experiments, we found that keeping the balance of the three KL divergence terms for zc, zw and zv via adjusting the\\nvalue of λ1, λ2 and λ3 is beneﬁcial for the ﬁnal results.\\nTable 11.Hyperparametes and their default values.\\nDataset Parameters Value\\nCircle\\nlearning rate for Ec,D,C 5e-5\\nlearning rate for Ew,Fw,Ev,Fv 5e-6\\nbatch size 24\\nλ1,λ2,λ3 1.0, 1.0, 1.0\\nα 0.05\\nCircle-C\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 1.0, 2.0, 1.0\\nα 0.05\\nSine\\nlearning rate for Ec,D,C 5e-5\\nlearning rate for Ew,Fw,Ev,Fv 5e-6\\nbatch size 24\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nSine-C\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nRMNIST\\nlearning rate for Ec,D,C 1e-3\\nlearning rate for Ew,Fw,Ev,Fv 1e-4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='α 0.05\\nSine-C\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nRMNIST\\nlearning rate for Ec,D,C 1e-3\\nlearning rate for Ew,Fw,Ev,Fv 1e-4\\nbatch size 48\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.05\\nPortraits\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 24\\nλ1,λ2,λ3 0.5, 1.0, 1.0\\nα 0.05\\nCalTran\\nlearning rate for Ec,D,C 5e-5\\nlearning rate for Ew,Fw,Ev,Fv 5e-6\\nbatch size 24\\nλ1,λ2,λ3 1.0, 1.0, 1.0\\nα 0.1\\nPowerSupply\\nlearning rate for Ec,D,C 1e-5\\nlearning rate for Ew,Fw,Ev,Fv 1e-6\\nbatch size 48\\nλ1,λ2,λ3 2.0, 1.0, 1.0\\nα 0.1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nD. Additional Experimental Results\\nIn this section, we provide more experimental results for our proposed evolving domain generalization task. As we can see,\\nin most of the cases, we can achieve the state-of-the-art performance compared with other domain generalization baselines.\\nHowever, in some cases, our proposed method cannot achieve desired results. For example, in Sine-C, ERM and some\\ndomain generalization baselines can achieve a desired classiﬁcation performance especially for domain index 17,18 and 19,\\nwhich we conjecture the reason that their decision boundaries overﬁt to the abrupt concept shift, which further leads to their\\npoor performance on the following domains after domain 19. However, our proposed method aims to learn the evolving\\npattern starting from t= 0, which may not be optimal to suit to this abrupt concept shift case. One possible future direction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='pattern starting from t= 0, which may not be optimal to suit to this abrupt concept shift case. One possible future direction\\nis only to learn the evolving pattern in a certain time duration (i.e., time duration in [t−T0,t], where tis the current time\\nstamp). Nevertheless, our proposed method shows its effectiveness in dynamic modeling for evolving domain generalization\\ntask. We will leave the discussion how to ﬁnd a suitable time duration (i.e., a suitable T0) in our future work.\\nTable 12.Circle. We show the results on each target domain by domain index.\\nAlgorithm 21 22 23 24 25 26 27 28 29 30 Avg\\nERM 53.9 ±3.5 55.8 ±4.8 53.9 ±5.2 44.7 ±6.3 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 49.9\\nMixup 48.6 ±3.8 51.7 ±4.0 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 48.4\\nMMD 50.0 ±3.9 53.6 ±4.4 55.0 ±4.3 51.9 ±6.0 60.8 ±3.9 49.7 ±7.2 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 50.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='MMD 50.0 ±3.9 53.6 ±4.4 55.0 ±4.3 51.9 ±6.0 60.8 ±3.9 49.7 ±7.2 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 50.7\\nMLDG 57.8 ±3.6 57.7 ±5.0 55.3 ±4.9 46.4 ±6.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 50.8\\nIRM 57.8 ±3.9 59.4 ±5.4 56.9 ±4.9 48.1 ±7.4 57.5 ±4.3 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 51.3\\nRSC 45.3 ±3.6 51.4 ±3.8 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 48.0\\nMTL 61.4 ±2.2 57.2 ±6.4 53.3 ±5.1 48.3 ±6.2 56.9 ±4.8 49.2 ±3.7 43.3 ±5.0 45.8 ±2.8 54.2 ±5.7 42.2 ±4.9 51.2\\nFish 51.7 ±3.7 53.1 ±3.7 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 48.8\\nCORAL 65.3 ±3.2 63.9 ±4.4 60.0 ±4.8 56.4 ±6.0 60.2 ±4.3 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 47.8 ±5.7 53.9\\nAndMask 42.8 ±3.4 50.6 ±4.1 49.4 ±4.5 43.6 ±5.8 56.9 ±4.4 47.8 ±5.8 41.9 ±7.5 41.7 ±5.9 54.2 ±3.0 49.7 ±5.4 47.9\\nDIV A 81.3 ±3.5 76.3 ±4.2 74.7 ±4.6 56.7 ±5.1 67.0 ±6.1 62.3 ±5.1 62.0 ±5.6 66.3 ±4.1 70.3 ±5.6 62.0 ±4.2 67.9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='DIV A 81.3 ±3.5 76.3 ±4.2 74.7 ±4.6 56.7 ±5.1 67.0 ±6.1 62.3 ±5.1 62.0 ±5.6 66.3 ±4.1 70.3 ±5.6 62.0 ±4.2 67.9\\nLSSAE (Ours) 95.8 ±1.9 95.6 ±2.1 93.5 ±2.9 96.3 ±1.8 83.8 ±5.2 74.3 ±3.6 51.9 ±5.6 52.3 ±8.1 46.5 ±9.2 48.4 ±5.3 73.8\\nTable 13.Sine. We show the results on each target domain denoted by domain index.\\nAlgorithm 17 18 19 20 21 22 23 24 Avg\\nERM 71.4 ±6.1 91.0 ±1.5 81.6 ±2.4 53.4 ±2.9 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 63.0\\nMixup 63.1 ±5.9 93.5 ±1.7 80.6 ±3.8 52.8 ±2.9 60.3 ±7.2 54.2 ±2.7 49.5 ±4.4 49.3 ±8.0 62.9\\nMMD 57.0 ±4.2 57.1 ±4.1 47.6 ±5.4 50.0 ±1.8 55.1 ±6.7 54.4 ±4.7 49.5 ±4.8 51.7 ±5.0 55.8\\nMLDG 69.2 ±4.2 67.7 ±4.1 52.1 ±5.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 63.2\\nIRM 66.9 ±6.2 81.1 ±3.2 88.5 ±3.0 56.6 ±6.0 57.2 ±5.8 53.7 ±5.1 49.5 ±2.2 51.7 ±5.4 63.2\\nRSC 61.3 ±6.6 83.5 ±1.9 84.5 ±2.6 52.8 ±2.8 55.1 ±6.7 54.4 ±4.7 49.5 ±4.8 51.7 ±5.0 61.5\\nMTL 70.6 ±6.6 91.6 ±1.2 79.9 ±3.4 51.0 ±4.7 60.3 ±7.6 53.6 ±5.2 49.5 ±5.3 46.9 ±5.9 62.9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='RSC 61.3 ±6.6 83.5 ±1.9 84.5 ±2.6 52.8 ±2.8 55.1 ±6.7 54.4 ±4.7 49.5 ±4.8 51.7 ±5.0 61.5\\nMTL 70.6 ±6.6 91.6 ±1.2 79.9 ±3.4 51.0 ±4.7 60.3 ±7.6 53.6 ±5.2 49.5 ±5.3 46.9 ±5.9 62.9\\nFish 66.1 ±6.9 82.0 ±2.7 87.5 ±2.4 55.2 ±3.0 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 62.3\\nCORAL 60.0 ±5.3 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 49.7 ±6.2 48.6 ±4.6 46.3 ±5.0 51.7 ±5.0 51.6\\nAndMask 44.2 ±5.1 42.9 ±4.2 54.2 ±7.0 71.9 ±1.9 86.4 ±3.2 90.4 ±2.9 88.1 ±3.4 76.4 ±3.7 69.3\\nDIV A 79.0 ±6.6 60.8 ±1.9 47.6 ±2.6 50.0 ±2.8 55.1 ±6.7 51.9 ±4.7 38.6 ±4.8 40.4 ±5.0 52.9\\nLSSAE (Ours) 93.0 ±1.7 86.9 ±0.7 69.2 ±1.5 63.8 ±3.8 68.8 ±2.5 76.8 ±4.8 63.9 ±1.3 49.0 ±3.1 71.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 14.RMNIST. We show the results on each target domain denoted by rotation angle.\\nAlgorithm 130◦ 140◦ 150◦ 160◦ 170◦ 180◦ Avg\\nERM 56.8 ±0.9 44.2 ±0.8 37.8 ±0.6 38.3 ±0.8 40.9 ±0.8 43.6 ±0.8 43.6\\nMixup 61.3 ±0.7 47.4 ±0.8 39.1 ±0.7 38.3 ±0.7 40.5 ±0.8 42.8 ±0.9 44.9\\nMMD 59.2 ±0.9 46.0 ±0.8 39.0 ±0.7 39.3 ±0.8 41.6 ±0.7 43.7 ±0.8 44.8\\nMLDG 57.4 ±0.7 44.5 ±0.9 37.5 ±0.8 37.5 ±0.8 39.9 ±0.8 42.0 ±0.9 43.1\\nIRM 47.7 ±0.9 38.5 ±0.7 34.1 ±0.7 35.7 ±0.8 37.8 ±0.8 40.3 ±0.8 39.0\\nRSC 54.1 ±0.9 41.9 ±0.8 35.8 ±0.7 37.0 ±0.8 39.8 ±0.8 41.6 ±0.8 41.7\\nMTL 54.8 ±0.9 43.1 ±0.8 36.4 ±0.8 36.1 ±0.8 39.1 ±0.9 40.9 ±0.8 41.7\\nFish 60.8 ±0.8 47.8 ±0.8 39.2 ±0.8 37.6 ±0.7 39.0 ±0.8 40.7 ±0.7 44.2\\nCORAL 58.8 ±0.9 46.2 ±0.8 38.9 ±0.7 38.5 ±0.8 41.3 ±0.8 43.5 ±0.8 44.5\\nAndMask 53.5 ±0.9 42.9 ±0.8 37.8 ±0.7 38.6 ±0.8 40.8 ±0.8 43.2 ±0.8 42.8\\nDIV A 58.3 ±0.8 45.0 ±0.8 37.6 ±0.8 36.9 ±0.7 38.1 ±0.8 40.1 ±0.8 42.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='AndMask 53.5 ±0.9 42.9 ±0.8 37.8 ±0.7 38.6 ±0.8 40.8 ±0.8 43.2 ±0.8 42.8\\nDIV A 58.3 ±0.8 45.0 ±0.8 37.6 ±0.8 36.9 ±0.7 38.1 ±0.8 40.1 ±0.8 42.7\\nLSSAE (Ours) 64.1 ±0.8 51.6 ±0.8 43.4 ±0.8 38.6 ±0.7 40.3 ±0.8 40.4 ±0.8 46.4\\nTable 15.Portraits. We show the results on each target domain denoted by domain index.\\nAlgorithm 25 26 27 28 29 30 31 32 33 34 Avg\\nERM 75.5 ±0.9 83.8 ±0.9 88.5 ±0.8 93.3 ±0.7 93.4 ±0.6 92.1 ±0.7 90.6 ±0.8 84.3 ±0.9 88.5 ±0.9 87.9 ±1.4 87.8\\nMixup 75.5 ±0.9 83.8 ±0.9 88.5 ±0.8 93.3 ±0.7 93.4 ±0.6 92.1 ±0.7 90.6 ±0.8 84.3 ±0.9 88.5 ±0.9 87.9 ±1.4 87.8\\nMMD 74.0 ±1.0 83.8 ±0.8 87.2 ±0.8 93.0 ±0.7 93.0 ±0.6 91.9 ±0.7 90.9 ±0.7 84.7 ±1.4 88.3 ±0.9 85.8 ±1.8 87.3\\nMLDG 76.4 ±0.8 85.5 ±0.9 90.1 ±0.7 94.3 ±0.6 93.5 ±0.6 92.0 ±0.7 90.8 ±0.8 85.6 ±1.1 89.3 ±0.8 87.6 ±1.6 88.5\\nIRM 74.2 ±0.9 83.5 ±0.9 88.5 ±0.8 91.0 ±0.8 90.4 ±0.7 87.3 ±0.8 87.0 ±0.9 80.4 ±1.5 86.7 ±0.9 85.1 ±1.8 85.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='IRM 74.2 ±0.9 83.5 ±0.9 88.5 ±0.8 91.0 ±0.8 90.4 ±0.7 87.3 ±0.8 87.0 ±0.9 80.4 ±1.5 86.7 ±0.9 85.1 ±1.8 85.4\\nRSC 75.2 ±0.9 84.7 ±0.8 87.9 ±0.7 93.3 ±0.7 92.5 ±0.7 91.0 ±0.7 90.0 ±0.7 84.6 ±1.2 88.2 ±0.8 85.8 ±1.9 87.3\\nMTL 78.2 ±0.9 86.5 ±0.8 90.9 ±0.8 94.2 ±0.7 93.8 ±0.6 92.0 ±0.7 91.2 ±0.7 86.0 ±1.2 89.3 ±0.8 87.4 ±1.4 89.0\\nFish 78.6 ±0.9 86.9 ±0.8 89.5 ±0.8 93.5 ±0.7 93.3 ±0.6 92.1 ±0.6 91.1 ±0.7 86.2 ±1.3 88.7 ±0.9 87.7 ±1.6 88.8\\nCORAL 74.6 ±0.9 84.6 ±0.8 87.9 ±0.8 93.3 ±0.6 92.7 ±0.7 91.5 ±0.7 90.7 ±0.7 84.6 ±1.5 88.1 ±0.9 85.9 ±1.9 87.4\\nAndMask 62.0 ±1.1 70.8 ±1.1 67.0 ±1.2 70.2 ±1.1 75.2 ±1.1 74.1 ±1.0 72.7 ±1.1 64.7 ±1.6 77.3 ±1.1 74.9 ±2.1 70.9\\nDIV A 76.2 ±1.0 86.6 ±0.8 88.8 ±0.8 93.5 ±0.7 93.1 ±0.6 91.6 ±0.6 91.1 ±0.7 84.7 ±1.3 89.1 ±0.8 87.0 ±1.5 88.2\\nLSSAE (Ours) 77.7 ±0.9 87.1 ±0.8 90.8 ±0.7 94.3 ±0.6 94.3 ±0.6 92.2 ±0.6 91.2 ±0.7 86.7 ±1.1 89.6 ±0.8 86.9 ±1.4 89.1\\nTable 16.CalTran. We show the results on each target domain denoted by domain index.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Table 16.CalTran. We show the results on each target domain denoted by domain index.\\nAlgorithm 25 26 27 28 29 30 31 32 33 34 Avg\\nERM 29.9 ±3.5 88.4 ±2.1 61.1 ±3.5 56.3 ±3.2 90.0 ±1.6 60.1 ±2.5 55.5 ±3.5 88.8 ±2.4 57.1 ±3.5 50.5 ±5.2 66.3\\nMixup 53.6 ±3.9 89.0 ±2.0 61.8 ±2.4 55.7 ±2.9 88.2 ±2.1 58.6 ±3.0 52.3 ±3.7 88.6 ±2.7 57.1 ±3.0 55.1 ±4.3 66.0\\nMMD 30.2 ±2.1 92.7 ±1.7 56.4 ±3.7 39.1 ±3.2 93.6 ±1.7 52.1 ±3.2 42.8 ±3.0 92.1 ±2.2 42.1 ±3.8 29.4 ±3.8 57.1\\nMLDG 54.8 ±4.1 88.6 ±2.6 62.2 ±3.6 55.1 ±4.1 88.3 ±1.7 60.9 ±4.3 51.7 ±2.6 89.0 ±1.9 56.5 ±3.4 55.3 ±4.8 66.2\\nIRM 46.4 ±3.7 90.8 ±1.7 60.8 ±3.4 52.9 ±3.1 91.8 ±1.7 56.6 ±3.1 52.1 ±2.9 90.9 ±2.6 55.6 ±3.9 43.1 ±5.5 64.1\\nRSC 57.2 ±3.0 88.4 ±2.6 62.6 ±3.0 56.5 ±3.7 88.0 ±2.4 59.4 ±3.0 51.9 ±2.9 90.0 ±2.0 59.4 ±2.9 56.0 ±3.1 67.0\\nMTL 64.2 ±3.0 87.2 ±2.5 64.9 ±3.9 60.0 ±4.8 84.5 ±2.2 60.6 ±3.5 52.6 ±3.7 83.9 ±2.9 58.2 ±4.1 65.7 ±5.6 68.2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='MTL 64.2 ±3.0 87.2 ±2.5 64.9 ±3.9 60.0 ±4.8 84.5 ±2.2 60.6 ±3.5 52.6 ±3.7 83.9 ±2.9 58.2 ±4.1 65.7 ±5.6 68.2\\nFish 61.1 ±3.5 88.2 ±1.5 64.7 ±4.0 57.9 ±3.1 88.3 ±2.2 59.9 ±3.0 57.5 ±2.7 87.4 ±2.8 57.7 ±3.7 63.0 ±6.1 68.6\\nCORAL 50.4 ±3.0 90.8 ±2.0 61.2 ±3.8 55.0 ±2.5 92.0 ±1.7 56.8 ±3.8 52.0 ±3.8 90.9 ±1.6 56.8 ±2.4 50.9 ±5.6 65.7\\nAndMask 30.0 ±2.2 92.7 ±1.7 56.2 ±3.8 39.1 ±3.2 93.6 ±1.7 51.6 ±3.2 42.6 ±2.9 92.1 ±2.2 41.2 ±3.7 29.9 ±3.6 56.9\\nDIV A 60.6 ±2.9 90.1 ±1.7 67.5 ±3.1 58.9 ±3.5 88.4 ±2.8 58.7 ±3.3 53.8 ±3.6 89.8 ±1.7 61.8 ±4.8 62.0 ±3.4 69.2\\nLSSV AE 63.4 ±3.4 92.1 ±2.0 62.6 ±4.7 58.8 ±4.4 92.9 ±1.6 62.0 ±3.9 54.3 ±3.0 92.1 ±2.2 60.5 ±3.8 67.4 ±3.6 70.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder\\nTable 17.Circle-C. We show the results on each target domain denoted by domain index.\\nAlgorithm 21 22 23 24 25 26 27 28 29 30 Avg\\nERM 40.6 ±3.8 43.1 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.9\\nMixup 40.6 ±3.8 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.8\\nMMD 38.6 ±3.7 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.6\\nMLDG 44.4 ±4.4 43.9 ±5.0 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 35.3\\nIRM 63.6 ±4.2 56.7 ±7.0 48.1 ±4.3 37.2 ±4.8 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 39.7\\nRSC 38.6 ±3.7 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.6\\nMTL 41.7 ±5.0 45.6 ±7.1 36.9 ±6.3 36.4 ±6.2 44.7 ±6.0 31.4 ±3.2 27.8 ±4.2 28.3 ±2.8 31.9 ±4.6 26.1 ±4.1 35.1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='MTL 41.7 ±5.0 45.6 ±7.1 36.9 ±6.3 36.4 ±6.2 44.7 ±6.0 31.4 ±3.2 27.8 ±4.2 28.3 ±2.8 31.9 ±4.6 26.1 ±4.1 35.1\\nFish 42.5 ±3.8 43.3 ±4.8 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 35.1\\nCORAL 40.8 ±3.7 43.3 ±4.8 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.9\\nAndMask 53.6 ±4.2 54.7 ±5.4 51.7 ±4.7 33.6 ±5.1 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 35.0 ±6.7 38.9\\nDIV A 40.6 ±3.8 42.5 ±4.7 39.7 ±4.0 33.6 ±5.0 44.7 ±6.0 31.4 ±6.4 27.8 ±7.7 26.9 ±6.0 27.8 ±3.5 33.1 ±7.1 34.8\\nLSSAE (Ours) 74.5 ±1.8 65.5 ±3.9 55.5 ±3.2 36.0 ±3.5 45.0 ±0.7 40.5 ±4.6 35.0 ±1.4 33.0 ±1.4 34.0 ±6.4 29.0 ±2.8 44.8\\nTable 18.Sine-C. We show the results on each target domain denoted by domain index.\\nAlgorithm 17 18 19 20 21 22 23 24 Avg\\nERM 64.2 ±6.8 84.9 ±3.2 83.7 ±3.1 54.9 ±2.1 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.8\\nMixup 60.3 ±7.8 77.4 ±3.3 87.8 ±1.8 57.3 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='ERM 64.2 ±6.8 84.9 ±3.2 83.7 ±3.1 54.9 ±2.1 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.8\\nMixup 60.3 ±7.8 77.4 ±3.3 87.8 ±1.8 57.3 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.2\\nMMD 55.8 ±5.1 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 52.4\\nMLDG 65.6 ±6.4 88.7 ±2.3 84.4 ±2.8 52.4 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 62.2\\nIRM 61.4 ±7.0 82.8 ±3.2 86.5 ±3.0 54.9 ±1.8 55.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.5\\nRSC 65.0 ±6.7 83.5 ±3.1 85.4 ±3.1 53.8 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 61.8\\nMTL 61.9 ±7.3 82.0 ±2.3 83.7 ±4.2 54.2 ±5.4 60.3 ±7.6 53.6 ±5.2 49.5 ±5.3 46.9 ±5.9 61.5\\nFish 69.4 ±6.2 94.5 ±1.5 79.5 ±3.1 52.4 ±2.6 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 62.8\\nCORAL 70.3 ±5.0 77.2 ±3.6 67.0 ±4.2 52.1 ±2.5 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 59.2\\nAndMask 55.8 ±5.1 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 52.3\\nDIV A 76.9 ±6.1 61.1 ±5.9 47.6 ±3.4 48.6 ±4.2 51.1 ±7.0 52.9 ±6.1 38.5 ±5.0 36.5 ±6.8 51.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='AndMask 55.8 ±5.1 57.1 ±4.2 48.6 ±6.4 50.7 ±1.8 51.1 ±6.7 54.3 ±4.7 49.5 ±4.8 51.7 ±5.0 52.3\\nDIV A 76.9 ±6.1 61.1 ±5.9 47.6 ±3.4 48.6 ±4.2 51.1 ±7.0 52.9 ±6.1 38.5 ±5.0 36.5 ±6.8 51.7\\nLSSAE (Ours) 62.3 ±3.1 63.2 ±6.7 57.6 ±2.6 66.4 ±2.6 63.5 ±3.9 59.5 ±4.0 52.6 ±2.2 61.3 ±1.9 60.8\\nTable 19.PowerSupply. We show the results on each target domain denoted by domain index.\\nAlgorithm 21 22 23 24 25 26 27 28 29 30 Avg\\nERM 69.8 ±1.4 70.0 ±1.4 69.2 ±1.3 64.4 ±1.5 85.8 ±1.0 76.0 ±1.3 70.1 ±1.5 69.8 ±1.5 69.0 ±1.3 65.5 ±1.5 71.0\\nMixup 69.6 ±1.4 69.5 ±1.5 68.3 ±1.5 64.3 ±1.5 87.1 ±1.0 76.6 ±1.3 70.1 ±1.4 69.2 ±1.3 68.1 ±1.5 65.0 ±1.6 70.8\\nMMD 70.0 ±1.3 69.7 ±1.4 68.7 ±1.4 64.8 ±1.5 85.6 ±1.0 76.1 ±1.3 70.0 ±1.5 69.5 ±1.4 68.7 ±1.3 65.6 ±1.5 70.9\\nMLDG 69.7 ±1.4 69.7 ±1.5 68.6 ±1.5 64.6 ±1.5 86.4 ±1.1 76.3 ±1.4 70.1 ±1.4 69.4 ±1.3 68.4 ±1.5 65.6 ±1.5 70.8\\nIRM 69.8 ±1.4 69.5 ±1.4 68.3 ±1.4 64.1 ±1.4 87.2 ±0.9 76.5 ±1.3 70.0 ±1.5 69.1 ±1.5 68.2 ±1.3 65.0 ±1.4 70.8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-06-17T01:34:02+00:00', 'author': 'Anonymous Submission', 'keywords': '', 'moddate': '2022-06-17T01:34:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': 'Proceedings of the International Conference on Machine Learning 2022', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\2205.07649v2.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': '2205.07649v2.pdf', 'file_type': 'pdf'}, page_content='IRM 69.8 ±1.4 69.5 ±1.4 68.3 ±1.4 64.1 ±1.4 87.2 ±0.9 76.5 ±1.3 70.0 ±1.5 69.1 ±1.5 68.2 ±1.3 65.0 ±1.4 70.8\\nRSC 69.9 ±1.4 69.6 ±1.4 68.6 ±1.4 64.4 ±1.5 86.6 ±1.0 76.3 ±1.3 70.0 ±1.5 69.4 ±1.4 68.4 ±1.3 65.4 ±1.5 70.9\\nMTL 69.6 ±1.4 69.4 ±1.5 68.2 ±1.6 64.2 ±1.5 87.4 ±1.2 76.6 ±1.3 69.9 ±1.5 69.1 ±1.5 68.2 ±1.5 64.6 ±1.4 70.7\\nFish 69.7 ±1.4 69.4 ±1.4 68.2 ±1.4 64.2 ±1.4 87.3 ±1.0 76.6 ±1.3 69.9 ±1.5 69.2 ±1.5 68.2 ±1.3 65.2 ±1.5 70.8\\nCORAL 69.9 ±1.4 69.7 ±1.4 68.9 ±1.4 64.6 ±1.4 86.1 ±1.0 76.3 ±1.3 70.0 ±1.5 69.5 ±1.5 68.8 ±1.3 65.7 ±1.5 71.0\\nAndMask 69.9 ±1.4 69.4 ±1.4 68.2 ±1.3 64.0 ±1.4 87.4 ±0.9 76.7 ±1.3 70.0 ±1.5 69.1 ±1.5 68.0 ±1.3 64.7 ±1.5 70.7\\nDIV A 69.7 ±1.4 69.5 ±1.3 68.2 ±1.4 63.9 ±1.5 87.5 ±1.0 76.5 ±1.3 69.9 ±1.5 69.1 ±1.5 68.1 ±1.3 64.7 ±1.5 70.7\\nLSSAE (Ours) 70.0 ±1.4 69.8 ±1.4 69.0 ±1.5 65.4 ±1.4 85.1 ±1.1 76.0 ±1.4 70.1 ±1.7 69.9 ±1.3 69.0 ±1.6 66.3 ±1.4 71.1')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "becce322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 383 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12/12 [00:04<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (383, 384)\n",
      "Adding 383 documents to vector store...\n",
      "Successfully added 383 documents toi vector store\n",
      "Total documents in collection: <bound method Collection.count of Collection(name=pdf_documents)>\n"
     ]
    }
   ],
   "source": [
    "# Convert text to mbeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in vector database\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e5727",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6dc910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1fdd3af5eb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a class for RAG retrieval\n",
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbbeddingManager, top_k: int = 5):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.top_k = top_k\n",
    "\n",
    "    # Add a score threshold parameter if needed in retrieve function\n",
    "    def retrieve(self, query: str, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        print(f\"Generating embedding for query: {query}\")\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        print(f\"Searching for top {self.top_k} similar documents...\")\n",
    "\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=self.top_k\n",
    "            )\n",
    "\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    distance = float(distance)\n",
    "                    similarity_score = 1 / (1 + distance)  # Map distance to (0,1] to avoid negatives and keep monotonicity\n",
    "                    if similarity_score < score_threshold:\n",
    "                        print(f\"Skipping document {doc_id} due to low similarity score: {similarity_score}\")\n",
    "                        continue \n",
    "                    else:\n",
    "                        retrieved_docs.append({'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            \n",
    "            else:\n",
    "                print(\"No documents found.\")\n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store=vectorstore, embedding_manager=embedding_manager, top_k=5)\n",
    "rag_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e903c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: What is CNN training procedure described under Unsupervised Domain Adaptation?\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Searching for top 5 similar documents...\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_d360e825_59',\n",
       "  'content': 'its in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning 2011, 2011.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\\nand transferring mid-level image representations using\\nconvolutional neural networks. In CVPR, 2014.\\nPan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\\nYang, Qiang. Domain adaptation via transfer component\\nanalysis. IEEE Transactions on Neural Networks, 22(2):\\n199–210, 2011.\\nS. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\\nlearning for domain adaptation by interpolating between\\ndomains. In ICML Workshop on Challenges in Repre-\\nsentation Learning, 2013.\\nSaenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\\nTrevor. Adapting visual category models to new do-\\nmains. In ECCV, pp. 213–226. 2010.\\nShimodaira, Hidetoshi. Improving predictive inference un-\\nder covariate shift by weighting the log-likelihood func-\\ntion. Journal of Statistical Planning and Inference , 90',\n",
       "  'metadata': {'page_label': '11',\n",
       "   'content_length': 975,\n",
       "   'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning',\n",
       "   'page': 10,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1',\n",
       "   'author': 'Yaroslav Ganin, Victor Lempitsky',\n",
       "   'total_pages': 11,\n",
       "   'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf',\n",
       "   'source_file': '1409.7495v2.pdf',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2015-03-02T01:33:21+00:00',\n",
       "   'producer': 'pdfTeX-1.40.12',\n",
       "   'moddate': '2015-03-02T01:33:21+00:00',\n",
       "   'doc_index': 59,\n",
       "   'trapped': '/False',\n",
       "   'subject': '',\n",
       "   'title': 'Unsupervised Domain Adaptation by Backpropagation'},\n",
       "  'similarity_score': 0.397655189037323,\n",
       "  'distance': 0.602344810962677,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_5bc5554a_59',\n",
       "  'content': 'its in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning 2011, 2011.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\\nand transferring mid-level image representations using\\nconvolutional neural networks. In CVPR, 2014.\\nPan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\\nYang, Qiang. Domain adaptation via transfer component\\nanalysis. IEEE Transactions on Neural Networks, 22(2):\\n199–210, 2011.\\nS. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\\nlearning for domain adaptation by interpolating between\\ndomains. In ICML Workshop on Challenges in Repre-\\nsentation Learning, 2013.\\nSaenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\\nTrevor. Adapting visual category models to new do-\\nmains. In ECCV, pp. 213–226. 2010.\\nShimodaira, Hidetoshi. Improving predictive inference un-\\nder covariate shift by weighting the log-likelihood func-\\ntion. Journal of Statistical Planning and Inference , 90',\n",
       "  'metadata': {'moddate': '2015-03-02T01:33:21+00:00',\n",
       "   'subject': '',\n",
       "   'author': 'Yaroslav Ganin, Victor Lempitsky',\n",
       "   'creationdate': '2015-03-02T01:33:21+00:00',\n",
       "   'page': 10,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1',\n",
       "   'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning',\n",
       "   'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf',\n",
       "   'doc_index': 59,\n",
       "   'title': 'Unsupervised Domain Adaptation by Backpropagation',\n",
       "   'trapped': '/False',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'content_length': 975,\n",
       "   'producer': 'pdfTeX-1.40.12',\n",
       "   'page_label': '11',\n",
       "   'source_file': '1409.7495v2.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 11},\n",
       "  'similarity_score': 0.397655189037323,\n",
       "  'distance': 0.602344810962677,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_4fd614e6_57',\n",
       "  'content': 'Unsupervised Domain Adaptation by Backpropagation\\nGong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\\nnecting the dots with landmarks: Discriminatively learn-\\ning domain-invariant features for unsupervised domain\\nadaptation. In ICML, pp. 222–230, 2013.\\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\\nBing, Warde-Farley, David, Ozair, Sherjil, Courville,\\nAaron, and Bengio, Yoshua. Generative adversarial nets.\\nIn NIPS, 2014.\\nGopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\\nDomain adaptation for object recognition: An unsuper-\\nvised approach. In ICCV, pp. 999–1006, 2011.\\nHoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\\nSaenko, Kate, and Darrell, Trevor. One-shot adapta-\\ntion of supervised deep convolutional models. CoRR,\\nabs/1312.6204, 2013.\\nHuang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\\nBorgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\\nrecting sample selection bias by unlabeled data. InNIPS,\\npp. 601–608, 2006.',\n",
       "  'metadata': {'trapped': '/False',\n",
       "   'subject': '',\n",
       "   'content_length': 953,\n",
       "   'title': 'Unsupervised Domain Adaptation by Backpropagation',\n",
       "   'doc_index': 57,\n",
       "   'moddate': '2015-03-02T01:33:21+00:00',\n",
       "   'producer': 'pdfTeX-1.40.12',\n",
       "   'creationdate': '2015-03-02T01:33:21+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf',\n",
       "   'total_pages': 11,\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': '1409.7495v2.pdf',\n",
       "   'page_label': '11',\n",
       "   'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning',\n",
       "   'author': 'Yaroslav Ganin, Victor Lempitsky',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'page': 10},\n",
       "  'similarity_score': 0.3799741864204407,\n",
       "  'distance': 0.6200258135795593,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_79b55266_57',\n",
       "  'content': 'Unsupervised Domain Adaptation by Backpropagation\\nGong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\\nnecting the dots with landmarks: Discriminatively learn-\\ning domain-invariant features for unsupervised domain\\nadaptation. In ICML, pp. 222–230, 2013.\\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\\nBing, Warde-Farley, David, Ozair, Sherjil, Courville,\\nAaron, and Bengio, Yoshua. Generative adversarial nets.\\nIn NIPS, 2014.\\nGopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\\nDomain adaptation for object recognition: An unsuper-\\nvised approach. In ICCV, pp. 999–1006, 2011.\\nHoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\\nSaenko, Kate, and Darrell, Trevor. One-shot adapta-\\ntion of supervised deep convolutional models. CoRR,\\nabs/1312.6204, 2013.\\nHuang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\\nBorgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\\nrecting sample selection bias by unlabeled data. InNIPS,\\npp. 601–608, 2006.',\n",
       "  'metadata': {'page': 10,\n",
       "   'producer': 'pdfTeX-1.40.12',\n",
       "   'subject': '',\n",
       "   'author': 'Yaroslav Ganin, Victor Lempitsky',\n",
       "   'source_file': '1409.7495v2.pdf',\n",
       "   'doc_index': 57,\n",
       "   'page_label': '11',\n",
       "   'title': 'Unsupervised Domain Adaptation by Backpropagation',\n",
       "   'creationdate': '2015-03-02T01:33:21+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf',\n",
       "   'total_pages': 11,\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'content_length': 953,\n",
       "   'moddate': '2015-03-02T01:33:21+00:00',\n",
       "   'trapped': '/False',\n",
       "   'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning'},\n",
       "  'similarity_score': 0.3799741864204407,\n",
       "  'distance': 0.6200258135795593,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_bca9231e_42',\n",
       "  'content': 'Unsupervised Domain Adaptation by Backpropagation\\n1 2 3 4 5\\n·104\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nBatches seen\\nValidation error\\nReal data only\\nSynthetic data only\\nBoth\\nFigure 4.Semi-supervised domain adaptation for the trafﬁc signs.\\nAs labeled target domain data are shown to the method, it achieves\\nsigniﬁcantly lower error than the model trained on target domain\\ndata only or on source domain data only.\\npected to be more generic and to perform reasonably on\\nthe MNIST dataset. This, indeed, turns out to be the case\\nand is supported by the appearance of the feature distribu-\\ntions. We observe a quite strong separation between the\\ndomains when we feed them into the CNN trained solely\\non MNIST, whereas for the SVHN-trained network the\\nfeatures are much more intermixed. This difference prob-\\nably explains why our method succeeded in improving the\\nperformance by adaptation in the SVHN →MNIST sce-\\nnario (see Table 1) but not in the opposite direction (SA is',\n",
       "  'metadata': {'producer': 'pdfTeX-1.40.12',\n",
       "   'moddate': '2015-03-02T01:33:21+00:00',\n",
       "   'author': 'Yaroslav Ganin, Victor Lempitsky',\n",
       "   'content_length': 950,\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 11,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1',\n",
       "   'source_file': '1409.7495v2.pdf',\n",
       "   'page_label': '8',\n",
       "   'subject': '',\n",
       "   'doc_index': 42,\n",
       "   'page': 7,\n",
       "   'keywords': 'Gradient Reversal, Unsupervised Domain Adaptation, Deep Learning',\n",
       "   'source': '..\\\\data\\\\pdf\\\\1409.7495v2.pdf',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'trapped': '/False',\n",
       "   'title': 'Unsupervised Domain Adaptation by Backpropagation',\n",
       "   'creationdate': '2015-03-02T01:33:21+00:00'},\n",
       "  'similarity_score': 0.37141597270965576,\n",
       "  'distance': 0.6285840272903442,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is CNN training procedure described under Unsupervised Domain Adaptation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c53c4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG Pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq LLM\n",
    "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"Set GROQ_API_KEY in your .env file\")\n",
    "\n",
    "llm = ChatGroq(api_key=groq_api_key, model=\"qwen/qwen3-32b\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "# Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query: str, retriever: RAGRetriever, llm: ChatGroq) -> str:\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.retrieve(query)\n",
    "\n",
    "    # Combine retrieved documents into context\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs]) if retrieved_docs else \"\"\n",
    "\n",
    "    if not context:\n",
    "        return \"I'm sorry, I couldn't find any relevant information to answer your question.\"\n",
    "\n",
    "    # Create prompt for LLM\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    print(f\"Generated Prompt:\\n{prompt}\\n\")\n",
    "\n",
    "    # Generate response from LLM\n",
    "    response = llm.invoke([prompt.format(context=context, question=query)])\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52f16adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: What is CNN training procedure described under Unsupervised Domain Adaptation?\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Searching for top 5 similar documents...\n",
      "Retrieved 5 documents (after filtering)\n",
      "Generated Prompt:\n",
      "Context:\n",
      "its in natural images with unsupervised feature learning.\n",
      "In NIPS Workshop on Deep Learning and Unsupervised\n",
      "Feature Learning 2011, 2011.\n",
      "Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\n",
      "and transferring mid-level image representations using\n",
      "convolutional neural networks. In CVPR, 2014.\n",
      "Pan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\n",
      "Yang, Qiang. Domain adaptation via transfer component\n",
      "analysis. IEEE Transactions on Neural Networks, 22(2):\n",
      "199–210, 2011.\n",
      "S. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\n",
      "learning for domain adaptation by interpolating between\n",
      "domains. In ICML Workshop on Challenges in Repre-\n",
      "sentation Learning, 2013.\n",
      "Saenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\n",
      "Trevor. Adapting visual category models to new do-\n",
      "mains. In ECCV, pp. 213–226. 2010.\n",
      "Shimodaira, Hidetoshi. Improving predictive inference un-\n",
      "der covariate shift by weighting the log-likelihood func-\n",
      "tion. Journal of Statistical Planning and Inference , 90\n",
      "\n",
      "its in natural images with unsupervised feature learning.\n",
      "In NIPS Workshop on Deep Learning and Unsupervised\n",
      "Feature Learning 2011, 2011.\n",
      "Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\n",
      "and transferring mid-level image representations using\n",
      "convolutional neural networks. In CVPR, 2014.\n",
      "Pan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\n",
      "Yang, Qiang. Domain adaptation via transfer component\n",
      "analysis. IEEE Transactions on Neural Networks, 22(2):\n",
      "199–210, 2011.\n",
      "S. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\n",
      "learning for domain adaptation by interpolating between\n",
      "domains. In ICML Workshop on Challenges in Repre-\n",
      "sentation Learning, 2013.\n",
      "Saenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\n",
      "Trevor. Adapting visual category models to new do-\n",
      "mains. In ECCV, pp. 213–226. 2010.\n",
      "Shimodaira, Hidetoshi. Improving predictive inference un-\n",
      "der covariate shift by weighting the log-likelihood func-\n",
      "tion. Journal of Statistical Planning and Inference , 90\n",
      "\n",
      "Unsupervised Domain Adaptation by Backpropagation\n",
      "Gong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\n",
      "necting the dots with landmarks: Discriminatively learn-\n",
      "ing domain-invariant features for unsupervised domain\n",
      "adaptation. In ICML, pp. 222–230, 2013.\n",
      "Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\n",
      "Bing, Warde-Farley, David, Ozair, Sherjil, Courville,\n",
      "Aaron, and Bengio, Yoshua. Generative adversarial nets.\n",
      "In NIPS, 2014.\n",
      "Gopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\n",
      "Domain adaptation for object recognition: An unsuper-\n",
      "vised approach. In ICCV, pp. 999–1006, 2011.\n",
      "Hoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\n",
      "Saenko, Kate, and Darrell, Trevor. One-shot adapta-\n",
      "tion of supervised deep convolutional models. CoRR,\n",
      "abs/1312.6204, 2013.\n",
      "Huang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\n",
      "Borgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\n",
      "recting sample selection bias by unlabeled data. InNIPS,\n",
      "pp. 601–608, 2006.\n",
      "\n",
      "Unsupervised Domain Adaptation by Backpropagation\n",
      "Gong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\n",
      "necting the dots with landmarks: Discriminatively learn-\n",
      "ing domain-invariant features for unsupervised domain\n",
      "adaptation. In ICML, pp. 222–230, 2013.\n",
      "Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\n",
      "Bing, Warde-Farley, David, Ozair, Sherjil, Courville,\n",
      "Aaron, and Bengio, Yoshua. Generative adversarial nets.\n",
      "In NIPS, 2014.\n",
      "Gopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\n",
      "Domain adaptation for object recognition: An unsuper-\n",
      "vised approach. In ICCV, pp. 999–1006, 2011.\n",
      "Hoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\n",
      "Saenko, Kate, and Darrell, Trevor. One-shot adapta-\n",
      "tion of supervised deep convolutional models. CoRR,\n",
      "abs/1312.6204, 2013.\n",
      "Huang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\n",
      "Borgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\n",
      "recting sample selection bias by unlabeled data. InNIPS,\n",
      "pp. 601–608, 2006.\n",
      "\n",
      "Unsupervised Domain Adaptation by Backpropagation\n",
      "1 2 3 4 5\n",
      "·104\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Batches seen\n",
      "Validation error\n",
      "Real data only\n",
      "Synthetic data only\n",
      "Both\n",
      "Figure 4.Semi-supervised domain adaptation for the trafﬁc signs.\n",
      "As labeled target domain data are shown to the method, it achieves\n",
      "signiﬁcantly lower error than the model trained on target domain\n",
      "data only or on source domain data only.\n",
      "pected to be more generic and to perform reasonably on\n",
      "the MNIST dataset. This, indeed, turns out to be the case\n",
      "and is supported by the appearance of the feature distribu-\n",
      "tions. We observe a quite strong separation between the\n",
      "domains when we feed them into the CNN trained solely\n",
      "on MNIST, whereas for the SVHN-trained network the\n",
      "features are much more intermixed. This difference prob-\n",
      "ably explains why our method succeeded in improving the\n",
      "performance by adaptation in the SVHN →MNIST sce-\n",
      "nario (see Table 1) but not in the opposite direction (SA is\n",
      "\n",
      "Question: What is CNN training procedure described under Unsupervised Domain Adaptation?\n",
      "Answer:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Answer:\n",
      "<think>\n",
      "Okay, let's tackle this question. The user is asking about the CNN training procedure described under Unsupervised Domain Adaptation in the provided context. First, I need to parse through the given information to find relevant details.\n",
      "\n",
      "Looking at the context, there are several references to domain adaptation techniques. The key paper mentioned is \"Unsupervised Domain Adaptation by Backpropagation\" by Gong et al. (2013). The figure (Figure 4) shows a semi-supervised approach where labeled target data is used, leading to lower error than using only source or target data. \n",
      "\n",
      "The answer should focus on the training procedure outlined in Gong et al.'s work. From the figure's caption, it mentions using both source and target data, with labeled target data being incorporated. The method is called \"Connecting the dots with landmarks,\" which involves discriminatively learning domain-invariant features. \n",
      "\n",
      "The procedure likely involves training a CNN to minimize a loss that combines the source domain's task-specific loss (like classification) and a domain adaptation loss that encourages features to be invariant between domains. Since it's unsupervised, the target domain doesn't have labels, so the adaptation loss might use techniques like maximum mean discrepancy (MMD) or adversarial training to align the feature distributions.\n",
      "\n",
      "Additionally, the text mentions that in the SVHN→MNIST scenario, the method improved performance, but not the reverse. This suggests that the source domain is SVHN (synthetic digits) and the target is MNIST (real digits), which is a common setup. The training would involve using the source domain's labeled data for the task (e.g., digit classification) and the target domain's unlabeled data to adapt the features to be domain-invariant.\n",
      "\n",
      "The figure also refers to semi-supervised learning, meaning some labeled target data is available. However, the main focus is on unsupervised adaptation, so the procedure might combine supervised loss on source data and unsupervised domain adaptation loss on both domains. The network is trained end-to-end via backpropagation, adjusting weights to perform well on the source task while making features domain-invariant.\n",
      "\n",
      "I should also consider other cited works, like GANs (Goodfellow et al. 2014) and adversarial training, but the specific procedure here is from Gong et al. Their method uses a discriminative approach to learn features that are invariant across domains. The key components are the task-specific loss (e.g., cross-entropy for classification on source data) and a domain adaptation loss that minimizes the discrepancy between source and target feature distributions.\n",
      "\n",
      "In summary, the training procedure involves optimizing a combined loss function that includes both the source task loss and a domain adaptation loss, using backpropagation to update the network parameters. The domain adaptation loss ensures that the learned features are similar across domains, even when the target domain has no labels. The semi-supervised aspect comes into play when some labeled target data is available, which can further improve performance by incorporating those labels into the loss function.\n",
      "</think>\n",
      "\n",
      "The CNN training procedure described under **Unsupervised Domain Adaptation** in the context of the paper *\"Unsupervised Domain Adaptation by Backpropagation\"* (Gong et al., 2013) involves the following key steps and principles:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Objective**\n",
      "The goal is to learn **domain-invariant features** that generalize from a labeled source domain to an unlabeled target domain. This is achieved by minimizing a combined loss function that balances:\n",
      "- **Task-specific loss** (e.g., classification loss on the source domain).\n",
      "- **Domain adaptation loss** (to align feature distributions between source and target domains).\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Key Components of the Training Procedure**\n",
      "#### **a. Semi-Supervised Learning**\n",
      "- **Labeled source data** (e.g., synthetic images like SVHN) is used to train the model for the primary task (e.g., classification).\n",
      "- **Unlabeled target data** (e.g., real-world images like MNIST) is used to enforce domain invariance via unsupervised domain adaptation.\n",
      "- Optionally, **a small amount of labeled target data** can be incorporated to improve performance (semi-supervised setting).\n",
      "\n",
      "#### **b. Domain-Invariant Feature Learning**\n",
      "- The CNN is trained to produce features that are **discriminative for the source task** (e.g., classifying digits in SVHN) while being **invariant to domain shifts** (e.g., synthetic-to-real image differences).\n",
      "- This is achieved by minimizing a **domain adaptation loss** (e.g., Maximum Mean Discrepancy, MMD) between the source and target feature distributions.\n",
      "\n",
      "#### **c. Backpropagation Optimization**\n",
      "- The network is trained end-to-end using **backpropagation** to optimize the combined loss:\n",
      "  $$\n",
      "  \\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{task}}(\\text{source}) + \\lambda \\cdot \\math\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"What is CNN training procedure described under Unsupervised Domain Adaptation?\", rag_retriever, llm)\n",
    "print(f\"RAG Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bacb2ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: What is CNN training procedure described under Unsupervised Domain Adaptation?\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 121.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Searching for top 5 similar documents...\n",
      "Retrieved 5 documents (after filtering)\n",
      "Generated Prompt:\n",
      "Context:\n",
      "its in natural images with unsupervised feature learning.\n",
      "In NIPS Workshop on Deep Learning and Unsupervised\n",
      "Feature Learning 2011, 2011.\n",
      "Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\n",
      "and transferring mid-level image representations using\n",
      "convolutional neural networks. In CVPR, 2014.\n",
      "Pan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\n",
      "Yang, Qiang. Domain adaptation via transfer component\n",
      "analysis. IEEE Transactions on Neural Networks, 22(2):\n",
      "199–210, 2011.\n",
      "S. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\n",
      "learning for domain adaptation by interpolating between\n",
      "domains. In ICML Workshop on Challenges in Repre-\n",
      "sentation Learning, 2013.\n",
      "Saenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\n",
      "Trevor. Adapting visual category models to new do-\n",
      "mains. In ECCV, pp. 213–226. 2010.\n",
      "Shimodaira, Hidetoshi. Improving predictive inference un-\n",
      "der covariate shift by weighting the log-likelihood func-\n",
      "tion. Journal of Statistical Planning and Inference , 90\n",
      "\n",
      "its in natural images with unsupervised feature learning.\n",
      "In NIPS Workshop on Deep Learning and Unsupervised\n",
      "Feature Learning 2011, 2011.\n",
      "Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\n",
      "and transferring mid-level image representations using\n",
      "convolutional neural networks. In CVPR, 2014.\n",
      "Pan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\n",
      "Yang, Qiang. Domain adaptation via transfer component\n",
      "analysis. IEEE Transactions on Neural Networks, 22(2):\n",
      "199–210, 2011.\n",
      "S. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\n",
      "learning for domain adaptation by interpolating between\n",
      "domains. In ICML Workshop on Challenges in Repre-\n",
      "sentation Learning, 2013.\n",
      "Saenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\n",
      "Trevor. Adapting visual category models to new do-\n",
      "mains. In ECCV, pp. 213–226. 2010.\n",
      "Shimodaira, Hidetoshi. Improving predictive inference un-\n",
      "der covariate shift by weighting the log-likelihood func-\n",
      "tion. Journal of Statistical Planning and Inference , 90\n",
      "\n",
      "Unsupervised Domain Adaptation by Backpropagation\n",
      "Gong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\n",
      "necting the dots with landmarks: Discriminatively learn-\n",
      "ing domain-invariant features for unsupervised domain\n",
      "adaptation. In ICML, pp. 222–230, 2013.\n",
      "Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\n",
      "Bing, Warde-Farley, David, Ozair, Sherjil, Courville,\n",
      "Aaron, and Bengio, Yoshua. Generative adversarial nets.\n",
      "In NIPS, 2014.\n",
      "Gopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\n",
      "Domain adaptation for object recognition: An unsuper-\n",
      "vised approach. In ICCV, pp. 999–1006, 2011.\n",
      "Hoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\n",
      "Saenko, Kate, and Darrell, Trevor. One-shot adapta-\n",
      "tion of supervised deep convolutional models. CoRR,\n",
      "abs/1312.6204, 2013.\n",
      "Huang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\n",
      "Borgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\n",
      "recting sample selection bias by unlabeled data. InNIPS,\n",
      "pp. 601–608, 2006.\n",
      "\n",
      "Unsupervised Domain Adaptation by Backpropagation\n",
      "Gong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\n",
      "necting the dots with landmarks: Discriminatively learn-\n",
      "ing domain-invariant features for unsupervised domain\n",
      "adaptation. In ICML, pp. 222–230, 2013.\n",
      "Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\n",
      "Bing, Warde-Farley, David, Ozair, Sherjil, Courville,\n",
      "Aaron, and Bengio, Yoshua. Generative adversarial nets.\n",
      "In NIPS, 2014.\n",
      "Gopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\n",
      "Domain adaptation for object recognition: An unsuper-\n",
      "vised approach. In ICCV, pp. 999–1006, 2011.\n",
      "Hoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\n",
      "Saenko, Kate, and Darrell, Trevor. One-shot adapta-\n",
      "tion of supervised deep convolutional models. CoRR,\n",
      "abs/1312.6204, 2013.\n",
      "Huang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\n",
      "Borgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\n",
      "recting sample selection bias by unlabeled data. InNIPS,\n",
      "pp. 601–608, 2006.\n",
      "\n",
      "Unsupervised Domain Adaptation by Backpropagation\n",
      "1 2 3 4 5\n",
      "·104\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Batches seen\n",
      "Validation error\n",
      "Real data only\n",
      "Synthetic data only\n",
      "Both\n",
      "Figure 4.Semi-supervised domain adaptation for the trafﬁc signs.\n",
      "As labeled target domain data are shown to the method, it achieves\n",
      "signiﬁcantly lower error than the model trained on target domain\n",
      "data only or on source domain data only.\n",
      "pected to be more generic and to perform reasonably on\n",
      "the MNIST dataset. This, indeed, turns out to be the case\n",
      "and is supported by the appearance of the feature distribu-\n",
      "tions. We observe a quite strong separation between the\n",
      "domains when we feed them into the CNN trained solely\n",
      "on MNIST, whereas for the SVHN-trained network the\n",
      "features are much more intermixed. This difference prob-\n",
      "ably explains why our method succeeded in improving the\n",
      "performance by adaptation in the SVHN →MNIST sce-\n",
      "nario (see Table 1) but not in the opposite direction (SA is\n",
      "\n",
      "Question: What is CNN training procedure described under Unsupervised Domain Adaptation?\n",
      "Answer:\n",
      "\n",
      "Advanced RAG Answer:\n",
      "<think>\n",
      "Okay, let's try to figure out the CNN training procedure described under Unsupervised Domain Adaptation based on the provided context. First, I need to recall what unsupervised domain adaptation (UDA) is. UDA is a technique where a model trained on a source domain (with labeled data) is adapted to a target domain (with unlabeled data) without using any labels from the target domain. The goal is to make the model perform well on the target domain by aligning the feature distributions between the source and target domains.\n",
      "\n",
      "Looking at the references mentioned, there are several papers cited. The key ones here are \"Unsupervised Domain Adaptation by Backpropagation\" by Gong et al. (2013), which is likely the main paper being discussed. Other references include works by Goodfellow et al. on Generative Adversarial Networks (GANs), which might be relevant if adversarial training is involved. Also, there's a mention of connecting the dots with landmarks and discriminatively learning domain-invariant features.\n",
      "\n",
      "The figure mentioned (Figure 4) shows a semi-supervised domain adaptation for traffic signs. The validation error is plotted against batches seen, comparing models trained on real data only, synthetic data only, and both. The method achieves lower error when combining both, suggesting that the model is using some form of domain adaptation to leverage both domains.\n",
      "\n",
      "In the text, there's a discussion about feature distributions. When using a CNN trained solely on MNIST, the features are strongly separated between domains, but when trained on SVHN, the features are more intermixed. This implies that the training procedure involves learning domain-invariant features. The method succeeded in improving performance in the SVHN → MNIST scenario but not the opposite, which might indicate that the adaptation is directional and depends on the source and target domains' characteristics.\n",
      "\n",
      "Now, putting this together with the key paper by Gong et al. (2013), their approach is called \"Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation.\" The main idea is to learn features that are invariant across domains. The procedure likely involves a CNN where the features are trained to be similar for the same class across different domains. Since it's unsupervised, the target domain doesn't have labels, so the model must find a way to align the domains without explicit supervision.\n",
      "\n",
      "Possible components of the training procedure:\n",
      "\n",
      "1. **Domain-Invariant Feature Learning**: The CNN is trained to extract features that are similar for the same class in both source and target domains. This could involve minimizing a loss that encourages the features from both domains to be close in the feature space.\n",
      "\n",
      "2. **Adversarial Training**: Inspired by GANs (Goodfellow et al., 2014), there might be a domain classifier that tries to distinguish between source and target features, while the feature extractor is trained to fool this classifier, leading to domain-invariant features.\n",
      "\n",
      "3. **Landmark Matching**: The paper's title mentions \"connecting the dots with landmarks,\" which might refer to using specific points (landmarks) in the feature space that correspond to the same class in both domains. These landmarks could be used to align the distributions.\n",
      "\n",
      "4. **Semi-Supervised Learning**: The figure shows a semi-supervised approach where some labeled target data is used. However, the question specifies unsupervised domain adaptation, so maybe the semi-supervised part is an extension or a different scenario. The main UDA procedure would not use labeled target data.\n",
      "\n",
      "5. **Backpropagation with Domain Adaptation Loss**: The training procedure would involve backpropagating gradients from both the task-specific loss (e.g., classification loss on the source domain) and the domain adaptation loss (e.g., adversarial loss or MMD loss) to adjust the network parameters.\n",
      "\n",
      "6. **Feature Distribution Alignment**: Techniques like Maximum Mean Discrepancy (MMD) or Correlation Alignment (CORAL) might be used to measure and minimize the difference between source and target feature distributions.\n",
      "\n",
      "Given the references to adversarial methods (GANs) and the mention of domain-invariant features, the training procedure likely combines a standard classification loss on the source domain with an adversarial loss that encourages the features to be indistinguishable between domains. The adversarial component would involve a domain discriminator network that is trained to classify whether a feature comes from the source or target domain, while the feature extractor is trained to confuse this discriminator.\n",
      "\n",
      "Additionally, the paper by Gong et al. might use a specific approach where they first pre-train the network on the source domain and then fine-tune it with domain adaptation, or they might train both the feature extractor and the classifier jointly with domain adaptation objectives.\n",
      "\n",
      "The key steps in the training procedure would be:\n",
      "\n",
      "- Train the CNN on the source domain with labeled data to minimize classification loss.\n",
      "- Introduce a domain adaptation component (e.g., adversarial training) to align the source and target domain features.\n",
      "- Backpropagate the combined\n",
      "\n",
      "Sources:\n",
      "[{'source': '1409.7495v2.pdf', 'page': 10, 'score': 0.397655189037323, 'preview': 'its in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning 2011, 2011.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\\nand transferring mid-level image representations using\\nconvolutional neural networks. In CVPR, 2014.\\nPan...'}, {'source': '1409.7495v2.pdf', 'page': 10, 'score': 0.397655189037323, 'preview': 'its in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning 2011, 2011.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\\nand transferring mid-level image representations using\\nconvolutional neural networks. In CVPR, 2014.\\nPan...'}, {'source': '1409.7495v2.pdf', 'page': 10, 'score': 0.3799741864204407, 'preview': 'Unsupervised Domain Adaptation by Backpropagation\\nGong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\\nnecting the dots with landmarks: Discriminatively learn-\\ning domain-invariant features for unsupervised domain\\nadaptation. In ICML, pp. 222–230, 2013.\\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Meh...'}, {'source': '1409.7495v2.pdf', 'page': 10, 'score': 0.3799741864204407, 'preview': 'Unsupervised Domain Adaptation by Backpropagation\\nGong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\\nnecting the dots with landmarks: Discriminatively learn-\\ning domain-invariant features for unsupervised domain\\nadaptation. In ICML, pp. 222–230, 2013.\\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Meh...'}, {'source': '1409.7495v2.pdf', 'page': 7, 'score': 0.37141597270965576, 'preview': 'Unsupervised Domain Adaptation by Backpropagation\\n1 2 3 4 5\\n·104\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nBatches seen\\nValidation error\\nReal data only\\nSynthetic data only\\nBoth\\nFigure 4.Semi-supervised domain adaptation for the trafﬁc signs.\\nAs labeled target domain data are shown to the method, it achieves\\nsigniﬁcantly ...'}]\n",
      "\n",
      "Confidence Score: 0.397655189037323\n",
      "\n",
      "Context Preview:\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RAG pipeline Features\n",
    "def rag_advanced(query: str, retriever: RAGRetriever, llm: ChatGroq, score_threshold: float = 0.2, return_context: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "       returns answers, sources, confidence score and optionally full context.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.retrieve(query, score_threshold=score_threshold)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return {\"answer\": \"I'm sorry, I couldn't find any relevant information to answer your question.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"context\": \"\"\n",
    "               }\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs]) \n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('doc_index', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + \"...\"\n",
    "    } for doc in retrieved_docs]\n",
    "    confidence = max(doc['similarity_score'] for doc in retrieved_docs)\n",
    "\n",
    "    # Generate answer using LLM\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    print(f\"Generated Prompt:\\n{prompt}\\n\")\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context, question=query)])\n",
    "\n",
    "    output = {\n",
    "        \"answer\": response.content,\n",
    "        \"sources\": sources,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output[\"context\"] = context\n",
    "    return output\n",
    "\n",
    "# Example usage of advanced RAG\n",
    "result = rag_advanced(\"What is CNN training procedure described under Unsupervised Domain Adaptation?\", rag_retriever, llm, score_threshold=0.1)\n",
    "# Show  answwers, sources, confidence and context preview\n",
    "print(f\"Advanced RAG Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Sources:\\n{result['sources']}\\n\")\n",
    "print(f\"Confidence Score: {result['confidence']}\\n\")\n",
    "print(f\"Context Preview:\\n{result.get('context', '')[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1384ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: What is CNN training procedure described under Unsupervised Domain Adaptation?\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Searching for top 5 similar documents...\n",
      "Retrieved 5 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "its in natural images with unsupervised feature learning.\n",
      "In NIPS Workshop on Deep Learning and Unsupervised\n",
      "Feature Learning 2011, 2011.\n",
      "Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\n",
      "and transferring mid-level image representations using\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolutional neural networks. In CVPR, 2014.\n",
      "Pan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\n",
      "Yang, Qiang. Domain adaptation via transfer component\n",
      "analysis. IEEE Transactions on Neural Networks, 22(2):\n",
      "199–210, 2011.\n",
      "S. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\n",
      "learning for domain adaptation by interpolating between\n",
      "domains. In ICML Workshop on Challenges in Repre-\n",
      "sentation Learning, 2013.\n",
      "Saenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\n",
      "Trevor. Adapting visual category models to new do-\n",
      "mains. In ECCV, pp. 213–226. 2010.\n",
      "Shimodaira, Hidetoshi. Improving predictive inference un-\n",
      "der covariate shift by weighting the log-likelihood func-\n",
      "tion. Journal of Statistical Planning and Inference , 90\n",
      "\n",
      "its in natural images with unsupervised feature learning.\n",
      "In NIPS Workshop on Deep Learning and Unsupervised\n",
      "Feature Learning 2011, 2011.\n",
      "Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\n",
      "and transferring mid-level image representations using\n",
      "convolutional neural networks. In CVPR, 2014.\n",
      "Pan, Sinno Jialin, Tsang, Ivor W., Kwok, James T., and\n",
      "Yang, Qiang. Domain adaptation via transfer component\n",
      "analysis. IEEE Transactions on Neural Networks, 22(2):\n",
      "199–210, 2011.\n",
      "S. Chopra, S. Balakrishnan and Gopalan, R. Dlid: Deep\n",
      "learning for domain adaptation by interpolating between\n",
      "domains. In ICML Workshop on Challenges in Repre-\n",
      "sentation Learning, 2013.\n",
      "Saenko, Kate, Kulis, Brian, Fritz, Mario, and Darrell,\n",
      "Trevor. Adapting visual category models to new do-\n",
      "mains. In ECCV, pp. 213–226. 2010.\n",
      "Shimodaira, Hidetoshi. Improving predictive inference un-\n",
      "der covariate shift by weighting the log-likelihood func-\n",
      "tion. Journal of Statistical Planning and Inference , 90\n",
      "\n",
      "Unsupervised Domain Adaptation by Backpropagation\n",
      "Gong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\n",
      "necting the dots with landmarks: Discriminatively learn-\n",
      "ing domain-invariant features for unsupervised domain\n",
      "adaptation. In ICML, pp. 222–230, 2013.\n",
      "Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\n",
      "Bing, Warde-Farley, David, Ozair, Sherjil, Courville,\n",
      "Aaron, and Bengio, Yoshua. Generative adversarial nets.\n",
      "In NIPS, 2014.\n",
      "Gopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\n",
      "Domain adaptation for object recognition: An unsuper-\n",
      "vised approach. In ICCV, pp. 999–1006, 2011.\n",
      "Hoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\n",
      "Saenko, Kate, and Darrell, Trevor. One-shot adapta-\n",
      "tion of supervised deep convolutional models. CoRR,\n",
      "abs/1312.6204, 2013.\n",
      "Huang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\n",
      "Borgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\n",
      "recting sample selection bias by unlabeled data. InNIPS,\n",
      "pp. 601–608, 2006.\n",
      "\n",
      "Unsupervised Domain Adaptation by Backpropagation\n",
      "Gong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\n",
      "necting the dots with landmarks: Discriminatively learn-\n",
      "ing domain-invariant features for unsupervised domain\n",
      "adaptation. In ICML, pp. 222–230, 2013.\n",
      "Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\n",
      "Bing, Warde-Farley, David, Ozair, Sherjil, Courville,\n",
      "Aaron, and Bengio, Yoshua. Generative adversarial nets.\n",
      "In NIPS, 2014.\n",
      "Gopalan, Raghuraman, Li, Ruonan, and Chellappa, Rama.\n",
      "Domain adaptation for object recognition: An unsuper-\n",
      "vised approach. In ICCV, pp. 999–1006, 2011.\n",
      "Hoffman, Judy, Tzeng, Eric, Donahue, Jeff, Jia, Yangqing,\n",
      "Saenko, Kate, and Darrell, Trevor. One-shot adapta-\n",
      "tion of supervised deep convolutional models. CoRR,\n",
      "abs/1312.6204, 2013.\n",
      "Huang, Jiayuan, Smola, Alexander J., Gretton, Arthur,\n",
      "Borgwardt, Karsten M., and Sch ¨olkopf, Bernhard. Cor-\n",
      "recting sample selection bias by unlabeled data. InNIPS,\n",
      "pp. 601–608, 2006.\n",
      "\n",
      "Unsupervised Domain Adaptation by Backpropagation\n",
      "1 2 3 4 5\n",
      "·104\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Batches seen\n",
      "Validation error\n",
      "Real data only\n",
      "Synthetic data only\n",
      "Both\n",
      "Figure 4.Semi-supervised domain adaptation for the trafﬁc signs.\n",
      "As labeled target domain data are shown to the method, it achieves\n",
      "signiﬁcantly lower error than the model trained on target domain\n",
      "data only or on source domain data only.\n",
      "pected to be more generic and to perform reasonably on\n",
      "the MNIST dataset. This, indeed, turns out to be the case\n",
      "and is supported by the appearance of the feature distribu-\n",
      "tions. We observe a quite strong separation between the\n",
      "domains when we feed them into the CNN trained solely\n",
      "on MNIST, whereas for the SVHN-trained network the\n",
      "features are much more intermixed. This difference prob-\n",
      "ably explains why our method succeeded in improving the\n",
      "performance by adaptation in the SVHN →MNIST sce-\n",
      "nario (see Table 1) but not in the opposite direction (SA is\n",
      "\n",
      "Question: What is CNN training procedure described under Unsupervised Domain Adaptation?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: <think>\n",
      "Okay, let's see. The user is asking about the CNN training procedure described under Unsupervised Domain Adaptation. I need to look through the provided context to find relevant information.\n",
      "\n",
      "Looking at the context, there are several papers mentioned related to domain adaptation. The key part here is the section titled \"Unsupervised Domain Adaptation by Backpropagation.\" The user provided some text that includes a figure (Figure 4) about semi-supervised domain adaptation for traffic signs. The figure's caption mentions that as labeled target domain data are shown to the method, it achieves lower error than models trained on target or source data only. \n",
      "\n",
      "In the text following the figure, there's a discussion about feature distributions. It mentions that when a CNN is trained solely on MNIST, there's a strong separation between domains, but when trained on SVHN, the features are more intermixed. This suggests that the training procedure involves adapting the model from a source domain (like SVHN) to a target domain (like MNIST) using unlabeled target data. The method likely uses backpropagation to adjust the model parameters to minimize domain discrepancy, possibly through techniques like adversarial training or domain-invariant feature learning.\n",
      "\n",
      "The paper by Gong et al. (2013) titled \"Connecting the dots with landmarks\" is mentioned, which focuses on discriminatively learning domain-invariant features. Also, Goodfellow's GANs (2014) might be relevant if adversarial training is part of the procedure. The key idea here is that the CNN is trained using both source and target data, with the target data being unlabeled. The model learns to produce features that are invariant across domains, which helps in adapting the model to the target domain without labeled examples there.\n",
      "\n",
      "The procedure probably involves a loss function that includes both the task-specific loss (like classification) on the source domain and a domain adaptation loss that encourages the features to be similar across domains. Techniques like Maximum Mean Discrepancy (MMD) or adversarial domain classifiers might be used here. The mention of semi-supervised learning in Figure 4 indicates that some labeled target data are used, but the main focus is on unsupervised adaptation using unlabeled target data.\n",
      "\n",
      "So, putting it all together, the training procedure uses backpropagation to optimize the CNN parameters by minimizing both the source domain task loss and a domain adaptation loss, which reduces the discrepancy between source and target domain features. This allows the model to generalize better to the target domain without requiring labeled target examples beyond what's necessary.\n",
      "</think>\n",
      "\n",
      "The CNN training procedure under Unsupervised Domain Adaptation involves **backpropagation to learn domain-invariant features** by minimizing both task-specific (e.g., classification) loss on the source domain and a domain adaptation loss that reduces discrepancy between source and target domain features. Key aspects include:  \n",
      "1. **Unsupervised adaptation**: Uses unlabeled target domain data alongside labeled source data.  \n",
      "2. **Domain-invariant feature learning**: Techniques like adversarial training (e.g., GANs) or maximum mean discrepancy (MMD) align feature distributions across domains.  \n",
      "3. **Semi-supervised refinement**: Incorporates limited labeled target data (if available) to further improve adaptation (as shown in Figure 4 for traffic signs).  \n",
      "4. **End-to-end optimization**: The model is trained discriminatively to produce features that generalize across domains, as demonstrated in works like Gong et al. (2013) and Gopalan et al. (2011).  \n",
      "\n",
      "This approach enables the CNN to adapt to a target domain without requiring full supervision in that domain.\n",
      "\n",
      "Citations:\n",
      "[1] 1409.7495v2.pdf (page 10)\n",
      "[2] 1409.7495v2.pdf (page 10)\n",
      "[3] 1409.7495v2.pdf (page 10)\n",
      "[4] 1409.7495v2.pdf (page 10)\n",
      "[5] 1409.7495v2.pdf (page 7)\n",
      "Summary: <think>\n",
      "Okay, let me try to summarize this. The user wants a two-sentence summary of the CNN training procedure under Unsupervised Domain Adaptation. The original answer mentions backpropagation, domain-invariant features, minimizing task loss and domain discrepancy, adversarial training, MMD, and semi-supervised refinement.\n",
      "\n",
      "First, I need to condense the key points. The main idea is using backpropagation to learn features that work across domains by combining source task loss with domain adaptation loss. Techniques like adversarial training or MMD help align domains. Also, semi-supervised methods use some labeled target data if available.\n",
      "\n",
      "So, the first sentence should state the method: using backpropagation to minimize both task loss and domain discrepancy via domain-invariant features. The second sentence can mention the techniques (adversarial training, MMD) and the semi-supervised approach with limited labeled target data. Need to keep it concise and ensure both sentences capture the core elements without extra details.\n",
      "</think>\n",
      "\n",
      "The CNN training procedure under Unsupervised Domain Adaptation uses **backpropagation to minimize both source task loss and domain discrepancy**, learning domain-invariant features via techniques like adversarial training or maximum mean discrepancy (MMD). It leverages unlabeled target data and optionally limited labeled target examples to align source and target feature distributions, enabling adaptation without full supervision in the target domain.\n",
      "History: {'question': 'What is CNN training procedure described under Unsupervised Domain Adaptation?', 'answer': '<think>\\nOkay, let\\'s see. The user is asking about the CNN training procedure described under Unsupervised Domain Adaptation. I need to look through the provided context to find relevant information.\\n\\nLooking at the context, there are several papers mentioned related to domain adaptation. The key part here is the section titled \"Unsupervised Domain Adaptation by Backpropagation.\" The user provided some text that includes a figure (Figure 4) about semi-supervised domain adaptation for traffic signs. The figure\\'s caption mentions that as labeled target domain data are shown to the method, it achieves lower error than models trained on target or source data only. \\n\\nIn the text following the figure, there\\'s a discussion about feature distributions. It mentions that when a CNN is trained solely on MNIST, there\\'s a strong separation between domains, but when trained on SVHN, the features are more intermixed. This suggests that the training procedure involves adapting the model from a source domain (like SVHN) to a target domain (like MNIST) using unlabeled target data. The method likely uses backpropagation to adjust the model parameters to minimize domain discrepancy, possibly through techniques like adversarial training or domain-invariant feature learning.\\n\\nThe paper by Gong et al. (2013) titled \"Connecting the dots with landmarks\" is mentioned, which focuses on discriminatively learning domain-invariant features. Also, Goodfellow\\'s GANs (2014) might be relevant if adversarial training is part of the procedure. The key idea here is that the CNN is trained using both source and target data, with the target data being unlabeled. The model learns to produce features that are invariant across domains, which helps in adapting the model to the target domain without labeled examples there.\\n\\nThe procedure probably involves a loss function that includes both the task-specific loss (like classification) on the source domain and a domain adaptation loss that encourages the features to be similar across domains. Techniques like Maximum Mean Discrepancy (MMD) or adversarial domain classifiers might be used here. The mention of semi-supervised learning in Figure 4 indicates that some labeled target data are used, but the main focus is on unsupervised adaptation using unlabeled target data.\\n\\nSo, putting it all together, the training procedure uses backpropagation to optimize the CNN parameters by minimizing both the source domain task loss and a domain adaptation loss, which reduces the discrepancy between source and target domain features. This allows the model to generalize better to the target domain without requiring labeled target examples beyond what\\'s necessary.\\n</think>\\n\\nThe CNN training procedure under Unsupervised Domain Adaptation involves **backpropagation to learn domain-invariant features** by minimizing both task-specific (e.g., classification) loss on the source domain and a domain adaptation loss that reduces discrepancy between source and target domain features. Key aspects include:  \\n1. **Unsupervised adaptation**: Uses unlabeled target domain data alongside labeled source data.  \\n2. **Domain-invariant feature learning**: Techniques like adversarial training (e.g., GANs) or maximum mean discrepancy (MMD) align feature distributions across domains.  \\n3. **Semi-supervised refinement**: Incorporates limited labeled target data (if available) to further improve adaptation (as shown in Figure 4 for traffic signs).  \\n4. **End-to-end optimization**: The model is trained discriminatively to produce features that generalize across domains, as demonstrated in works like Gong et al. (2013) and Gopalan et al. (2011).  \\n\\nThis approach enables the CNN to adapt to a target domain without requiring full supervision in that domain.', 'sources': [{'source': '1409.7495v2.pdf', 'page': 10, 'score': 0.397655189037323, 'preview': 'its in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Lea...'}, {'source': '1409.7495v2.pdf', 'page': 10, 'score': 0.397655189037323, 'preview': 'its in natural images with unsupervised feature learning.\\nIn NIPS Workshop on Deep Learning and Unsupervised\\nFeature Lea...'}, {'source': '1409.7495v2.pdf', 'page': 10, 'score': 0.3799741864204407, 'preview': 'Unsupervised Domain Adaptation by Backpropagation\\nGong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\\nnecting the dots wi...'}, {'source': '1409.7495v2.pdf', 'page': 10, 'score': 0.3799741864204407, 'preview': 'Unsupervised Domain Adaptation by Backpropagation\\nGong, Boqing, Grauman, Kristen, and Sha, Fei. Con-\\nnecting the dots wi...'}, {'source': '1409.7495v2.pdf', 'page': 7, 'score': 0.37141597270965576, 'preview': 'Unsupervised Domain Adaptation by Backpropagation\\n1 2 3 4 5\\n·104\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nBatches seen\\nValidation error\\nReal ...'}], 'summary': '<think>\\nOkay, let me try to summarize this. The user wants a two-sentence summary of the CNN training procedure under Unsupervised Domain Adaptation. The original answer mentions backpropagation, domain-invariant features, minimizing task loss and domain discrepancy, adversarial training, MMD, and semi-supervised refinement.\\n\\nFirst, I need to condense the key points. The main idea is using backpropagation to learn features that work across domains by combining source task loss with domain adaptation loss. Techniques like adversarial training or MMD help align domains. Also, semi-supervised methods use some labeled target data if available.\\n\\nSo, the first sentence should state the method: using backpropagation to minimize both task loss and domain discrepancy via domain-invariant features. The second sentence can mention the techniques (adversarial training, MMD) and the semi-supervised approach with limited labeled target data. Need to keep it concise and ensure both sentences capture the core elements without extra details.\\n</think>\\n\\nThe CNN training procedure under Unsupervised Domain Adaptation uses **backpropagation to minimize both source task loss and domain discrepancy**, learning domain-invariant features via techniques like adversarial training or maximum mean discrepancy (MMD). It leverages unlabeled target data and optionally limited labeled target examples to align source and target feature distributions, enabling adaptation without full supervision in the target domain.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"What is CNN training procedure described under Unsupervised Domain Adaptation?\", top_k=5, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e9620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}